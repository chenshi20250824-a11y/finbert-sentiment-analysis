{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67b19a99",
   "metadata": {},
   "source": [
    "## PART 1: Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f820b757",
   "metadata": {},
   "source": [
    "##### 1.1: Fetch US stock price data (7,400 entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1827af96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“Š å¼€å§‹è·å–ç¾è‚¡ä»·æ ¼æ•°æ®\n",
      "============================================================\n",
      "è‚¡ç¥¨åˆ—è¡¨: AAPL, MSFT, GOOGL, NVDA, TSLA, JPM, BAC, GS, WMT, DIS\n",
      "æ—¥æœŸèŒƒå›´: 2023-01-01 åˆ° 2025-12-13\n",
      "ä¿å­˜ä½ç½®: å½“å‰æ–‡ä»¶å¤¹\n",
      "============================================================\n",
      "\n",
      "æ­£åœ¨è·å– AAPL çš„æ•°æ®...\n",
      "âœ… AAPL:  è·å–äº† 740 æ¡æ•°æ®\n",
      "æ­£åœ¨è·å– MSFT çš„æ•°æ®...\n",
      "âœ… MSFT:  è·å–äº† 740 æ¡æ•°æ®\n",
      "æ­£åœ¨è·å– GOOGL çš„æ•°æ®...\n",
      "âœ… GOOGL:  è·å–äº† 740 æ¡æ•°æ®\n",
      "æ­£åœ¨è·å– NVDA çš„æ•°æ®...\n",
      "âœ… NVDA:  è·å–äº† 740 æ¡æ•°æ®\n",
      "æ­£åœ¨è·å– TSLA çš„æ•°æ®...\n",
      "âœ… TSLA:  è·å–äº† 740 æ¡æ•°æ®\n",
      "æ­£åœ¨è·å– JPM çš„æ•°æ®...\n",
      "âœ… JPM:  è·å–äº† 740 æ¡æ•°æ®\n",
      "æ­£åœ¨è·å– BAC çš„æ•°æ®...\n",
      "âœ… BAC:  è·å–äº† 740 æ¡æ•°æ®\n",
      "æ­£åœ¨è·å– GS çš„æ•°æ®...\n",
      "âœ… GS:  è·å–äº† 740 æ¡æ•°æ®\n",
      "æ­£åœ¨è·å– WMT çš„æ•°æ®...\n",
      "âœ… WMT:  è·å–äº† 740 æ¡æ•°æ®\n",
      "æ­£åœ¨è·å– DIS çš„æ•°æ®...\n",
      "âœ… DIS:  è·å–äº† 740 æ¡æ•°æ®\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ æ•°æ®ç»Ÿè®¡\n",
      "============================================================\n",
      "æ€»è®°å½•æ•°: 7400\n",
      "è‚¡ç¥¨æ•°é‡: 10\n",
      "æ—¥æœŸèŒƒå›´:  2023-01-03 00:00:00 åˆ° 2025-12-12 00:00:00\n",
      "\n",
      "å„è‚¡ç¥¨æ•°æ®é‡:\n",
      "ticker\n",
      "AAPL     740\n",
      "BAC      740\n",
      "DIS      740\n",
      "GOOGL    740\n",
      "GS       740\n",
      "JPM      740\n",
      "MSFT     740\n",
      "NVDA     740\n",
      "TSLA     740\n",
      "WMT      740\n",
      "Name: count, dtype: int64\n",
      "\n",
      "âœ… åˆå¹¶æ•°æ®å·²ä¿å­˜åˆ°: .\\all_stocks_prices.csv\n",
      "   - AAPL: .\\AAPL_prices.csv\n",
      "   - MSFT: .\\MSFT_prices.csv\n",
      "   - GOOGL: .\\GOOGL_prices.csv\n",
      "   - NVDA: .\\NVDA_prices.csv\n",
      "   - TSLA: .\\TSLA_prices.csv\n",
      "   - JPM: .\\JPM_prices.csv\n",
      "   - BAC: .\\BAC_prices.csv\n",
      "   - GS: .\\GS_prices.csv\n",
      "   - WMT: .\\WMT_prices.csv\n",
      "   - DIS: .\\DIS_prices.csv\n",
      "\n",
      "============================================================\n",
      "ğŸ“‹ æ•°æ®é¢„è§ˆï¼ˆå‰10è¡Œï¼‰\n",
      "============================================================\n",
      "  ticker       date    open    high     low   close       volume\n",
      "0   AAPL 2023-01-03  127.06  127.68  120.95  121.85  112117471.0\n",
      "1   AAPL 2023-01-04  123.67  125.44  121.86  123.14   89113633.0\n",
      "2   AAPL 2023-01-05  123.91  124.55  121.54  121.80   80962708.0\n",
      "3   AAPL 2023-01-06  122.79  127.07  121.67  126.40   87754715.0\n",
      "4   AAPL 2023-01-09  127.25  130.19  126.67  126.93   70790813.0\n",
      "5   AAPL 2023-01-10  127.04  128.04  124.90  127.51   63896155.0\n",
      "6   AAPL 2023-01-11  128.03  130.29  127.24  130.27   69458949.0\n",
      "7   AAPL 2023-01-12  130.66  131.04  128.22  130.19   71379648.0\n",
      "8   AAPL 2023-01-13  128.81  131.70  128.44  131.54   57809719.0\n",
      "9   AAPL 2023-01-17  131.61  134.07  130.91  132.72   63646627.0\n",
      "\n",
      "âœ… ç¬¬ä¸€æ­¥å®Œæˆï¼è‚¡ç¥¨ä»·æ ¼æ•°æ®å·²è·å–ã€‚\n",
      "\n",
      "ğŸ“ ç”Ÿæˆçš„æ–‡ä»¶åœ¨å½“å‰æ–‡ä»¶å¤¹ï¼š\n",
      "   - all_stocks_prices.csv (æ‰€æœ‰è‚¡ç¥¨åˆå¹¶)\n",
      "   - AAPL_prices.csv, MSFT_prices.csv ç­‰ (å•ç‹¬è‚¡ç¥¨)\n"
     ]
    }
   ],
   "source": [
    "# step1_fetch_stock_prices.py\n",
    "\n",
    "\"\"\"\n",
    "Fetch US stock price data using akshare.\n",
    "Goal: 10 stocks, past two years.\n",
    "\"\"\"\n",
    "\n",
    "import akshare as ak\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'TSLA', 'JPM', 'BAC', 'GS', 'WMT', 'DIS']\n",
    "START_DATE = '2023-01-01'\n",
    "END_DATE = '2025-12-13'\n",
    "OUTPUT_DIR = '.'  # current folder\n",
    "\n",
    "def fetch_single_stock(ticker, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch historical price data for a single stock.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Fetching data for {ticker}...\")\n",
    "\n",
    "        # Fetch US daily data using akshare\n",
    "        df = ak.stock_us_daily(symbol=ticker, adjust=\"qfq\")\n",
    "\n",
    "        # Convert date format\n",
    "        df['date'] = pd. to_datetime(df['date'])\n",
    "\n",
    "        # Filter by date range\n",
    "        df = df[(df['date'] >= start_date) & (df['date'] <= end_date)]\n",
    "\n",
    "        # Add ticker column\n",
    "        df['ticker'] = ticker\n",
    "\n",
    "        # Reorder columns\n",
    "        df = df[['ticker', 'date', 'open', 'high', 'low', 'close', 'volume']]\n",
    "\n",
    "        print(f\"âœ… {ticker}: fetched {len(df)} records\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {ticker} fetch failed: {e}\")\n",
    "        return None\n",
    "\n",
    "def fetch_all_stocks(tickers, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Fetch all stock data in batch.\n",
    "    \"\"\"\n",
    "    all_data = []\n",
    "\n",
    "    for ticker in tickers:\n",
    "        df = fetch_single_stock(ticker, start_date, end_date)\n",
    "        if df is not None:\n",
    "            all_data.append(df)\n",
    "\n",
    "        # Avoid too frequent requests, sleep a bit\n",
    "        time.sleep(2)\n",
    "\n",
    "    # Combine all data\n",
    "    if all_data:\n",
    "        combined_df = pd.concat(all_data, ignore_index=True)\n",
    "        return combined_df\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def save_data(df, output_dir):\n",
    "    \"\"\"\n",
    "    Save data to CSV files (current folder).\n",
    "    \"\"\"\n",
    "    # Save combined data to current folder\n",
    "    combined_file = os.path. join(output_dir, 'all_stocks_prices.csv')\n",
    "    df.to_csv(combined_file, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nâœ… Combined data saved to: {combined_file}\")\n",
    "\n",
    "    # Also save each stock's data to separate files in current folder\n",
    "    for ticker in df['ticker'].unique():\n",
    "        ticker_df = df[df['ticker'] == ticker]\n",
    "        ticker_file = os. path.join(output_dir, f'{ticker}_prices.csv')\n",
    "        ticker_df.to_csv(ticker_file, index=False, encoding='utf-8-sig')\n",
    "        print(f\"   - {ticker}: {ticker_file}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ“Š Start fetching US stock price data\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Stock list: {', '.join(TICKERS)}\")\n",
    "    print(f\"Date range: {START_DATE} to {END_DATE}\")\n",
    "    print(f\"Save location: current folder\")\n",
    "    print(\"=\"*60 + \"\\n\")\n",
    "\n",
    "    # Fetch data\n",
    "    df = fetch_all_stocks(TICKERS, START_DATE, END_DATE)\n",
    "\n",
    "    if df is not None:\n",
    "        # Show statistics\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“ˆ Data statistics\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Total records: {len(df)}\")\n",
    "        print(f\"Number of stocks: {df['ticker'].nunique()}\")\n",
    "        print(f\"Date range:  {df['date'].min()} to {df['date'].max()}\")\n",
    "        print(\"\\nRecords per stock:\")\n",
    "        print(df['ticker'].value_counts().sort_index())\n",
    "\n",
    "        # Save data\n",
    "        save_data(df, OUTPUT_DIR)\n",
    "\n",
    "        # Show preview of first rows\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"ğŸ“‹ Data preview (first 10 rows)\")\n",
    "        print(\"=\"*60)\n",
    "        print(df.head(10))\n",
    "\n",
    "        print(\"\\nâœ… Step 1 complete! Stock price data fetched.\")\n",
    "        print(f\"\\nğŸ“ Generated files in current folder:\")\n",
    "        print(f\"   - all_stocks_prices.csv (all stocks combined)\")\n",
    "        print(f\"   - AAPL_prices.csv, MSFT_prices.csv, etc. (individual stocks)\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nâŒ Data fetch failed, please check your network connection or akshare version.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b97d44d",
   "metadata": {},
   "source": [
    "##### 1.2: Fetch US stock news from Finviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c1d12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "ğŸ“° Finviz æ–°é—»çˆ¬è™«\n",
      "============================================================\n",
      "ç›®æ ‡è‚¡ç¥¨: AAPL, MSFT, GOOGL, NVDA, TSLA, JPM, BAC, GS, WMT, DIS\n",
      "ä¿å­˜æ–‡ä»¶: finviz_news_data.csv\n",
      "å¼€å§‹æ—¶é—´: 2025-12-13 20:58:56\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "å¼€å§‹æŠ“å– 10 åªè‚¡ç¥¨çš„æ–°é—»\n",
      "============================================================\n",
      "\n",
      "[1/10] æ­£åœ¨å¤„ç†:  AAPL\n",
      "----------------------------------------\n",
      "  ğŸ“¡ æ­£åœ¨è®¿é—®:  https://finviz.com/quote.ashx?t=AAPL\n",
      "  âœ… AAPL:  æˆåŠŸæŠ“å– 100 æ¡æ–°é—»\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 100 æ¡æ–°é—»\n",
      "\n",
      "  â³ ç­‰å¾… 3.5 ç§’.. .\n",
      "\n",
      "[2/10] æ­£åœ¨å¤„ç†:  MSFT\n",
      "----------------------------------------\n",
      "  ğŸ“¡ æ­£åœ¨è®¿é—®:  https://finviz.com/quote.ashx?t=MSFT\n",
      "  âœ… MSFT:  æˆåŠŸæŠ“å– 100 æ¡æ–°é—»\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 200 æ¡æ–°é—»\n",
      "\n",
      "  â³ ç­‰å¾… 6.0 ç§’.. .\n",
      "\n",
      "[3/10] æ­£åœ¨å¤„ç†:  GOOGL\n",
      "----------------------------------------\n",
      "  ğŸ“¡ æ­£åœ¨è®¿é—®:  https://finviz.com/quote.ashx?t=GOOGL\n",
      "  âœ… GOOGL:  æˆåŠŸæŠ“å– 100 æ¡æ–°é—»\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 300 æ¡æ–°é—»\n",
      "\n",
      "  â³ ç­‰å¾… 5.1 ç§’.. .\n",
      "\n",
      "[4/10] æ­£åœ¨å¤„ç†:  NVDA\n",
      "----------------------------------------\n",
      "  ğŸ“¡ æ­£åœ¨è®¿é—®:  https://finviz.com/quote.ashx?t=NVDA\n",
      "  âœ… NVDA:  æˆåŠŸæŠ“å– 100 æ¡æ–°é—»\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 400 æ¡æ–°é—»\n",
      "\n",
      "  â³ ç­‰å¾… 3.3 ç§’.. .\n",
      "\n",
      "[5/10] æ­£åœ¨å¤„ç†:  TSLA\n",
      "----------------------------------------\n",
      "  ğŸ“¡ æ­£åœ¨è®¿é—®:  https://finviz.com/quote.ashx?t=TSLA\n",
      "  âœ… TSLA:  æˆåŠŸæŠ“å– 100 æ¡æ–°é—»\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 500 æ¡æ–°é—»\n",
      "\n",
      "  â³ ç­‰å¾… 6.5 ç§’.. .\n",
      "\n",
      "[6/10] æ­£åœ¨å¤„ç†:  JPM\n",
      "----------------------------------------\n",
      "  ğŸ“¡ æ­£åœ¨è®¿é—®:  https://finviz.com/quote.ashx?t=JPM\n",
      "  âœ… JPM:  æˆåŠŸæŠ“å– 100 æ¡æ–°é—»\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 600 æ¡æ–°é—»\n",
      "\n",
      "  â³ ç­‰å¾… 6.0 ç§’.. .\n",
      "\n",
      "[7/10] æ­£åœ¨å¤„ç†:  BAC\n",
      "----------------------------------------\n",
      "  ğŸ“¡ æ­£åœ¨è®¿é—®:  https://finviz.com/quote.ashx?t=BAC\n",
      "  âœ… BAC:  æˆåŠŸæŠ“å– 100 æ¡æ–°é—»\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 700 æ¡æ–°é—»\n",
      "\n",
      "  â³ ç­‰å¾… 4.1 ç§’.. .\n",
      "\n",
      "[8/10] æ­£åœ¨å¤„ç†:  GS\n",
      "----------------------------------------\n",
      "  ğŸ“¡ æ­£åœ¨è®¿é—®:  https://finviz.com/quote.ashx?t=GS\n",
      "  âœ… GS:  æˆåŠŸæŠ“å– 100 æ¡æ–°é—»\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 800 æ¡æ–°é—»\n",
      "\n",
      "  â³ ç­‰å¾… 3.1 ç§’.. .\n",
      "\n",
      "[9/10] æ­£åœ¨å¤„ç†:  WMT\n",
      "----------------------------------------\n",
      "  ğŸ“¡ æ­£åœ¨è®¿é—®:  https://finviz.com/quote.ashx?t=WMT\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 233\u001b[39m\n\u001b[32m    230\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33må»ºè®®:  è¿è¡Œå…¶ä»–çˆ¬è™«è„šæœ¬è¡¥å……æ•°æ®ï¼ˆYahoo Financeã€MarketWatchï¼‰\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 215\u001b[39m, in \u001b[36mmain\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# å¼€å§‹æŠ“å–\u001b[39;00m\n\u001b[32m    214\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m news_list = \u001b[43mfetch_all_tickers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTICKERS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;66;03m# ä¿å­˜æ•°æ®\u001b[39;00m\n\u001b[32m    218\u001b[39m df = save_to_csv(news_list, OUTPUT_FILE)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 145\u001b[39m, in \u001b[36mfetch_all_tickers\u001b[39m\u001b[34m(tickers)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m40\u001b[39m)\n\u001b[32m    144\u001b[39m \u001b[38;5;66;03m# æŠ“å–æ–°é—»\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m145\u001b[39m news = \u001b[43mfetch_finviz_news\u001b[49m\u001b[43m(\u001b[49m\u001b[43mticker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    146\u001b[39m all_news.extend(news)\n\u001b[32m    148\u001b[39m \u001b[38;5;66;03m# æ˜¾ç¤ºè¿›åº¦\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 72\u001b[39m, in \u001b[36mfetch_finviz_news\u001b[39m\u001b[34m(ticker)\u001b[39m\n\u001b[32m     69\u001b[39m response = requests.get(url, headers=HEADERS, timeout=\u001b[32m15\u001b[39m)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response. status_code == \u001b[32m200\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     soup = \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhtml.parser\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     \u001b[38;5;66;03m# Finviz çš„æ–°é—»è¡¨æ ¼\u001b[39;00m\n\u001b[32m     75\u001b[39m     news_table = soup.find(\u001b[33m'\u001b[39m\u001b[33mtable\u001b[39m\u001b[33m'\u001b[39m, {\u001b[33m'\u001b[39m\u001b[33mid\u001b[39m\u001b[33m'\u001b[39m:  \u001b[33m'\u001b[39m\u001b[33mnews-table\u001b[39m\u001b[33m'\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Anaconda\\envs\\CISC7201\\Lib\\site-packages\\bs4\\__init__.py:476\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    474\u001b[39m \u001b[38;5;28mself\u001b[39m.builder.initialize_soup(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    475\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m476\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_feed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    477\u001b[39m     success = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Anaconda\\envs\\CISC7201\\Lib\\site-packages\\bs4\\__init__.py:661\u001b[39m, in \u001b[36mBeautifulSoup._feed\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;28mself\u001b[39m.builder.reset()\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.markup \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m661\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    662\u001b[39m \u001b[38;5;66;03m# Close out any unfinished strings and close all the open tags.\u001b[39;00m\n\u001b[32m    663\u001b[39m \u001b[38;5;28mself\u001b[39m.endData()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Anaconda\\envs\\CISC7201\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:455\u001b[39m, in \u001b[36mHTMLParserTreeBuilder.feed\u001b[39m\u001b[34m(self, markup, _parser_class)\u001b[39m\n\u001b[32m    452\u001b[39m parser = _parser_class(\u001b[38;5;28mself\u001b[39m.soup, *args, **kwargs)\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m     \u001b[43mparser\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmarkup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    456\u001b[39m     parser.close()\n\u001b[32m    457\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    458\u001b[39m     \u001b[38;5;66;03m# html.parser raises AssertionError in rare cases to\u001b[39;00m\n\u001b[32m    459\u001b[39m     \u001b[38;5;66;03m# indicate a fatal problem with the markup, especially\u001b[39;00m\n\u001b[32m    460\u001b[39m     \u001b[38;5;66;03m# when there's an error in the doctype declaration.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Anaconda\\envs\\CISC7201\\Lib\\html\\parser.py:129\u001b[39m, in \u001b[36mHTMLParser.feed\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Feed data to the parser.\u001b[39;00m\n\u001b[32m    124\u001b[39m \n\u001b[32m    125\u001b[39m \u001b[33;03mCall this as often as you want, with as little or as much text\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[33;03mas you want (may include '\\n').\u001b[39;00m\n\u001b[32m    127\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;28mself\u001b[39m.rawdata = \u001b[38;5;28mself\u001b[39m.rawdata + data\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgoahead\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Anaconda\\envs\\CISC7201\\Lib\\html\\parser.py:189\u001b[39m, in \u001b[36mHTMLParser.goahead\u001b[39m\u001b[34m(self, end)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m startswith(\u001b[33m'\u001b[39m\u001b[33m<\u001b[39m\u001b[33m'\u001b[39m, i):\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m starttagopen.match(rawdata, i): \u001b[38;5;66;03m# < + letter\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m         k = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    190\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m startswith(\u001b[33m\"\u001b[39m\u001b[33m</\u001b[39m\u001b[33m\"\u001b[39m, i):\n\u001b[32m    191\u001b[39m         k = \u001b[38;5;28mself\u001b[39m.parse_endtag(i)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Anaconda\\envs\\CISC7201\\Lib\\html\\parser.py:356\u001b[39m, in \u001b[36mHTMLParser.parse_starttag\u001b[39m\u001b[34m(self, i)\u001b[39m\n\u001b[32m    354\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_startendtag(tag, attrs)\n\u001b[32m    355\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m356\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.CDATA_CONTENT_ELEMENTS:\n\u001b[32m    358\u001b[39m         \u001b[38;5;28mself\u001b[39m.set_cdata_mode(tag)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Anaconda\\envs\\CISC7201\\Lib\\site-packages\\bs4\\builder\\_htmlparser.py:182\u001b[39m, in \u001b[36mBeautifulSoupHTMLParser.handle_starttag\u001b[39m\u001b[34m(self, tag, attrs, handle_empty_element)\u001b[39m\n\u001b[32m    180\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    181\u001b[39m     sourceline = sourcepos = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m tagObj = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msoup\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_starttag\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtag\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattr_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msourceline\u001b[49m\u001b[43m=\u001b[49m\u001b[43msourceline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msourcepos\u001b[49m\u001b[43m=\u001b[49m\u001b[43msourcepos\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tagObj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tagObj.is_empty_element \u001b[38;5;129;01mand\u001b[39;00m handle_empty_element:\n\u001b[32m    186\u001b[39m     \u001b[38;5;66;03m# Unlike other parsers, html.parser doesn't send separate end tag\u001b[39;00m\n\u001b[32m    187\u001b[39m     \u001b[38;5;66;03m# events for empty-element tags. (It's handled in\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# don't want handle_endtag() to cross off any previous end\u001b[39;00m\n\u001b[32m    194\u001b[39m     \u001b[38;5;66;03m# events for tags of this name.\u001b[39;00m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m.handle_endtag(tag, check_already_closed=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Anaconda\\envs\\CISC7201\\Lib\\site-packages\\bs4\\__init__.py:1053\u001b[39m, in \u001b[36mBeautifulSoup.handle_starttag\u001b[39m\u001b[34m(self, name, namespace, nsprefix, attrs, sourceline, sourcepos, namespaces)\u001b[39m\n\u001b[32m   1051\u001b[39m     \u001b[38;5;28mself\u001b[39m._most_recent_element.next_element = tag\n\u001b[32m   1052\u001b[39m \u001b[38;5;28mself\u001b[39m._most_recent_element = tag\n\u001b[32m-> \u001b[39m\u001b[32m1053\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpushTag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtag\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tag\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\Anaconda\\envs\\CISC7201\\Lib\\site-packages\\bs4\\__init__.py:812\u001b[39m, in \u001b[36mBeautifulSoup.pushTag\u001b[39m\u001b[34m(self, tag)\u001b[39m\n\u001b[32m    809\u001b[39m         \u001b[38;5;28mself\u001b[39m.currentTag = \u001b[38;5;28mself\u001b[39m.tagStack[-\u001b[32m1\u001b[39m]\n\u001b[32m    810\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.currentTag\n\u001b[32m--> \u001b[39m\u001b[32m812\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpushTag\u001b[39m(\u001b[38;5;28mself\u001b[39m, tag: Tag) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    813\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Internal method called by handle_starttag when a tag is opened.\u001b[39;00m\n\u001b[32m    814\u001b[39m \n\u001b[32m    815\u001b[39m \u001b[33;03m    :meta private:\u001b[39;00m\n\u001b[32m    816\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m    817\u001b[39m     \u001b[38;5;66;03m# print(\"Push\", tag.name)\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# step2_scrape_finviz_news.py\n",
    "\n",
    "\"\"\"\n",
    "Scrape US stock news from Finviz.\n",
    "Advantages: No login required, weak anti-scraping, clear data structure.\n",
    "Goal: 10 stocks Ã— 500 news = 5,000 items.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'TSLA', 'JPM', 'BAC', 'GS', 'WMT', 'DIS']\n",
    "OUTPUT_FILE = 'finviz_news_data.csv'\n",
    "\n",
    "# Request headers (simulate real browser)\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}\n",
    "\n",
    "def parse_finviz_date(date_str):\n",
    "    \"\"\"\n",
    "    Parse Finviz date format.\n",
    "    Example: 'Dec-13-24 09:30AM' or '09:30AM'\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # If only time, it's today\n",
    "        if re.match(r'^\\d{1,2}:\\d{2}(AM|PM)$', date_str):\n",
    "            return datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "        # Parse full date\n",
    "        # Format: Dec-13-24\n",
    "        date_match = re.match(r'([A-Za-z]{3})-(\\d{2})-(\\d{2})', date_str)\n",
    "        if date_match:\n",
    "            month_str, day, year = date_match.groups()\n",
    "            # Convert month\n",
    "            months = {\n",
    "                'Jan': '01', 'Feb': '02', 'Mar': '03', 'Apr': '04',\n",
    "                'May': '05', 'Jun': '06', 'Jul': '07', 'Aug': '08',\n",
    "                'Sep': '09', 'Oct': '10', 'Nov':  '11', 'Dec': '12'\n",
    "            }\n",
    "            month = months. get(month_str, '01')\n",
    "            year = f\"20{year}\"\n",
    "            return f\"{year}-{month}-{day}\"\n",
    "\n",
    "        return datetime.now().strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        return datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "def fetch_finviz_news(ticker):\n",
    "    \"\"\"\n",
    "    Fetch news for a single stock from Finviz.\n",
    "    \"\"\"\n",
    "    news_list = []\n",
    "\n",
    "    try:\n",
    "        url = f\"https://finviz.com/quote.ashx?t={ticker}\"\n",
    "        print(f\"  ğŸ“¡ Accessing:  {url}\")\n",
    "\n",
    "        response = requests.get(url, headers=HEADERS, timeout=15)\n",
    "        \n",
    "        if response. status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Finviz news table\n",
    "            news_table = soup.find('table', {'id':  'news-table'})\n",
    "\n",
    "            if news_table:\n",
    "                rows = news_table.find_all('tr')\n",
    "\n",
    "                current_date = None\n",
    "\n",
    "                for row in rows:\n",
    "                    try:\n",
    "                        cells = row.find_all('td')\n",
    "\n",
    "                        if len(cells) >= 2:\n",
    "                            # First column: date/time\n",
    "                            date_cell = cells[0]. text.strip()\n",
    "\n",
    "                            # Second column: news title and link\n",
    "                            title_elem = cells[1].find('a')\n",
    "\n",
    "                            if title_elem:\n",
    "                                title = title_elem. text.strip()\n",
    "                                link = title_elem. get('href', '')\n",
    "\n",
    "                                # Parse date\n",
    "                                parsed_date = parse_finviz_date(date_cell)\n",
    "\n",
    "                                # Extract source\n",
    "                                source_elem = cells[1].find('span')\n",
    "                                source = source_elem.text.strip() if source_elem else 'Unknown'\n",
    "                                \n",
    "                                news_list. append({\n",
    "                                    'ticker': ticker,\n",
    "                                    'date': parsed_date,\n",
    "                                    'time': date_cell,\n",
    "                                    'title': title,\n",
    "                                    'url': link,\n",
    "                                    'source': source,\n",
    "                                    'platform': 'finviz'\n",
    "                                })\n",
    "                    except Exception as e:\n",
    "                        continue\n",
    "\n",
    "                print(f\"  âœ… {ticker}:  Successfully fetched {len(news_list)} news items\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸ {ticker}: News table not found\")\n",
    "        else:\n",
    "            print(f\"  âŒ {ticker}: HTTP {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions. Timeout:\n",
    "        print(f\"  âŒ {ticker}:  Request timed out\")\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ {ticker}: {str(e)}\")\n",
    "\n",
    "    return news_list\n",
    "\n",
    "def fetch_all_tickers(tickers):\n",
    "    \"\"\"\n",
    "    Batch fetch news for all stocks.\n",
    "    \"\"\"\n",
    "    all_news = []\n",
    "    total = len(tickers)\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Start fetching news for {total} stocks\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "    for idx, ticker in enumerate(tickers, 1):\n",
    "        print(f\"[{idx}/{total}] Processing:  {ticker}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "        # Fetch news\n",
    "        news = fetch_finviz_news(ticker)\n",
    "        all_news.extend(news)\n",
    "\n",
    "        # Show progress\n",
    "        print(f\"  ğŸ“Š Current total: {len(all_news)} news items\\n\")\n",
    "\n",
    "        # Random delay (important! Avoid being blocked)\n",
    "        if idx < total:\n",
    "            delay = random.uniform(3, 7)\n",
    "            print(f\"  â³ Waiting {delay:.1f} seconds...\\n\")\n",
    "            time.sleep(delay)\n",
    "\n",
    "    return all_news\n",
    "\n",
    "def save_to_csv(news_list, output_file):\n",
    "    \"\"\"\n",
    "    Save data to CSV.\n",
    "    \"\"\"\n",
    "    if not news_list:\n",
    "        print(\"\\nâŒ No data to save\")\n",
    "        return None\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(news_list)\n",
    "\n",
    "    # Remove duplicates (by ticker and title)\n",
    "    original_count = len(df)\n",
    "    df = df.drop_duplicates(subset=['ticker', 'title'], keep='first')\n",
    "    removed_count = original_count - len(df)\n",
    "\n",
    "    # Sort by date\n",
    "    df = df.sort_values(['ticker', 'date'], ascending=[True, False])\n",
    "\n",
    "    # Save\n",
    "    df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "    # Statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ… Data saved to: {output_file}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"ğŸ“Š Data statistics:\")\n",
    "    print(f\"  - Original: {original_count} items\")\n",
    "    print(f\"  - After deduplication: {len(df)} items\")\n",
    "    print(f\"  - Duplicates removed: {removed_count} items\")\n",
    "\n",
    "    print(f\"\\nNews count per stock:\")\n",
    "    print(df['ticker'].value_counts().sort_index().to_string())\n",
    "\n",
    "    print(f\"\\nSource distribution:\")\n",
    "    if 'source' in df.columns:\n",
    "        print(df['source'].value_counts().head(10).to_string())\n",
    "\n",
    "    print(f\"\\nData preview (first 5 items):\")\n",
    "    print(df[['ticker', 'date', 'title']].head().to_string())\n",
    "\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"ğŸ“° Finviz News Scraper\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Target stocks: {', '.join(TICKERS)}\")\n",
    "    print(f\"Output file: {OUTPUT_FILE}\")\n",
    "    print(f\"Start time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Start fetching\n",
    "    start_time = time.time()\n",
    "    news_list = fetch_all_tickers(TICKERS)\n",
    "\n",
    "    # Save data\n",
    "    df = save_to_csv(news_list, OUTPUT_FILE)\n",
    "\n",
    "    # Done\n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"âœ… Fetch complete!\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"â±ï¸  Total time: {elapsed:.1f} seconds\")\n",
    "    print(f\"ğŸ“ File location: {OUTPUT_FILE}\")\n",
    "\n",
    "    if df is not None and len(df) < 1000:\n",
    "        print(f\"\\nâš ï¸  Note: Only {len(df)} items currently\")\n",
    "        print(f\"Suggestion: Run other scraper scripts to supplement data (Yahoo Finance, MarketWatch)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7924869",
   "metadata": {},
   "source": [
    "##### 1.3: Fetch US stock news from MarketWatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae44c973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MarketWatch æ–°é—»çˆ¬è™«ï¼ˆç®€åŒ–ç‰ˆï¼‰\n",
      "============================================================\n",
      "ç›®æ ‡è‚¡ç¥¨: AAPL, MSFT, GOOGL, NVDA, TSLA, JPM, BAC, GS, WMT, DIS\n",
      "ä¿å­˜æ–‡ä»¶: marketwatch_news_data.csv\n",
      "============================================================\n",
      "\n",
      "\n",
      "[1/10] æ­£åœ¨æŠ“å–: AAPL\n",
      "--------------------------------------------------\n",
      "  è®¿é—®: https://www.marketwatch.com/investing/stock/aapl\n",
      "  æˆåŠŸ:  AAPL è·å– 92 æ¡æ–°é—»\n",
      "  å½“å‰æ€»è®¡: 92 æ¡æ–°é—»\n",
      "  ç­‰å¾… 7.1 ç§’...\n",
      "\n",
      "\n",
      "[2/10] æ­£åœ¨æŠ“å–: MSFT\n",
      "--------------------------------------------------\n",
      "  è®¿é—®: https://www.marketwatch.com/investing/stock/msft\n",
      "  æˆåŠŸ:  MSFT è·å– 92 æ¡æ–°é—»\n",
      "  å½“å‰æ€»è®¡: 184 æ¡æ–°é—»\n",
      "  ç­‰å¾… 7.2 ç§’...\n",
      "\n",
      "\n",
      "[3/10] æ­£åœ¨æŠ“å–: GOOGL\n",
      "--------------------------------------------------\n",
      "  è®¿é—®: https://www.marketwatch.com/investing/stock/googl\n",
      "  æˆåŠŸ:  GOOGL è·å– 92 æ¡æ–°é—»\n",
      "  å½“å‰æ€»è®¡: 276 æ¡æ–°é—»\n",
      "  ç­‰å¾… 5.1 ç§’...\n",
      "\n",
      "\n",
      "[4/10] æ­£åœ¨æŠ“å–: NVDA\n",
      "--------------------------------------------------\n",
      "  è®¿é—®: https://www.marketwatch.com/investing/stock/nvda\n",
      "  æˆåŠŸ:  NVDA è·å– 92 æ¡æ–°é—»\n",
      "  å½“å‰æ€»è®¡: 368 æ¡æ–°é—»\n",
      "  ç­‰å¾… 8.1 ç§’...\n",
      "\n",
      "\n",
      "[5/10] æ­£åœ¨æŠ“å–: TSLA\n",
      "--------------------------------------------------\n",
      "  è®¿é—®: https://www.marketwatch.com/investing/stock/tsla\n",
      "  æˆåŠŸ:  TSLA è·å– 92 æ¡æ–°é—»\n",
      "  å½“å‰æ€»è®¡: 460 æ¡æ–°é—»\n",
      "  ç­‰å¾… 6.3 ç§’...\n",
      "\n",
      "\n",
      "[6/10] æ­£åœ¨æŠ“å–: JPM\n",
      "--------------------------------------------------\n",
      "  è®¿é—®: https://www.marketwatch.com/investing/stock/jpm\n",
      "  æˆåŠŸ:  JPM è·å– 92 æ¡æ–°é—»\n",
      "  å½“å‰æ€»è®¡: 552 æ¡æ–°é—»\n",
      "  ç­‰å¾… 5.5 ç§’...\n",
      "\n",
      "\n",
      "[7/10] æ­£åœ¨æŠ“å–: BAC\n",
      "--------------------------------------------------\n",
      "  è®¿é—®: https://www.marketwatch.com/investing/stock/bac\n",
      "  æˆåŠŸ:  BAC è·å– 92 æ¡æ–°é—»\n",
      "  å½“å‰æ€»è®¡: 644 æ¡æ–°é—»\n",
      "  ç­‰å¾… 5.8 ç§’...\n",
      "\n",
      "\n",
      "[8/10] æ­£åœ¨æŠ“å–: GS\n",
      "--------------------------------------------------\n",
      "  è®¿é—®: https://www.marketwatch.com/investing/stock/gs\n",
      "  æˆåŠŸ:  GS è·å– 92 æ¡æ–°é—»\n",
      "  å½“å‰æ€»è®¡: 736 æ¡æ–°é—»\n",
      "  ç­‰å¾… 9.9 ç§’...\n",
      "\n",
      "\n",
      "[9/10] æ­£åœ¨æŠ“å–: WMT\n",
      "--------------------------------------------------\n",
      "  è®¿é—®: https://www.marketwatch.com/investing/stock/wmt\n",
      "  æˆåŠŸ:  WMT è·å– 92 æ¡æ–°é—»\n",
      "  å½“å‰æ€»è®¡: 828 æ¡æ–°é—»\n",
      "  ç­‰å¾… 6.9 ç§’...\n",
      "\n",
      "\n",
      "[10/10] æ­£åœ¨æŠ“å–: DIS\n",
      "--------------------------------------------------\n",
      "  è®¿é—®: https://www.marketwatch.com/investing/stock/dis\n",
      "  æˆåŠŸ:  DIS è·å– 92 æ¡æ–°é—»\n",
      "  å½“å‰æ€»è®¡: 920 æ¡æ–°é—»\n",
      "\n",
      "============================================================\n",
      "æ•°æ®å·²ä¿å­˜: marketwatch_news_data.csv\n",
      "============================================================\n",
      "åŸå§‹æ•°æ®: 920 æ¡\n",
      "å»é‡å: 410 æ¡\n",
      "\n",
      "å„è‚¡ç¥¨æ–°é—»æ•°é‡:\n",
      "ticker\n",
      "AAPL     41\n",
      "BAC      41\n",
      "DIS      41\n",
      "GOOGL    41\n",
      "GS       41\n",
      "JPM      41\n",
      "MSFT     41\n",
      "NVDA     41\n",
      "TSLA     41\n",
      "WMT      41\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "æ•°æ®æ±‡æ€»:\n",
      "============================================================\n",
      "  Finviz:         996 æ¡\n",
      "  MarketWatch:   410 æ¡\n",
      "  æ€»è®¡:          1406 æ¡\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# step2d_scrape_marketwatch_simple.py\n",
    "\n",
    "\"\"\"\n",
    "Simplified MarketWatch scraper (only uses MarketWatch, removes Google).\n",
    "Focuses on stability and avoids formatting errors.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Configuration\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'TSLA', 'JPM', 'BAC', 'GS', 'WMT', 'DIS']\n",
    "OUTPUT_FILE = 'marketwatch_news_data.csv'\n",
    "\n",
    "# Request headers\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Connection': 'keep-alive',\n",
    "}\n",
    "\n",
    "def fetch_marketwatch_news(ticker):\n",
    "    \"\"\"\n",
    "    Fetch news from MarketWatch\n",
    "    \"\"\"\n",
    "    news_list = []\n",
    "\n",
    "    try:\n",
    "        base_url = \"https://www.marketwatch.com/investing/stock/\" + ticker.lower()\n",
    "\n",
    "        print(\"  Visiting:\", base_url)\n",
    "        response = requests.get(base_url, headers=HEADERS, timeout=15)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Find news articles\n",
    "            articles = soup.find_all('div', class_=re.compile('article__content|element--article'))\n",
    "\n",
    "            if not articles:\n",
    "                articles = soup.find_all('article')\n",
    "\n",
    "            if not articles:\n",
    "                news_section = soup.find('div', class_=re.compile('latest|news'))\n",
    "                if news_section:\n",
    "                    articles = news_section.find_all('a', href=re.compile('/story/'))\n",
    "\n",
    "            for article in articles[:100]:\n",
    "                try:\n",
    "                    title_elem = article.find('h3') or article.find('h2') or article\n",
    "                    if title_elem:\n",
    "                        title = title_elem.get_text(strip=True)\n",
    "\n",
    "                        link_elem = article.find('a') or title_elem.find('a')\n",
    "                        if link_elem and link_elem.get('href'):\n",
    "                            url = link_elem['href']\n",
    "                            if url.startswith('/'):\n",
    "                                url = \"https://www.marketwatch.com\" + url\n",
    "\n",
    "                            # Try to get date\n",
    "                            time_elem = article.find('time') or article.find('span', class_=re.compile('time|date'))\n",
    "                            if time_elem:\n",
    "                                date_str = time_elem.get('datetime', '') or time_elem.get_text(strip=True)\n",
    "                                try:\n",
    "                                    if 'T' in date_str:\n",
    "                                        date_obj = datetime.fromisoformat(date_str. replace('Z', '+00:00'))\n",
    "                                        formatted_date = date_obj.strftime('%Y-%m-%d')\n",
    "                                    else:\n",
    "                                        formatted_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                                except:\n",
    "                                    formatted_date = datetime.now().strftime('%Y-%m-%d')\n",
    "                            else:\n",
    "                                formatted_date = datetime.now().strftime('%Y-%m-%d')\n",
    "\n",
    "                            news_list.append({\n",
    "                                'ticker': ticker,\n",
    "                                'date': formatted_date,\n",
    "                                'time': '',\n",
    "                                'title': title,\n",
    "                                'url': url,\n",
    "                                'source': 'MarketWatch',\n",
    "                                'platform': 'marketwatch'\n",
    "                            })\n",
    "                except:\n",
    "                    continue\n",
    "\n",
    "            print(\"  Success:  \" + ticker + \" fetched \" + str(len(news_list)) + \" news items\")\n",
    "        else:\n",
    "            print(\"  Failed: \" + ticker + \" HTTP \" + str(response.status_code))\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"  Error:  \" + ticker + \" - \" + str(e))\n",
    "\n",
    "    return news_list\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    \"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"MarketWatch News Scraper (Simplified)\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Target stocks:\", \", \".join(TICKERS))\n",
    "    print(\"Output file:\", OUTPUT_FILE)\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    all_news = []\n",
    "    total = len(TICKERS)\n",
    "\n",
    "    for idx, ticker in enumerate(TICKERS, 1):\n",
    "        print()\n",
    "        print(\"[\" + str(idx) + \"/\" + str(total) + \"] Fetching:\", ticker)\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        news = fetch_marketwatch_news(ticker)\n",
    "        all_news.extend(news)\n",
    "\n",
    "        print(\"  Current total:\", len(all_news), \"news items\")\n",
    "\n",
    "        # Delay (use string concatenation to avoid formatting issues)\n",
    "        if idx < total:\n",
    "            delay = random. uniform(5, 10)\n",
    "            delay_str = str(round(delay, 1))\n",
    "            print(\"  Waiting \" + delay_str + \" seconds...\")\n",
    "            print()\n",
    "            time.sleep(delay)\n",
    "\n",
    "    # Save data\n",
    "    if all_news:\n",
    "        df = pd.DataFrame(all_news)\n",
    "\n",
    "        # Remove duplicates\n",
    "        original_count = len(df)\n",
    "        df = df.drop_duplicates(subset=['ticker', 'title'], keep='first')\n",
    "\n",
    "        # Save\n",
    "        df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Data saved:\", OUTPUT_FILE)\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Original data:\", original_count, \"items\")\n",
    "        print(\"After deduplication:\", len(df), \"items\")\n",
    "        print()\n",
    "        print(\"News count per stock:\")\n",
    "        print(df['ticker'].value_counts().sort_index())\n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "        print(\"Data summary:\")\n",
    "        print(\"=\" * 60)\n",
    "        print(\"  Finviz:         996 items\")\n",
    "        print(\"  MarketWatch:  \", len(df), \"items\")\n",
    "        print(\"  Total:         \", 996 + len(df), \"items\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        if 996 + len(df) >= 1500:\n",
    "            print()\n",
    "            print(\"Congratulations! Enough data, you can proceed to the next step!\")\n",
    "    else:\n",
    "        print()\n",
    "        print(\"No data fetched\")\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1b723d",
   "metadata": {},
   "source": [
    "##### 1.4: Fetch stock news via Alpha Vantage API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efa7889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Œ Alpha Vantage æ–°é—» API\n",
      "\n",
      "ä¼˜åŠ¿ï¼š\n",
      "  âœ… å®˜æ–¹ APIï¼Œç¨³å®šå¯é \n",
      "  âœ… ä¸éœ€è¦çˆ¬è™«\n",
      "  âœ… æ•°æ®è´¨é‡é«˜\n",
      "\n",
      "é™åˆ¶ï¼š\n",
      "  âš ï¸  å…è´¹ç‰ˆ:  æ¯åˆ†é’Ÿ 5 æ¬¡è¯·æ±‚ï¼Œæ¯å¤© 500 æ¬¡\n",
      "  âš ï¸  10 åªè‚¡ç¥¨éœ€è¦çº¦ 3-4 åˆ†é’Ÿ\n",
      "\n",
      "============================================================\n",
      "Alpha Vantage æ–°é—» API\n",
      "============================================================\n",
      "\n",
      "âœ… API Key å·²è®¾ç½®\n",
      "\n",
      "============================================================\n",
      "æ—¶é—´èŒƒå›´è®¾ç½®\n",
      "============================================================\n",
      "\n",
      "é€‰é¡¹ï¼š\n",
      "  1. è·å–æ‰€æœ‰å¯ç”¨çš„æ–°é—»ï¼ˆä¸é™æ—¶é—´ï¼‰\n",
      "  2. æŒ‡å®šæ—¶é—´èŒƒå›´ï¼ˆæ¨èï¼š2023-2024å¹´ï¼‰\n",
      "\n",
      "âœ… å°†è·å–æ‰€æœ‰å¯ç”¨çš„æ–°é—»\n",
      "\n",
      "\n",
      "============================================================\n",
      "å¼€å§‹è·å–æ–°é—»...\n",
      "============================================================\n",
      "è‚¡ç¥¨æ•°é‡: 10\n",
      "============================================================\n",
      "\n",
      "[1/10] AAPL\n",
      "--------------------------------------------------\n",
      "  æ­£åœ¨è·å– AAPL çš„æ–°é—»...\n",
      "  âœ… AAPL:   1000 æ¡\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 1000 æ¡\n",
      "\n",
      "  â³ ç­‰å¾… 15 ç§’ï¼ˆAPI é™åˆ¶ï¼‰...\n",
      "\n",
      "[2/10] MSFT\n",
      "--------------------------------------------------\n",
      "  æ­£åœ¨è·å– MSFT çš„æ–°é—»...\n",
      "  âœ… MSFT:   1000 æ¡\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 2000 æ¡\n",
      "\n",
      "  â³ ç­‰å¾… 15 ç§’ï¼ˆAPI é™åˆ¶ï¼‰...\n",
      "\n",
      "[3/10] GOOGL\n",
      "--------------------------------------------------\n",
      "  æ­£åœ¨è·å– GOOGL çš„æ–°é—»...\n",
      "  âœ… GOOGL:   1000 æ¡\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 3000 æ¡\n",
      "\n",
      "  â³ ç­‰å¾… 15 ç§’ï¼ˆAPI é™åˆ¶ï¼‰...\n",
      "\n",
      "[4/10] NVDA\n",
      "--------------------------------------------------\n",
      "  æ­£åœ¨è·å– NVDA çš„æ–°é—»...\n",
      "  âœ… NVDA:   1000 æ¡\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 4000 æ¡\n",
      "\n",
      "  â³ ç­‰å¾… 15 ç§’ï¼ˆAPI é™åˆ¶ï¼‰...\n",
      "\n",
      "[5/10] TSLA\n",
      "--------------------------------------------------\n",
      "  æ­£åœ¨è·å– TSLA çš„æ–°é—»...\n",
      "  âœ… TSLA:   1000 æ¡\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 5000 æ¡\n",
      "\n",
      "  â³ ç­‰å¾… 15 ç§’ï¼ˆAPI é™åˆ¶ï¼‰...\n",
      "\n",
      "[6/10] JPM\n",
      "--------------------------------------------------\n",
      "  æ­£åœ¨è·å– JPM çš„æ–°é—»...\n",
      "  âœ… JPM:   1000 æ¡\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 6000 æ¡\n",
      "\n",
      "  â³ ç­‰å¾… 15 ç§’ï¼ˆAPI é™åˆ¶ï¼‰...\n",
      "\n",
      "[7/10] BAC\n",
      "--------------------------------------------------\n",
      "  æ­£åœ¨è·å– BAC çš„æ–°é—»...\n",
      "  âœ… BAC:   1000 æ¡\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 7000 æ¡\n",
      "\n",
      "  â³ ç­‰å¾… 15 ç§’ï¼ˆAPI é™åˆ¶ï¼‰...\n",
      "\n",
      "[8/10] GS\n",
      "--------------------------------------------------\n",
      "  æ­£åœ¨è·å– GS çš„æ–°é—»...\n",
      "  âœ… GS:   1000 æ¡\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 8000 æ¡\n",
      "\n",
      "  â³ ç­‰å¾… 15 ç§’ï¼ˆAPI é™åˆ¶ï¼‰...\n",
      "\n",
      "[9/10] WMT\n",
      "--------------------------------------------------\n",
      "  æ­£åœ¨è·å– WMT çš„æ–°é—»...\n",
      "  âœ… WMT:   1000 æ¡\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 9000 æ¡\n",
      "\n",
      "  â³ ç­‰å¾… 15 ç§’ï¼ˆAPI é™åˆ¶ï¼‰...\n",
      "\n",
      "[10/10] DIS\n",
      "--------------------------------------------------\n",
      "  æ­£åœ¨è·å– DIS çš„æ–°é—»...\n",
      "  âœ… DIS:   1000 æ¡\n",
      "  ğŸ“Š å½“å‰æ€»è®¡: 10000 æ¡\n",
      "\n",
      "\n",
      "============================================================\n",
      "âœ… å®Œæˆï¼\n",
      "============================================================\n",
      "åŸå§‹æ•°æ®:   10000 æ¡\n",
      "å»é‡å: 9390 æ¡\n",
      "ä¿å­˜åˆ°: alphavantage_news.csv\n",
      "\n",
      "å„è‚¡ç¥¨åˆ†å¸ƒ:\n",
      "ticker\n",
      "AAPL     930\n",
      "BAC      945\n",
      "DIS      973\n",
      "GOOGL    944\n",
      "GS       912\n",
      "JPM      946\n",
      "MSFT     948\n",
      "NVDA     960\n",
      "TSLA     881\n",
      "WMT      951\n",
      "Name: count, dtype: int64\n",
      "\n",
      "æ—¥æœŸèŒƒå›´:\n",
      "  2024-02-29 00:00:00 åˆ° 2025-12-13 00:00:00\n",
      "\n",
      "2023-2024å¹´çš„æ–°é—»:  102 æ¡\n",
      "\n",
      "ğŸ’¡ æç¤º:\n",
      "  å¦‚æœå†å²æ•°æ®è¾ƒå°‘ï¼ŒAlpha Vantage ä¸»è¦æä¾›è¿‘æœŸæ–°é—»\n",
      "  ä½†æ•°æ®è´¨é‡å¾ˆé«˜ï¼Œé€‚åˆè®­ç»ƒä½¿ç”¨\n"
     ]
    }
   ],
   "source": [
    "# step2_alphavantage_news.py\n",
    "\n",
    "\"\"\"\n",
    "Fetch stock news using Alpha Vantage API.\n",
    "Official API, stable and reliable.\n",
    "\"\"\"\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Configuration\n",
    "API_KEY = 'R6F8B9TXFSLDCL3Z'  # Your Alpha Vantage API Key\n",
    "TICKERS = ['AAPL', 'MSFT', 'GOOGL', 'NVDA', 'TSLA', 'JPM', 'BAC', 'GS', 'WMT', 'DIS']\n",
    "OUTPUT_FILE = 'alphavantage_news.csv'\n",
    "\n",
    "# Alpha Vantage free tier limits\n",
    "# 5 requests per minute, 500 requests per day\n",
    "\n",
    "def check_api_key():\n",
    "    \"\"\"Check if API Key is valid\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Alpha Vantage News API\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    if API_KEY == 'YOUR_API_KEY_HERE':\n",
    "        print(\"âŒ Please set your API Key first!\")\n",
    "        print()\n",
    "        print(\"Steps:\")\n",
    "        print(\"1. If you already have an API Key, enter it at line 12 of the script\")\n",
    "        print(\"2. If not, visit:  https://www.alphavantage.co/support/#api-key\")\n",
    "        print(\"   (Free, no credit card required)\")\n",
    "        print()\n",
    "        return False\n",
    "\n",
    "    print(\"âœ… API Key is set\")\n",
    "    return True\n",
    "\n",
    "def fetch_news_alphavantage(ticker, api_key, time_from=None, time_to=None):\n",
    "    \"\"\"\n",
    "    Fetch news from Alpha Vantage.\n",
    "    \n",
    "    API docs:   https://www.alphavantage.co/documentation/#news-sentiment\n",
    "    \"\"\"\n",
    "    news_list = []\n",
    "\n",
    "    try: \n",
    "        # Alpha Vantage News & Sentiment API\n",
    "        url = \"https://www.alphavantage.co/query\"\n",
    "\n",
    "        params = {\n",
    "            'function': 'NEWS_SENTIMENT',\n",
    "            'tickers': ticker,\n",
    "            'apikey': api_key,\n",
    "            'limit': 1000,  # Up to 1000 items (actual may be less)\n",
    "        }\n",
    "\n",
    "        # If time range specified\n",
    "        if time_from: \n",
    "            params['time_from'] = time_from  # Format:  20230101T0000\n",
    "        if time_to:\n",
    "            params['time_to'] = time_to\n",
    "\n",
    "        print(f\"  Fetching news for {ticker}...\")\n",
    "\n",
    "        response = requests.get(url, params=params, timeout=30)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "\n",
    "            # Check for error message\n",
    "            if 'Error Message' in data:\n",
    "                print(f\"  âŒ API Error: {data['Error Message']}\")\n",
    "                return news_list\n",
    "\n",
    "            if 'Note' in data: \n",
    "                print(f\"  âš ï¸  API Limit: {data['Note']}\")\n",
    "                print(\"     (Free tier: max 5 requests per minute)\")\n",
    "                return news_list\n",
    "\n",
    "            # Extract news\n",
    "            if 'feed' in data: \n",
    "                articles = data['feed']\n",
    "\n",
    "                for article in articles: \n",
    "                    try:\n",
    "                        # Extract info\n",
    "                        title = article.get('title', '')\n",
    "                        url_link = article.get('url', '')\n",
    "                        time_published = article.get('time_published', '')\n",
    "                        source = article.get('source', '')\n",
    "\n",
    "                        # Convert time format\n",
    "                        # Alpha Vantage format: 20231215T120000\n",
    "                        if time_published:\n",
    "                            try:\n",
    "                                date_obj = datetime.strptime(time_published, '%Y%m%dT%H%M%S')\n",
    "                                formatted_date = date_obj. strftime('%Y-%m-%d')\n",
    "                                formatted_time = date_obj.strftime('%H:%M:%S')\n",
    "                            except:\n",
    "                                formatted_date = time_published[: 8]  # First 8 chars as date\n",
    "                                formatted_time = ''\n",
    "                        else:\n",
    "                            formatted_date = None\n",
    "                            formatted_time = ''\n",
    "\n",
    "                        if title and formatted_date:\n",
    "                            news_list.append({\n",
    "                                'ticker': ticker,\n",
    "                                'date': formatted_date,\n",
    "                                'time': formatted_time,\n",
    "                                'title': title,\n",
    "                                'url': url_link,\n",
    "                                'source': source,\n",
    "                                'platform': 'alphavantage'\n",
    "                            })\n",
    "\n",
    "                    except Exception as e: \n",
    "                        continue\n",
    "\n",
    "                print(f\"  âœ… {ticker}:   {len(news_list)} items\")\n",
    "            else:\n",
    "                print(f\"  âš ï¸  {ticker}:  No news data found\")\n",
    "\n",
    "        else:\n",
    "            print(f\"  âŒ HTTP {response.status_code}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  âŒ Error: {e}\")\n",
    "\n",
    "    return news_list\n",
    "\n",
    "def fetch_all_tickers(tickers, api_key, time_from=None, time_to=None):\n",
    "    \"\"\"\n",
    "    Fetch news for all stocks.\n",
    "    \"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Start fetching news...\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Number of stocks: {len(tickers)}\")\n",
    "    if time_from:\n",
    "        print(f\"Time range: {time_from} to {time_to}\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    all_news = []\n",
    "\n",
    "    for idx, ticker in enumerate(tickers, 1):\n",
    "        print(f\"[{idx}/{len(tickers)}] {ticker}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "        news = fetch_news_alphavantage(ticker, api_key, time_from, time_to)\n",
    "        all_news.extend(news)\n",
    "\n",
    "        print(f\"  ğŸ“Š Current total: {len(all_news)} items\")\n",
    "        print()\n",
    "\n",
    "        # Alpha Vantage free tier: max 5 requests per minute\n",
    "        # So wait at least 12 seconds after each request\n",
    "        if idx < len(tickers):\n",
    "            delay = 15  # Wait 15 seconds, safe\n",
    "            print(f\"  â³ Waiting {delay} seconds (API limit)...\")\n",
    "            time.sleep(delay)\n",
    "            print()\n",
    "\n",
    "    return all_news\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print()\n",
    "    print(\"ğŸ“Œ Alpha Vantage News API\")\n",
    "    print()\n",
    "    print(\"Advantages:\")\n",
    "    print(\"  âœ… Official API, stable and reliable\")\n",
    "    print(\"  âœ… No need for web scraping\")\n",
    "    print(\"  âœ… High data quality\")\n",
    "    print()\n",
    "    print(\"Limits:\")\n",
    "    print(\"  âš ï¸  Free tier: 5 requests per minute, 500 per day\")\n",
    "    print(\"  âš ï¸  10 stocks take about 3-4 minutes\")\n",
    "    print()\n",
    "\n",
    "    # Check API Key\n",
    "    if not check_api_key():\n",
    "        return\n",
    "\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Time range settings\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(\"Options:\")\n",
    "    print(\"  1. Fetch all available news (no time limit)\")\n",
    "    print(\"  2. Specify time range (recommended: 2023-2024)\")\n",
    "    print()\n",
    "\n",
    "    choice = input(\"Please choose (1 or 2): \").strip()\n",
    "\n",
    "    time_from = None\n",
    "    time_to = None\n",
    "\n",
    "    if choice == '2':\n",
    "        # Specify time range\n",
    "        # Alpha Vantage format:   YYYYMMDDTHHMM\n",
    "        time_from = '20230101T0000'  # Jan 1, 2023\n",
    "        time_to = '20241231T2359'    # Dec 31, 2024\n",
    "        print(f\"âœ… Will fetch news from 2023-01-01 to 2024-12-31\")\n",
    "    else:\n",
    "        print(\"âœ… Will fetch all available news\")\n",
    "\n",
    "    print()\n",
    "    input(\"Press Enter to start...\")\n",
    "\n",
    "    # Start fetching\n",
    "    all_news = fetch_all_tickers(TICKERS, API_KEY, time_from, time_to)\n",
    "\n",
    "    # Save data\n",
    "    if all_news:\n",
    "        df = pd.DataFrame(all_news)\n",
    "\n",
    "        # Remove duplicates\n",
    "        original_count = len(df)\n",
    "        df = df.drop_duplicates(subset=['ticker', 'title'], keep='first')\n",
    "\n",
    "        # Save\n",
    "        df.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        print()\n",
    "        print(\"=\" * 60)\n",
    "        print(\"âœ… Done!\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Original data:   {original_count} items\")\n",
    "        print(f\"After deduplication: {len(df)} items\")\n",
    "        print(f\"Saved to: {OUTPUT_FILE}\")\n",
    "        print()\n",
    "        print(\"Stock distribution:\")\n",
    "        print(df['ticker'].value_counts().sort_index())\n",
    "        print()\n",
    "        print(\"Date range:\")\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        print(f\"  {df['date'].min()} to {df['date'].max()}\")\n",
    "\n",
    "        # Stats for 2023-2024\n",
    "        df_2023_2024 = df[(df['date'] >= '2023-01-01') & (df['date'] <= '2024-12-31')]\n",
    "        print()\n",
    "        print(f\"News in 2023-2024:  {len(df_2023_2024)} items\")\n",
    "\n",
    "        if len(df_2023_2024) < 500:\n",
    "            print()\n",
    "            print(\"ğŸ’¡ Note:\")\n",
    "            print(\"  If there is little historical data, Alpha Vantage mainly provides recent news\")\n",
    "            print(\"  But the data quality is high, suitable for training\")\n",
    "\n",
    "    else:\n",
    "        print()\n",
    "        print(\"âŒ No data fetched\")\n",
    "        print()\n",
    "        print(\"Possible reasons:\")\n",
    "        print(\"  1. Invalid API Key\")\n",
    "        print(\"  2. Daily request limit reached\")\n",
    "        print(\"  3. Network issues\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35b46d",
   "metadata": {},
   "source": [
    "##### 1.5: Merge the three datasets and data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e34857b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“° åˆå¹¶æ‰€æœ‰æ–°é—»æ•°æ®æº\n",
      "\n",
      "å°†åˆå¹¶:\n",
      "  â€¢ Finviz\n",
      "  â€¢ MarketWatch\n",
      "  â€¢ Alpha Vantage\n",
      "\n",
      "============================================================\n",
      "åˆå¹¶æ‰€æœ‰æ–°é—»æ•°æ®\n",
      "============================================================\n",
      "\n",
      "åŠ è½½ Finviz æ•°æ®...\n",
      "âœ… Finviz:  996 æ¡\n",
      "åŠ è½½ MarketWatch æ•°æ®...\n",
      "âœ… MarketWatch:  410 æ¡\n",
      "åŠ è½½ Alpha Vantage æ•°æ®...\n",
      "âœ… Alpha Vantage: 9390 æ¡\n",
      "\n",
      "============================================================\n",
      "åˆå¹¶æ•°æ®...\n",
      "============================================================\n",
      "åˆå¹¶å: 10796 æ¡\n",
      "\n",
      "============================================================\n",
      "æ¸…æ´—æ•°æ®...\n",
      "============================================================\n",
      "\n",
      "åŸå§‹æ•°æ®:  10796 æ¡\n",
      "ç§»é™¤ç©ºæ ‡é¢˜: 10786 æ¡\n",
      "ç§»é™¤è¿‡çŸ­æ ‡é¢˜: 10783 æ¡\n",
      "å»é‡:  10598 æ¡ (åˆ é™¤äº† 185 æ¡é‡å¤)\n",
      "å¤„ç†æ—¥æœŸ: 10598 æ¡\n",
      "\n",
      "============================================================\n",
      "æ•°æ®ç»Ÿè®¡\n",
      "============================================================\n",
      "\n",
      "æœ€ç»ˆæ•°æ®é‡: 10598 æ¡\n",
      "\n",
      "æ—¥æœŸèŒƒå›´:\n",
      "  æœ€æ—©: 2024-02-29 00:00:00\n",
      "  æœ€æ™š: 2025-12-13 00:00:00\n",
      "  è·¨åº¦: 653 å¤©\n",
      "\n",
      "æ•°æ®æ¥æºåˆ†å¸ƒ:\n",
      "  alphavantage: 9294 æ¡ (87.7%)\n",
      "  finviz: 996 æ¡ (9.4%)\n",
      "  marketwatch: 308 æ¡ (2.9%)\n",
      "\n",
      "å„è‚¡ç¥¨æ–°é—»æ•°é‡:\n",
      "  AAPL: 1060 æ¡\n",
      "  BAC: 1053 æ¡\n",
      "  DIS: 1099 æ¡\n",
      "  GOOGL: 1071 æ¡\n",
      "  GS: 1015 æ¡\n",
      "  JPM: 1065 æ¡\n",
      "  MSFT: 1071 æ¡\n",
      "  NVDA: 1091 æ¡\n",
      "  TSLA: 1003 æ¡\n",
      "  WMT: 1070 æ¡\n",
      "\n",
      "æ•°æ®é¢„è§ˆï¼ˆå‰5æ¡ï¼‰:\n",
      "------------------------------------------------------------\n",
      "1. AAPL | 2025-12-13\n",
      "   These 3 Warren Buffett AI Stocks Could Be Big Winners in 202...\n",
      "\n",
      "2. AAPL | 2025-12-13\n",
      "   Here's My Top \"Magnificent Seven\" Stock to Buy for 2026...\n",
      "\n",
      "3. AAPL | 2025-12-13\n",
      "   My Tune Is Changing On Apple In 2026: Heres Why...\n",
      "\n",
      "4. AAPL | 2025-12-13\n",
      "   The Best Dividend ETF for Right Now-and 2026...\n",
      "\n",
      "5. AAPL | 2025-12-13\n",
      "   Apple Wins Partial Reversal of Sanctions in Epic Games Antit...\n",
      "\n",
      "============================================================\n",
      "ä¿å­˜æ•°æ®...\n",
      "============================================================\n",
      "âœ… æ•°æ®å·²ä¿å­˜åˆ°:  combined_clean_news_all.csv\n",
      "   æ€»æ¡æ•°: 10598\n",
      "   æ–‡ä»¶å¤§å°: 3349.19 KB\n",
      "\n",
      "============================================================\n",
      "âœ… ç¬¬ä¸‰æ­¥Aå®Œæˆï¼\n",
      "============================================================\n",
      "\n",
      "ç”Ÿæˆæ–‡ä»¶: combined_clean_news_all.csv\n",
      "æ•°æ®æ¡æ•°: 10598\n",
      "\n",
      "ä¸‹ä¸€æ­¥:  è¿è¡Œ step3b_match_prices_2025.py\n",
      "       åŒ¹é…ä»·æ ¼å¹¶ç”Ÿæˆè®­ç»ƒæ•°æ®\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step3a_merge_all_news.py\n",
    "\n",
    "\"\"\"\n",
    "Step 3A (Updated): Merge all news data.\n",
    "Includes: Finviz + MarketWatch + Alpha Vantage\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "FINVIZ_FILE = 'finviz_news_data.csv'\n",
    "MARKETWATCH_FILE = 'marketwatch_news_data.csv'\n",
    "ALPHAVANTAGE_FILE = 'alphavantage_news.csv'\n",
    "OUTPUT_FILE = 'combined_clean_news_all.csv'\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Clean text\"\"\"\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s\\.,!?;:\\-\\'\\\"\\(\\)%$]', '', text)\n",
    "    return text.strip()\n",
    "\n",
    "def load_and_merge_news():\n",
    "    \"\"\"Load and merge all news data\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Merge all news data\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    all_dataframes = []\n",
    "\n",
    "    # 1. Load Finviz\n",
    "    if os.path.exists(FINVIZ_FILE):\n",
    "        print(\"Loading Finviz data...\")\n",
    "        try:\n",
    "            df_finviz = pd.read_csv(FINVIZ_FILE, encoding='utf-8-sig')\n",
    "            print(f\"âœ… Finviz:  {len(df_finviz)} items\")\n",
    "            all_dataframes.append(df_finviz)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Finviz load failed: {e}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Not found:\", FINVIZ_FILE)\n",
    "\n",
    "    # 2. Load MarketWatch\n",
    "    if os.path.exists(MARKETWATCH_FILE):\n",
    "        print(\"Loading MarketWatch data...\")\n",
    "        try:\n",
    "            df_marketwatch = pd.read_csv(MARKETWATCH_FILE, encoding='utf-8-sig')\n",
    "            print(f\"âœ… MarketWatch:  {len(df_marketwatch)} items\")\n",
    "            all_dataframes.append(df_marketwatch)\n",
    "        except Exception as e: \n",
    "            print(f\"âŒ MarketWatch load failed: {e}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Not found:\", MARKETWATCH_FILE)\n",
    "\n",
    "    # 3. Load Alpha Vantage\n",
    "    if os.path.exists(ALPHAVANTAGE_FILE):\n",
    "        print(\"Loading Alpha Vantage data...\")\n",
    "        try:\n",
    "            df_alpha = pd.read_csv(ALPHAVANTAGE_FILE, encoding='utf-8-sig')\n",
    "            print(f\"âœ… Alpha Vantage: {len(df_alpha)} items\")\n",
    "            all_dataframes.append(df_alpha)\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Alpha Vantage load failed:  {e}\")\n",
    "    else:\n",
    "        print(\"âš ï¸  Not found:\", ALPHAVANTAGE_FILE)\n",
    "\n",
    "    if len(all_dataframes) == 0:\n",
    "        print()\n",
    "        print(\"âŒ No data files available\")\n",
    "        return None\n",
    "\n",
    "    # Merge all data\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Merging data...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    df_combined = pd.concat(all_dataframes, ignore_index=True)\n",
    "    print(f\"After merge: {len(df_combined)} items\")\n",
    "\n",
    "    return df_combined\n",
    "\n",
    "def clean_news_data(df):\n",
    "    \"\"\"Clean news data\"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Cleaning data...\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    original_count = len(df)\n",
    "    print(f\"Original data:  {original_count} items\")\n",
    "\n",
    "    # 1. Remove empty titles\n",
    "    df = df[df['title'].notna()]\n",
    "    df = df[df['title'].str.strip() != '']\n",
    "    print(f\"After removing empty titles: {len(df)} items\")\n",
    "\n",
    "    # 2. Clean titles\n",
    "    df['title_clean'] = df['title'].apply(clean_text)\n",
    "\n",
    "    # 3. Remove short titles\n",
    "    df = df[df['title_clean'].str.len() >= 10]\n",
    "    print(f\"After removing short titles: {len(df)} items\")\n",
    "\n",
    "    # 4. Remove duplicates (by ticker + title)\n",
    "    before_dedup = len(df)\n",
    "    df = df.drop_duplicates(subset=['ticker', 'title_clean'], keep='first')\n",
    "    print(f\"After deduplication:  {len(df)} items (removed {before_dedup - len(df)} duplicates)\")\n",
    "\n",
    "    # 5. Process dates\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'], errors='coerce')\n",
    "        df = df[df['date'].notna()]\n",
    "        print(f\"After date processing: {len(df)} items\")\n",
    "\n",
    "        # Sort by date\n",
    "        df = df.sort_values(['ticker', 'date'], ascending=[True, False])\n",
    "\n",
    "    # 6. Reset index\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "def show_statistics(df):\n",
    "    \"\"\"Show statistics\"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Data statistics\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "\n",
    "    print(f\"Final data count: {len(df)} items\")\n",
    "    print()\n",
    "\n",
    "    # Date range\n",
    "    if 'date' in df.columns:\n",
    "        print(\"Date range:\")\n",
    "        print(f\"  Earliest: {df['date'].min()}\")\n",
    "        print(f\"  Latest: {df['date'].max()}\")\n",
    "        date_span = (df['date'].max() - df['date'].min()).days\n",
    "        print(f\"  Span: {date_span} days\")\n",
    "        print()\n",
    "\n",
    "    # Data source distribution\n",
    "    if 'platform' in df.columns:\n",
    "        print(\"Data source distribution:\")\n",
    "        platform_counts = df['platform'].value_counts()\n",
    "        for platform, count in platform_counts.items():\n",
    "            percentage = count / len(df) * 100\n",
    "            print(f\"  {platform}: {count} items ({round(percentage, 1)}%)\")\n",
    "        print()\n",
    "\n",
    "    # Distribution by stock\n",
    "    print(\"News count per stock:\")\n",
    "    ticker_counts = df['ticker'].value_counts().sort_index()\n",
    "    for ticker, count in ticker_counts.items():\n",
    "        print(f\"  {ticker}: {count} items\")\n",
    "    print()\n",
    "\n",
    "    # Data preview\n",
    "    print(\"Data preview (first 5 items):\")\n",
    "    print(\"-\" * 60)\n",
    "    for i in range(min(5, len(df))):\n",
    "        row = df.iloc[i]\n",
    "        date_str = str(row['date'])[:10] if 'date' in df.columns else 'N/A'\n",
    "        title_preview = row['title_clean'][:60] if 'title_clean' in df.columns else row['title'][:60]\n",
    "        print(f\"{i+1}. {row['ticker']} | {date_str}\")\n",
    "        print(f\"   {title_preview}...\")\n",
    "        print()\n",
    "\n",
    "def save_data(df, output_file):\n",
    "    \"\"\"Save data\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Saving data...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    try:\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "        # Calculate file size\n",
    "        file_size = os.path.getsize(output_file) / 1024\n",
    "\n",
    "        print(f\"âœ… Data saved to:  {output_file}\")\n",
    "        print(f\"   Total items: {len(df)}\")\n",
    "        print(f\"   File size: {round(file_size, 2)} KB\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Save failed: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print()\n",
    "    print(\"ğŸ“° Merge all news data sources\")\n",
    "    print()\n",
    "    print(\"Will merge:\")\n",
    "    print(\"  â€¢ Finviz\")\n",
    "    print(\"  â€¢ MarketWatch\")\n",
    "    print(\"  â€¢ Alpha Vantage\")\n",
    "    print()\n",
    "\n",
    "    # 1. Load and merge\n",
    "    df_combined = load_and_merge_news()\n",
    "\n",
    "    if df_combined is None: \n",
    "        return\n",
    "\n",
    "    # 2. Clean data\n",
    "    df_clean = clean_news_data(df_combined)\n",
    "\n",
    "    # 3. Show statistics\n",
    "    show_statistics(df_clean)\n",
    "\n",
    "    # 4. Save data\n",
    "    save_data(df_clean, OUTPUT_FILE)\n",
    "\n",
    "    # 5. Completion message\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ… Step 3A complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(f\"Generated file: {OUTPUT_FILE}\")\n",
    "    print(f\"Data count: {len(df_clean)}\")\n",
    "    print()\n",
    "    print(\"Next step:  Run step3b_match_prices_2025.py\")\n",
    "    print(\"           Match prices and generate training data\")\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e7c0f",
   "metadata": {},
   "source": [
    "##### 1.6: Generate training set (matching Nov-Dec 2025 news and price data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f2f53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ åŒ¹é…2025å¹´æ–°é—»å’Œä»·æ ¼æ•°æ®\n",
      "\n",
      "é…ç½®:\n",
      "  æ–°é—»æ–‡ä»¶: combined_clean_news_all.csv\n",
      "  ä»·æ ¼æ–‡ä»¶: all_stocks_prices.csv\n",
      "  è¾“å‡ºæ–‡ä»¶: training_data_2025.csv\n",
      "  é˜ˆå€¼: positive>1.0%, negative<-1.0%\n",
      "\n",
      "\n",
      "============================================================\n",
      "ç¬¬ä¸‰æ­¥Bï¼šåŒ¹é…2025å¹´æ–°é—»å’Œä»·æ ¼\n",
      "============================================================\n",
      "\n",
      "åŠ è½½æ–°é—»æ•°æ®...\n",
      "âœ… æ–°é—»æ•°æ®:  10598 æ¡\n",
      "   æ—¥æœŸèŒƒå›´: 2024-02-29 00:00:00 åˆ° 2025-12-13 00:00:00\n",
      "\n",
      "åŠ è½½ä»·æ ¼æ•°æ®...\n",
      "âœ… ä»·æ ¼æ•°æ®: 7400 æ¡\n",
      "   æ—¥æœŸèŒƒå›´: 2023-01-03 00:00:00 åˆ° 2025-12-12 00:00:00\n",
      "\n",
      "ç­›é€‰2025å¹´11-12æœˆçš„ä»·æ ¼æ•°æ®...\n",
      "âœ… 2025å¹´11-12æœˆä»·æ ¼:  290 æ¡\n",
      "\n",
      "============================================================\n",
      "å¼€å§‹åŒ¹é…ä»·æ ¼å¹¶æ ‡æ³¨...\n",
      "============================================================\n",
      "\n",
      "æ€»å…±éœ€è¦å¤„ç†:  10598 æ¡æ–°é—»\n",
      "\n",
      "è¿›åº¦: 500/10598 (4.7%) | å·²åŒ¹é…: 367 | é€Ÿåº¦:  169.5 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 59 ç§’\n",
      "è¿›åº¦: 1000/10598 (9.4%) | å·²åŒ¹é…: 835 | é€Ÿåº¦:  172.1 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 55 ç§’\n",
      "è¿›åº¦: 1500/10598 (14.2%) | å·²åŒ¹é…: 1150 | é€Ÿåº¦:  168.6 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 53 ç§’\n",
      "è¿›åº¦: 2000/10598 (18.9%) | å·²åŒ¹é…: 1650 | é€Ÿåº¦:  164.1 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 52 ç§’\n",
      "è¿›åº¦: 2500/10598 (23.6%) | å·²åŒ¹é…: 2030 | é€Ÿåº¦:  165.1 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 49 ç§’\n",
      "è¿›åº¦: 3000/10598 (28.3%) | å·²åŒ¹é…: 2060 | é€Ÿåº¦:  165.2 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 46 ç§’\n",
      "è¿›åº¦: 3500/10598 (33.0%) | å·²åŒ¹é…: 2216 | é€Ÿåº¦:  165.4 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 42 ç§’\n",
      "è¿›åº¦: 4000/10598 (37.7%) | å·²åŒ¹é…: 2716 | é€Ÿåº¦:  166.4 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 39 ç§’\n",
      "è¿›åº¦: 4500/10598 (42.5%) | å·²åŒ¹é…: 3104 | é€Ÿåº¦:  166.8 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 36 ç§’\n",
      "è¿›åº¦: 5000/10598 (47.2%) | å·²åŒ¹é…: 3604 | é€Ÿåº¦:  167.6 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 33 ç§’\n",
      "è¿›åº¦: 5500/10598 (51.9%) | å·²åŒ¹é…: 3962 | é€Ÿåº¦:  167.8 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 30 ç§’\n",
      "è¿›åº¦: 6000/10598 (56.6%) | å·²åŒ¹é…: 4462 | é€Ÿåº¦:  168.2 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 27 ç§’\n",
      "è¿›åº¦: 6500/10598 (61.3%) | å·²åŒ¹é…: 4826 | é€Ÿåº¦:  168.4 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 24 ç§’\n",
      "è¿›åº¦: 7000/10598 (66.1%) | å·²åŒ¹é…: 5326 | é€Ÿåº¦:  168.6 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 21 ç§’\n",
      "è¿›åº¦: 7500/10598 (70.8%) | å·²åŒ¹é…: 5761 | é€Ÿåº¦:  168.1 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 18 ç§’\n",
      "è¿›åº¦: 8000/10598 (75.5%) | å·²åŒ¹é…: 6188 | é€Ÿåº¦:  168.3 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 15 ç§’\n",
      "è¿›åº¦: 8500/10598 (80.2%) | å·²åŒ¹é…: 6688 | é€Ÿåº¦:  168.3 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 12 ç§’\n",
      "è¿›åº¦: 9000/10598 (84.9%) | å·²åŒ¹é…: 7060 | é€Ÿåº¦:  168.3 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 9 ç§’\n",
      "è¿›åº¦: 9500/10598 (89.6%) | å·²åŒ¹é…: 7498 | é€Ÿåº¦:  168.4 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 6 ç§’\n",
      "è¿›åº¦: 10000/10598 (94.4%) | å·²åŒ¹é…: 7848 | é€Ÿåº¦:  168.6 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 3 ç§’\n",
      "è¿›åº¦: 10500/10598 (99.1%) | å·²åŒ¹é…: 8348 | é€Ÿåº¦:  168.5 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 0 ç§’\n",
      "è¿›åº¦: 10598/10598 (100.0%) | å·²åŒ¹é…: 8377 | é€Ÿåº¦:  168.5 æ¡/ç§’ | é¢„è®¡å‰©ä½™: 0 ç§’\n",
      "\n",
      "============================================================\n",
      "åŒ¹é…å®Œæˆï¼\n",
      "============================================================\n",
      "æ€»æ•°æ®:  10598 æ¡\n",
      "æˆåŠŸåŒ¹é…: 8377 æ¡\n",
      "åŒ¹é…ç‡: 79.0%\n",
      "æ€»è€—æ—¶: 62.9ç§’\n",
      "\n",
      "============================================================\n",
      "æ•°æ®ç»Ÿè®¡\n",
      "============================================================\n",
      "\n",
      "æ€»æ•°æ®: 10598 æ¡\n",
      "æœ‰æ•ˆæ•°æ®: 8377 æ¡\n",
      "æ— æ•ˆæ•°æ®: 2221 æ¡\n",
      "\n",
      "æƒ…æ„Ÿæ ‡ç­¾åˆ†å¸ƒ:\n",
      "------------------------------------------------------------\n",
      "  positive  :  3020 æ¡ ( 36.1%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  negative  :  1983 æ¡ ( 23.7%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  neutral   :  3374 æ¡ ( 40.3%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "ä»·æ ¼å˜åŒ–ç»Ÿè®¡:\n",
      "  å¹³å‡å˜åŒ–: 0.14%\n",
      "  æ ‡å‡†å·®: 2.29%\n",
      "  æœ€å¤§æ¶¨å¹…: 6.82%\n",
      "  æœ€å¤§è·Œå¹…: -13.15%\n",
      "\n",
      "å„è‚¡ç¥¨æ•°æ®åˆ†å¸ƒ:\n",
      "------------------------------------------------------------\n",
      "è‚¡ç¥¨         Positive   Negative    Neutral      Total\n",
      "------------------------------------------------------------\n",
      "AAPL            202        126        507        835\n",
      "BAC             266         37        626        929\n",
      "DIS             118         33        145        296\n",
      "GOOGL           346        329        265        940\n",
      "GS              656        190         57        903\n",
      "JPM             264        171        488        923\n",
      "MSFT            275        275        385        935\n",
      "NVDA            276        323        354        953\n",
      "TSLA            359        240        185        784\n",
      "WMT             258        259        362        879\n",
      "\n",
      "============================================================\n",
      "å‡†å¤‡è®­ç»ƒæ•°æ®...\n",
      "============================================================\n",
      "\n",
      "âœ… è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ: 8377 æ¡\n",
      "\n",
      "============================================================\n",
      "æ•°æ®æ ·æœ¬ï¼ˆæ¯ç±»æ˜¾ç¤º3æ¡ï¼‰\n",
      "============================================================\n",
      "\n",
      "POSITIVE:\n",
      "------------------------------------------------------------\n",
      "1. JPM | 2025-12-10 | å˜åŒ–: +2.34%\n",
      "   WINTON GROUP Ltd Sells 8,528 Shares of Danaher Corporation $DHR\n",
      "\n",
      "2. TSLA | 2025-12-08 | å˜åŒ–: +1.27%\n",
      "   Tesla Downgraded by Morgan Stanley on Valuation Concerns\n",
      "\n",
      "3. GOOGL | 2025-12-02 | å˜åŒ–: +1.21%\n",
      "   Alphabet Inc. $GOOG Shares Sold by MT Bank Corp\n",
      "\n",
      "NEGATIVE:\n",
      "------------------------------------------------------------\n",
      "1. JPM | 2025-12-08 | å˜åŒ–: -4.66%\n",
      "   JPMorgan Chase  Co. Has Lowered Expectations for Enphase Energy (NASDAQ:ENPH) St\n",
      "   ...\n",
      "\n",
      "2. GOOGL | 2025-12-05 | å˜åŒ–: -2.29%\n",
      "   Just as We Surprised the World with Games, We Will Create a Success Story with A\n",
      "   ...\n",
      "\n",
      "3. MSFT | 2025-12-11 | å˜åŒ–: -1.02%\n",
      "   Diageo plc Stock Hits 52Week Lows: What Belfast Strikes, Profit Warning and a Ne\n",
      "   ...\n",
      "\n",
      "NEUTRAL:\n",
      "------------------------------------------------------------\n",
      "1. WMT | 2025-11-06 | å˜åŒ–: +0.90%\n",
      "   Coca-Cola quietly launches first festive flavor in five years but fans left conf\n",
      "   ...\n",
      "\n",
      "2. BAC | 2025-11-22 | å˜åŒ–: +0.72%\n",
      "   Major US bank confirms closure of several branches in 5 states - see full list\n",
      "\n",
      "3. DIS | 2025-11-16 | å˜åŒ–: -0.10%\n",
      "   DIS - Walt Disney Co Stock Price and Quote\n",
      "\n",
      "\n",
      "============================================================\n",
      "ä¿å­˜æ•°æ®...\n",
      "============================================================\n",
      "âœ… æ•°æ®å·²ä¿å­˜!\n",
      "   æ–‡ä»¶:  training_data_2025.csv\n",
      "   æ¡æ•°: 8377\n",
      "   å¤§å°: 891.30 KB\n",
      "\n",
      "============================================================\n",
      "âœ… å…¨éƒ¨å®Œæˆï¼\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š æœ€ç»ˆè®­ç»ƒæ•°æ®:  8377 æ¡\n",
      "\n",
      "æ ‡ç­¾åˆ†å¸ƒ:\n",
      "  neutral: 3374 æ¡ (40.3%)\n",
      "  positive: 3020 æ¡ (36.1%)\n",
      "  negative: 1983 æ¡ (23.7%)\n",
      "\n",
      "ğŸ‰ å¯ä»¥å¼€å§‹è®­ç»ƒ FinBERT æ¨¡å‹äº†ï¼\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step3b_match_prices_2025.py\n",
    "\"\"\"\n",
    "Match Nov-Dec 2025 news and price data.\n",
    "Uses the newly merged combined_clean_news_all.csv\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "NEWS_FILE = 'combined_clean_news_all.csv'\n",
    "PRICES_FILE = 'all_stocks_prices.csv'\n",
    "OUTPUT_FILE = 'training_data_2025.csv'\n",
    "\n",
    "# Sentiment label thresholds\n",
    "POSITIVE_THRESHOLD = 1.0   # Gain > 1%\n",
    "NEGATIVE_THRESHOLD = -1.0  # Loss < -1%\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load data\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Step 3B: Match 2025 news and prices\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    print(\"Loading news data...\")\n",
    "    try:\n",
    "        df_news = pd.read_csv(NEWS_FILE, encoding='utf-8-sig')\n",
    "        df_news['date'] = pd.to_datetime(df_news['date'])\n",
    "        print(f\"âœ… News data:  {len(df_news)} items\")\n",
    "        print(f\"   Date range: {df_news['date'].min()} to {df_news['date'].max()}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Failed to load news: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    print()\n",
    "    print(\"Loading price data...\")\n",
    "    try:\n",
    "        df_prices = pd.read_csv(PRICES_FILE, encoding='utf-8-sig')\n",
    "        df_prices['date'] = pd.to_datetime(df_prices['date'])\n",
    "        print(f\"âœ… Price data: {len(df_prices)} items\")\n",
    "        print(f\"   Date range: {df_prices['date'].min()} to {df_prices['date'].max()}\")\n",
    "    except Exception as e: \n",
    "        print(f\"âŒ Failed to load prices: {e}\")\n",
    "        return df_news, None\n",
    "    \n",
    "    # Filter Nov-Dec 2025 price data\n",
    "    print()\n",
    "    print(\"Filtering Nov-Dec 2025 price data...\")\n",
    "    df_prices_2025 = df_prices[df_prices['date'] >= '2025-11-01']. copy()\n",
    "    print(f\"âœ… Nov-Dec 2025 prices:  {len(df_prices_2025)} items\")\n",
    "    \n",
    "    if len(df_prices_2025) == 0:\n",
    "        print()\n",
    "        print(\"âš ï¸  Warning:  No Nov-Dec 2025 price data!\")\n",
    "        print(\"   You may need to update price data to latest\")\n",
    "        return df_news, None\n",
    "    \n",
    "    return df_news, df_prices_2025\n",
    "\n",
    "def calculate_price_change(df_prices, ticker, news_date):\n",
    "    \"\"\"\n",
    "    Calculate price change.\n",
    "    \n",
    "    Strategy:\n",
    "    1. Find the closing price on or before the news date (as baseline)\n",
    "    2. Find the closing price 1-5 days after the news date\n",
    "    3. Calculate percentage change\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Filter data for this stock\n",
    "        stock_data = df_prices[df_prices['ticker'] == ticker].copy()\n",
    "        \n",
    "        if len(stock_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        stock_data = stock_data.sort_values('date')\n",
    "        \n",
    "        # Method 1: Find the nearest price on or before news date\n",
    "        before_data = stock_data[stock_data['date'] <= news_date]\n",
    "        \n",
    "        if len(before_data) == 0:\n",
    "            # If news date is too early, try to find earliest price\n",
    "            before_data = stock_data\n",
    "        \n",
    "        if len(before_data) == 0:\n",
    "            return None\n",
    "        \n",
    "        price_before = before_data.iloc[-1]['close']\n",
    "        date_before = before_data.iloc[-1]['date']\n",
    "        \n",
    "        # Method 2: Find price 1-5 days after news date\n",
    "        after_data = stock_data[\n",
    "            (stock_data['date'] > news_date) & \n",
    "            (stock_data['date'] <= news_date + timedelta(days=5))\n",
    "        ]\n",
    "        \n",
    "        if len(after_data) == 0:\n",
    "            # If no data after, try same day data\n",
    "            same_day = stock_data[stock_data['date'] == news_date]\n",
    "            \n",
    "            if len(same_day) > 0:\n",
    "                # Use same day close vs open\n",
    "                row = same_day.iloc[0]\n",
    "                price_before = row['open']\n",
    "                price_after = row['close']\n",
    "            else:\n",
    "                # No matching data at all\n",
    "                return None\n",
    "        else:\n",
    "            price_after = after_data.iloc[0]['close']\n",
    "            date_after = after_data.iloc[0]['date']\n",
    "        \n",
    "        # Calculate percentage change\n",
    "        if price_before > 0:\n",
    "            price_change = ((price_after - price_before) / price_before) * 100\n",
    "            return round(price_change, 4)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    except Exception as e: \n",
    "        return None\n",
    "\n",
    "def assign_sentiment(price_change):\n",
    "    \"\"\"\n",
    "    Assign sentiment label based on price change.\n",
    "    \n",
    "    positive: Gain > 1%\n",
    "    negative: Loss < -1%\n",
    "    neutral:  -1% ~ 1%\n",
    "    \"\"\"\n",
    "    if price_change is None:\n",
    "        return 'unknown'\n",
    "    elif price_change > POSITIVE_THRESHOLD:\n",
    "        return 'positive'\n",
    "    elif price_change < NEGATIVE_THRESHOLD:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "def match_and_label(df_news, df_prices):\n",
    "    \"\"\"Match prices and label sentiment\"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Starting price matching and labeling...\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    total = len(df_news)\n",
    "    print(f\"Total to process:  {total} news items\")\n",
    "    print()\n",
    "    \n",
    "    price_changes = []\n",
    "    sentiments = []\n",
    "    matched_count = 0\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for idx, row in df_news.iterrows():\n",
    "        # Show progress\n",
    "        if (idx + 1) % 500 == 0 or (idx + 1) == total:\n",
    "            progress = (idx + 1) / total * 100\n",
    "            elapsed = time.time() - start_time\n",
    "            speed = (idx + 1) / elapsed\n",
    "            remaining = (total - idx - 1) / speed if speed > 0 else 0\n",
    "            \n",
    "            print(f\"Progress: {idx + 1}/{total} ({progress:.1f}%) | \"\n",
    "                  f\"Matched: {matched_count} | \"\n",
    "                  f\"Speed:  {speed:.1f} items/sec | \"\n",
    "                  f\"Remaining: {int(remaining)} sec\")\n",
    "        \n",
    "        # Calculate price change\n",
    "        price_change = calculate_price_change(\n",
    "            df_prices,\n",
    "            row['ticker'],\n",
    "            row['date']\n",
    "        )\n",
    "        \n",
    "        if price_change is not None:\n",
    "            matched_count += 1\n",
    "        \n",
    "        price_changes.append(price_change)\n",
    "        \n",
    "        # Assign sentiment label\n",
    "        sentiment = assign_sentiment(price_change)\n",
    "        sentiments.append(sentiment)\n",
    "    \n",
    "    # Add to DataFrame\n",
    "    df_news['price_change'] = price_changes\n",
    "    df_news['sentiment'] = sentiments\n",
    "    \n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Matching complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Total data:  {total} items\")\n",
    "    print(f\"Successfully matched: {matched_count} items\")\n",
    "    print(f\"Match rate: {matched_count/total*100:.1f}%\")\n",
    "    print(f\"Total time: {time.time() - start_time:.1f} sec\")\n",
    "    \n",
    "    return df_news\n",
    "\n",
    "def show_statistics(df):\n",
    "    \"\"\"Show statistics\"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Data statistics\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    # Remove unknown\n",
    "    df_valid = df[df['sentiment'] != 'unknown'].copy()\n",
    "    \n",
    "    print(f\"Total data: {len(df)} items\")\n",
    "    print(f\"Valid data: {len(df_valid)} items\")\n",
    "    print(f\"Invalid data: {len(df) - len(df_valid)} items\")\n",
    "    \n",
    "    if len(df_valid) == 0:\n",
    "        print()\n",
    "        print(\"âŒ No valid data!\")\n",
    "        return None\n",
    "    \n",
    "    print()\n",
    "    print(\"Sentiment label distribution:\")\n",
    "    print(\"-\" * 60)\n",
    "    sentiment_counts = df_valid['sentiment'].value_counts()\n",
    "    for sentiment in ['positive', 'negative', 'neutral']:\n",
    "        if sentiment in sentiment_counts.index:\n",
    "            count = sentiment_counts[sentiment]\n",
    "            percentage = count / len(df_valid) * 100\n",
    "            bar = \"â–ˆ\" * int(percentage / 2)\n",
    "            print(f\"  {sentiment.ljust(10)}: {str(count).rjust(5)} items ({percentage:5.1f}%) {bar}\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Price change statistics:\")\n",
    "    print(f\"  Mean change: {df_valid['price_change'].mean():.2f}%\")\n",
    "    print(f\"  Std dev: {df_valid['price_change'].std():.2f}%\")\n",
    "    print(f\"  Max gain: {df_valid['price_change'].max():.2f}%\")\n",
    "    print(f\"  Max loss: {df_valid['price_change'].min():.2f}%\")\n",
    "    \n",
    "    print()\n",
    "    print(\"Data distribution by stock:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Ticker':<8} {'Positive':>10} {'Negative':>10} {'Neutral':>10} {'Total':>10}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for ticker in sorted(df_valid['ticker'].unique()):\n",
    "        ticker_data = df_valid[df_valid['ticker'] == ticker]\n",
    "        pos = len(ticker_data[ticker_data['sentiment'] == 'positive'])\n",
    "        neg = len(ticker_data[ticker_data['sentiment'] == 'negative'])\n",
    "        neu = len(ticker_data[ticker_data['sentiment'] == 'neutral'])\n",
    "        total_ticker = len(ticker_data)\n",
    "        \n",
    "        print(f\"{ticker:<8} {pos:>10} {neg:>10} {neu:>10} {total_ticker:>10}\")\n",
    "    \n",
    "    return df_valid\n",
    "\n",
    "def prepare_training_data(df):\n",
    "    \"\"\"Prepare training data\"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Preparing training data...\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    # Select required columns\n",
    "    if 'title_clean' in df.columns:\n",
    "        text_col = 'title_clean'\n",
    "    elif 'title' in df.columns:\n",
    "        text_col = 'title'\n",
    "    else:\n",
    "        print(\"âŒ Title column not found\")\n",
    "        return None\n",
    "    \n",
    "    training_df = df[[\n",
    "        'ticker',\n",
    "        'date',\n",
    "        text_col,\n",
    "        'sentiment',\n",
    "        'price_change'\n",
    "    ]].copy()\n",
    "    \n",
    "    # Rename columns\n",
    "    training_df = training_df.rename(columns={\n",
    "        text_col: 'text',\n",
    "        'sentiment': 'label'\n",
    "    })\n",
    "    \n",
    "    # Ensure text is not empty\n",
    "    training_df = training_df[training_df['text'].notna()]\n",
    "    training_df = training_df[training_df['text'].str.strip() != '']\n",
    "    \n",
    "    print(f\"âœ… Training data ready: {len(training_df)} items\")\n",
    "    \n",
    "    return training_df\n",
    "\n",
    "def show_samples(df):\n",
    "    \"\"\"Show sample data\"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Data samples (3 per category)\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    \n",
    "    for label in ['positive', 'negative', 'neutral']:\n",
    "        label_data = df[df['label'] == label]\n",
    "        \n",
    "        if len(label_data) > 0:\n",
    "            print(f\"{label.upper()}:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            samples = label_data.sample(min(3, len(label_data)))\n",
    "            \n",
    "            for idx, (i, row) in enumerate(samples.iterrows(), 1):\n",
    "                date_str = str(row['date'])[:10]\n",
    "                change = row['price_change']\n",
    "                ticker = row['ticker']\n",
    "                text = row['text'][:80]\n",
    "                \n",
    "                print(f\"{idx}. {ticker} | {date_str} | Change: {change:+.2f}%\")\n",
    "                print(f\"   {text}\")\n",
    "                if len(row['text']) > 80:\n",
    "                    print(\"   ...\")\n",
    "                print()\n",
    "\n",
    "def save_data(df, output_file):\n",
    "    \"\"\"Save data\"\"\"\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Saving data...\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    try:\n",
    "        df.to_csv(output_file, index=False, encoding='utf-8-sig')\n",
    "        \n",
    "        import os\n",
    "        file_size = os.path.getsize(output_file) / 1024\n",
    "        \n",
    "        print(f\"âœ… Data saved!\")\n",
    "        print(f\"   File:  {output_file}\")\n",
    "        print(f\"   Count: {len(df)}\")\n",
    "        print(f\"   Size: {file_size:.2f} KB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Save failed: {e}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print()\n",
    "    print(\"ğŸ¯ Match 2025 news and price data\")\n",
    "    print()\n",
    "    print(\"Configuration:\")\n",
    "    print(f\"  News file: {NEWS_FILE}\")\n",
    "    print(f\"  Price file: {PRICES_FILE}\")\n",
    "    print(f\"  Output file: {OUTPUT_FILE}\")\n",
    "    print(f\"  Thresholds: positive>{POSITIVE_THRESHOLD}%, negative<{NEGATIVE_THRESHOLD}%\")\n",
    "    print()\n",
    "    \n",
    "    # 1. Load data\n",
    "    df_news, df_prices = load_data()\n",
    "    \n",
    "    if df_news is None or df_prices is None:\n",
    "        print()\n",
    "        print(\"âŒ Data loading failed, cannot continue\")\n",
    "        return\n",
    "    \n",
    "    # 2. Match and label\n",
    "    df_labeled = match_and_label(df_news, df_prices)\n",
    "    \n",
    "    # 3. Show statistics\n",
    "    df_valid = show_statistics(df_labeled)\n",
    "    \n",
    "    if df_valid is None or len(df_valid) < 100:\n",
    "        print()\n",
    "        print(\"âš ï¸  Too few valid data, please check price data\")\n",
    "        return\n",
    "    \n",
    "    # 4. Prepare training data\n",
    "    training_df = prepare_training_data(df_valid)\n",
    "    \n",
    "    if training_df is None:\n",
    "        return\n",
    "    \n",
    "    # 5. Show samples\n",
    "    show_samples(training_df)\n",
    "    \n",
    "    # 6. Save data\n",
    "    save_data(training_df, OUTPUT_FILE)\n",
    "    \n",
    "    # 7. Final summary\n",
    "    print()\n",
    "    print(\"=\" * 60)\n",
    "    print(\"âœ… All complete!\")\n",
    "    print(\"=\" * 60)\n",
    "    print()\n",
    "    print(f\"ğŸ“Š Final training data:  {len(training_df)} items\")\n",
    "    print()\n",
    "    print(\"Label distribution:\")\n",
    "    label_dist = training_df['label'].value_counts()\n",
    "    for label, count in label_dist.items():\n",
    "        percentage = count / len(training_df) * 100\n",
    "        print(f\"  {label}: {count} items ({percentage:.1f}%)\")\n",
    "    print()\n",
    "    print(\"ğŸ‰ Ready to train FinBERT model!\")\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c283e6e9",
   "metadata": {},
   "source": [
    "##### 1.7: Data quality analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e84f6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "è®­ç»ƒé›†è´¨é‡åˆ†ææŠ¥å‘Š\n",
      "================================================================================\n",
      "\n",
      "åŠ è½½æ•°æ®...\n",
      "âœ… æ•°æ®åŠ è½½æˆåŠŸ:   8377 æ¡\n",
      "\n",
      "================================================================================\n",
      "1. åŸºç¡€ç»Ÿè®¡\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Š æ€»æ•°æ®é‡: 8377 æ¡\n",
      "ğŸ“… æ—¥æœŸèŒƒå›´:   2025-10-29 åˆ° 2025-12-12\n",
      "ğŸ“† æ—¶é—´è·¨åº¦:   44 å¤©\n",
      "ğŸ¢ è‚¡ç¥¨æ•°é‡: 10 åª\n",
      "\n",
      "æ•°æ®åˆ—:\n",
      "  â€¢ ticker:  object\n",
      "  â€¢ date:  datetime64[ns]\n",
      "  â€¢ text:  object\n",
      "  â€¢ label:  object\n",
      "  â€¢ price_change:  float64\n",
      "\n",
      "================================================================================\n",
      "2. æ ‡ç­¾åˆ†å¸ƒ\n",
      "================================================================================\n",
      "\n",
      "æ ‡ç­¾ç»Ÿè®¡:\n",
      "--------------------------------------------------------------------------------\n",
      "æ ‡ç­¾                 æ•°é‡        ç™¾åˆ†æ¯”                  å¯è§†åŒ–\n",
      "--------------------------------------------------------------------------------\n",
      "neutral          3374     40.28% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "positive         3020     36.05% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "negative         1983     23.67% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "--------------------------------------------------------------------------------\n",
      "æ€»è®¡               8377   100. 00%\n",
      "\n",
      "æ•°æ®å¹³è¡¡æ€§:\n",
      "  æœ€å¤šç±»åˆ«: neutral (3374 æ¡)\n",
      "  æœ€å°‘ç±»åˆ«: negative (1983 æ¡)\n",
      "  å¹³è¡¡æ¯”ä¾‹: 0.59 (æœ€å°‘/æœ€å¤š)\n",
      "  è¯„ä¼°: â­â­â­ åŸºæœ¬å¹³è¡¡\n",
      "\n",
      "================================================================================\n",
      "3. ä»·æ ¼å˜åŒ–åˆ†æ\n",
      "================================================================================\n",
      "\n",
      "æ•´ä½“ä»·æ ¼å˜åŒ–ç»Ÿè®¡:\n",
      "--------------------------------------------------------------------------------\n",
      "  å¹³å‡å˜åŒ–: 0.1414%\n",
      "  ä¸­ä½æ•°:   0.3592%\n",
      "  æ ‡å‡†å·®:  2.2911%\n",
      "  æœ€å¤§æ¶¨å¹…: 6.8245%\n",
      "  æœ€å¤§è·Œå¹…: -13.1491%\n",
      "\n",
      "å„æ ‡ç­¾çš„ä»·æ ¼å˜åŒ–:\n",
      "--------------------------------------------------------------------------------\n",
      "æ ‡ç­¾                   å¹³å‡        ä¸­ä½æ•°        æ ‡å‡†å·®         æœ€å¤§         æœ€å°\n",
      "--------------------------------------------------------------------------------\n",
      "positive          2.05%      1.63%      1.26%      6.82%      1.01%\n",
      "negative         -2.72%     -1.97%      2.35%     -1.00%    -13.15%\n",
      "neutral           0.11%      0.12%      0.50%      1.00%     -0.87%\n",
      "\n",
      "ä»·æ ¼å˜åŒ–åŒºé—´åˆ†å¸ƒ:\n",
      "--------------------------------------------------------------------------------\n",
      "  å¤§è·Œ (<-5%)              102 æ¡ (  1.2%) \n",
      "  ä¸­è·Œ (-5%~-2%)           880 æ¡ ( 10.5%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  å°è·Œ (-2%~-1%)          1001 æ¡ ( 11.9%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  éœ‡è¡ (-1%~1%)           3374 æ¡ ( 40.3%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  å°æ¶¨ (1%~2%)            2018 æ¡ ( 24.1%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  ä¸­æ¶¨ (2%~5%)             837 æ¡ ( 10.0%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  å¤§æ¶¨ (>5%)               165 æ¡ (  2.0%) \n",
      "\n",
      "================================================================================\n",
      "4. è‚¡ç¥¨åˆ†å¸ƒåˆ†æ\n",
      "================================================================================\n",
      "\n",
      "å„è‚¡ç¥¨æ•°æ®é‡:\n",
      "--------------------------------------------------------------------------------\n",
      "è‚¡ç¥¨             æ€»æ•°   Positive   Negative    Neutral\n",
      "--------------------------------------------------------------------------------\n",
      "AAPL          835        202        126        507\n",
      "BAC           929        266         37        626\n",
      "DIS           296        118         33        145\n",
      "GOOGL         940        346        329        265\n",
      "GS            903        656        190         57\n",
      "JPM           923        264        171        488\n",
      "MSFT          935        275        275        385\n",
      "NVDA          953        276        323        354\n",
      "TSLA          784        359        240        185\n",
      "WMT           879        258        259        362\n",
      "--------------------------------------------------------------------------------\n",
      "æ€»è®¡           8377\n",
      "\n",
      "è‚¡ç¥¨æ•°æ®åˆ†å¸ƒå¹³è¡¡æ€§:\n",
      "  æœ€å¤š:  NVDA (953 æ¡)\n",
      "  æœ€å°‘: DIS (296 æ¡)\n",
      "  å¹³è¡¡æ¯”ä¾‹: 0.31\n",
      "  è¯„ä¼°: â­â­â­ è‚¡ç¥¨åˆ†å¸ƒæœ‰å·®å¼‚\n",
      "\n",
      "================================================================================\n",
      "5. æ–‡æœ¬è´¨é‡åˆ†æ\n",
      "================================================================================\n",
      "\n",
      "æ–‡æœ¬é•¿åº¦ç»Ÿè®¡:\n",
      "--------------------------------------------------------------------------------\n",
      "  å¹³å‡å­—ç¬¦æ•°: 74.8\n",
      "  ä¸­ä½æ•°:     72.0\n",
      "  æœ€çŸ­:       10 å­—ç¬¦\n",
      "  æœ€é•¿:      254 å­—ç¬¦\n",
      "\n",
      "  å¹³å‡å•è¯æ•°: 11.6\n",
      "  ä¸­ä½æ•°:    11.0\n",
      "  æœ€å°‘:      1 è¯\n",
      "  æœ€å¤š:      41 è¯\n",
      "\n",
      "æ–‡æœ¬é•¿åº¦åˆ†å¸ƒ:\n",
      "--------------------------------------------------------------------------------\n",
      "  å¾ˆçŸ­ (<50å­—ç¬¦)             821 æ¡ (  9.8%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  çŸ­ (50-100)            6721 æ¡ ( 80.2%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  ä¸­ç­‰ (100-150)           779 æ¡ (  9.3%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  é•¿ (150-200)             42 æ¡ (  0.5%) \n",
      "  å¾ˆé•¿ (>200)               14 æ¡ (  0.2%) \n",
      "\n",
      "æ•°æ®å®Œæ•´æ€§:\n",
      "--------------------------------------------------------------------------------\n",
      "  ç©ºå€¼æ•°é‡: 0\n",
      "  ç©ºæ–‡æœ¬:   0\n",
      "  é‡å¤æ•°æ®: 0\n",
      "  è¯„ä¼°: â­â­â­â­â­ æ•°æ®å®Œæ•´\n",
      "\n",
      "================================================================================\n",
      "6. æ—¶é—´åˆ†å¸ƒåˆ†æ\n",
      "================================================================================\n",
      "\n",
      "æŒ‰æœˆä»½åˆ†å¸ƒ:\n",
      "--------------------------------------------------------------------------------\n",
      "  2025-10:     99 æ¡ (  1.2%) \n",
      "  2025-11:   3451 æ¡ ( 41.2%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  2025-12:   4827 æ¡ ( 57.6%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "æŒ‰æ˜ŸæœŸåˆ†å¸ƒ:\n",
      "--------------------------------------------------------------------------------\n",
      "  Monday    :   1645 æ¡ ( 19.6%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Tuesday   :   1450 æ¡ ( 17.3%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Wednesday :   1243 æ¡ ( 14.8%) â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Thursday  :   1273 æ¡ ( 15.2%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Friday    :   1282 æ¡ ( 15.3%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  Saturday  :    625 æ¡ (  7.5%) â–ˆâ–ˆ\n",
      "  Sunday    :    859 æ¡ ( 10.3%) â–ˆâ–ˆâ–ˆ\n",
      "\n",
      "================================================================================\n",
      "7. æ ·æœ¬æ•°æ®å±•ç¤º\n",
      "================================================================================\n",
      "\n",
      "POSITIVE æ ·æœ¬ (éšæœºæŠ½å–3æ¡):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. è‚¡ç¥¨: MSFT | æ—¥æœŸ: 2025-12-07 | ä»·æ ¼å˜åŒ–: +1.63%\n",
      "   æ–‡æœ¬:  Brown Advisory Inc. Lowers Stake in WSFS Financial Corporation $WSFS\n",
      "\n",
      "2. è‚¡ç¥¨: AAPL | æ—¥æœŸ: 2025-11-10 | ä»·æ ¼å˜åŒ–: +2.16%\n",
      "   æ–‡æœ¬:  Apple Inc. $AAPL Shares Bought by Almanack Investment Partners LLC.\n",
      "\n",
      "3. è‚¡ç¥¨: MSFT | æ—¥æœŸ: 2025-12-07 | ä»·æ ¼å˜åŒ–: +1.63%\n",
      "   æ–‡æœ¬:  First Trust Advisors LP Buys 25,005 Shares of Emerson Electric Co. $EMR\n",
      "\n",
      "NEGATIVE æ ·æœ¬ (éšæœºæŠ½å–3æ¡):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. è‚¡ç¥¨: GS | æ—¥æœŸ: 2025-11-28 | ä»·æ ¼å˜åŒ–: -1.85%\n",
      "   æ–‡æœ¬:  Rhumbline Advisers Purchases 2,118 Shares of Allegion PLC $ALLE\n",
      "\n",
      "2. è‚¡ç¥¨: MSFT | æ—¥æœŸ: 2025-12-11 | ä»·æ ¼å˜åŒ–: -1.02%\n",
      "   æ–‡æœ¬:  Diageo plc Stock Hits 52Week Lows: What Belfast Strikes, Profit Warning and a New CEO Mean for DGEDE\n",
      "        O in December 2025\n",
      "\n",
      "3. è‚¡ç¥¨: TSLA | æ—¥æœŸ: 2025-11-03 | ä»·æ ¼å˜åŒ–: -5.15%\n",
      "   æ–‡æœ¬:  Elon Musk Says Bill Gates Is 'Not Strong' In Science, Recalling Visit To Tesla Factory Where He Dism\n",
      "        issed Long-Range Semitrucks As 'Impossible'\n",
      "\n",
      "NEUTRAL æ ·æœ¬ (éšæœºæŠ½å–3æ¡):\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1. è‚¡ç¥¨: AAPL | æ—¥æœŸ: 2025-11-19 | ä»·æ ¼å˜åŒ–: -0.86%\n",
      "   æ–‡æœ¬:  World's biggest company Nvidia stuns Wall Street as it gives biggest clue yet to state of US economy\n",
      "\n",
      "2. è‚¡ç¥¨: GOOGL | æ—¥æœŸ: 2025-12-03 | ä»·æ ¼å˜åŒ–: -0.63%\n",
      "   æ–‡æœ¬:  Michael Burry Says Tesla Is Ridiculously Overvalued. Should You Ditch TSLA Stock Here?\n",
      "\n",
      "3. è‚¡ç¥¨: TSLA | æ—¥æœŸ: 2025-12-04 | ä»·æ ¼å˜åŒ–: +0.10%\n",
      "   æ–‡æœ¬:  Elon Musk Says His 'Running Robot' Will 'Actually Eliminate Poverty' As He Shares Video Of Tesla Opt\n",
      "        imus Jogging\n",
      "\n",
      "================================================================================\n",
      "8. ç»¼åˆè´¨é‡è¯„åˆ†\n",
      "================================================================================\n",
      "\n",
      "å„é¡¹è¯„åˆ†:\n",
      "--------------------------------------------------------------------------------\n",
      "è¯„åˆ†é¡¹                   å¾—åˆ†       æ»¡åˆ†         ç­‰çº§\n",
      "--------------------------------------------------------------------------------\n",
      "æ•°æ®é‡                   20       20      â­â­â­â­â­\n",
      "æ ‡ç­¾å¹³è¡¡                  10       20        â­â­â­\n",
      "è‚¡ç¥¨è¦†ç›–                  15       15      â­â­â­â­â­\n",
      "æ–‡æœ¬è´¨é‡                  10       15        â­â­â­\n",
      "ä»·æ ¼å˜åŒ–                  15       15      â­â­â­â­â­\n",
      "æ•°æ®å®Œæ•´                  15       15      â­â­â­â­â­\n",
      "--------------------------------------------------------------------------------\n",
      "æ€»åˆ†                    85      100\n",
      "\n",
      "ç»¼åˆè¯„ä»·: â­â­â­â­ è‰¯å¥½ - æ•°æ®è´¨é‡å¾ˆå¥½ï¼Œé€‚åˆè®­ç»ƒä½¿ç”¨\n",
      "\n",
      "================================================================================\n",
      "âœ… åˆ†æå®Œæˆï¼\n",
      "================================================================================\n",
      "\n",
      "å»ºè®®:\n",
      "  1. å¦‚æœç»¼åˆè¯„åˆ† >= 75åˆ†ï¼Œå¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒ FinBERT\n",
      "  2. å¦‚æœæŸé¡¹è¯„åˆ†è¾ƒä½ï¼Œå¯ä»¥é’ˆå¯¹æ€§ä¼˜åŒ–\n",
      "  3. å»ºè®®å°†åˆ†æç»“æœä¿å­˜ï¼Œç”¨äºé¡¹ç›®æŠ¥å‘Š\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data quality analysis\n",
    "# analyze_training_data.py\n",
    "\"\"\"\n",
    "Detailed analysis of training set quality.\n",
    "Includes: data distribution, text quality, price changes, time distribution, etc.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration\n",
    "DATA_FILE = 'training_data_2025.csv'\n",
    "\n",
    "# Set font for display\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load data\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Training Set Quality Analysis Report\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_csv(DATA_FILE, encoding='utf-8-sig')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    print(f\"âœ… Data loaded successfully:   {len(df)} items\")\n",
    "    print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def basic_statistics(df):\n",
    "    \"\"\"Basic statistics\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"1. Basic Statistics\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    print(f\"ğŸ“Š Total data: {len(df)} items\")\n",
    "    print(f\"ğŸ“… Date range:   {df['date'].min().strftime('%Y-%m-%d')} to {df['date'].max().strftime('%Y-%m-%d')}\")\n",
    "    print(f\"ğŸ“† Time span:   {(df['date'].max() - df['date'].min()).days} days\")\n",
    "    print(f\"ğŸ¢ Stock count: {df['ticker'].nunique()} stocks\")\n",
    "    print()\n",
    "    \n",
    "    # Data column info\n",
    "    print(\"Data columns:\")\n",
    "    for col in df.columns:\n",
    "        print(f\"  â€¢ {col}:  {df[col].dtype}\")\n",
    "    print()\n",
    "\n",
    "def label_distribution(df):\n",
    "    \"\"\"Label distribution analysis\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"2. Label Distribution\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    label_counts = df['label'].value_counts()\n",
    "    \n",
    "    print(\"Label statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Label':<12} {'Count':>8} {'Percentage':>10} {'Visualization':>20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for label, count in label_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        bar = \"â–ˆ\" * int(percentage / 2)\n",
    "        print(f\"{label:<12} {count:>8} {percentage:>9.2f}% {bar}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Total':<12} {len(df):>8} {'100.00':>9}%\")\n",
    "    print()\n",
    "    \n",
    "    # Balance analysis\n",
    "    print(\"Data balance:\")\n",
    "    max_count = label_counts.max()\n",
    "    min_count = label_counts.min()\n",
    "    balance_ratio = min_count / max_count\n",
    "    \n",
    "    print(f\"  Most common: {label_counts.idxmax()} ({max_count} items)\")\n",
    "    print(f\"  Least common: {label_counts.idxmin()} ({min_count} items)\")\n",
    "    print(f\"  Balance ratio: {balance_ratio:.2f} (min/max)\")\n",
    "    \n",
    "    if balance_ratio >= 0.8:\n",
    "        print(f\"  Rating:  â­â­â­â­â­ Very balanced\")\n",
    "    elif balance_ratio >= 0.6:\n",
    "        print(f\"  Rating: â­â­â­â­ Fairly balanced\")\n",
    "    elif balance_ratio >= 0.4:\n",
    "        print(f\"  Rating: â­â­â­ Basically balanced\")\n",
    "    else:\n",
    "        print(f\"  Rating: â­â­ Not balanced, adjustment recommended\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def price_change_analysis(df):\n",
    "    \"\"\"Price change analysis\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"3. Price Change Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    print(\"Overall price change statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Mean change: {df['price_change'].mean():.4f}%\")\n",
    "    print(f\"  Median:   {df['price_change'].median():.4f}%\")\n",
    "    print(f\"  Std dev:  {df['price_change'].std():.4f}%\")\n",
    "    print(f\"  Max gain: {df['price_change'].max():.4f}%\")\n",
    "    print(f\"  Max loss: {df['price_change'].min():.4f}%\")\n",
    "    print()\n",
    "    \n",
    "    # Stats by label\n",
    "    print(\"Price change by label:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Label':<12} {'Mean':>10} {'Median':>10} {'Std Dev':>10} {'Max':>10} {'Min':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for label in ['positive', 'negative', 'neutral']:\n",
    "        if label in df['label'].values:\n",
    "            label_data = df[df['label'] == label]['price_change']\n",
    "            print(f\"{label:<12} {label_data.mean():>9.2f}% {label_data.median():>9.2f}% \"\n",
    "                  f\"{label_data.std():>9.2f}% {label_data.max():>9.2f}% {label_data.min():>9.2f}%\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Price change distribution ranges\n",
    "    print(\"Price change range distribution:\")\n",
    "    print(\"-\" * 80)\n",
    "    bins = [\n",
    "        (-float('inf'), -5, \"Big drop (<-5%)\"),\n",
    "        (-5, -2, \"Medium drop (-5%~-2%)\"),\n",
    "        (-2, -1, \"Small drop (-2%~-1%)\"),\n",
    "        (-1, 1, \"Flat (-1%~1%)\"),\n",
    "        (1, 2, \"Small gain (1%~2%)\"),\n",
    "        (2, 5, \"Medium gain (2%~5%)\"),\n",
    "        (5, float('inf'), \"Big gain (>5%)\")\n",
    "    ]\n",
    "    \n",
    "    for low, high, desc in bins:\n",
    "        count = len(df[(df['price_change'] > low) & (df['price_change'] <= high)])\n",
    "        percentage = count / len(df) * 100\n",
    "        bar = \"â–ˆ\" * int(percentage / 2)\n",
    "        print(f\"  {desc:<20} {count:>5} items ({percentage:>5.1f}%) {bar}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def ticker_distribution(df):\n",
    "    \"\"\"Stock distribution analysis\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"4. Stock Distribution Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    ticker_counts = df['ticker'].value_counts()\n",
    "    \n",
    "    print(\"Data count per stock:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Ticker':<8} {'Total':>8} {'Positive':>10} {'Negative':>10} {'Neutral':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for ticker in sorted(ticker_counts.index):\n",
    "        ticker_data = df[df['ticker'] == ticker]\n",
    "        total = len(ticker_data)\n",
    "        pos = len(ticker_data[ticker_data['label'] == 'positive'])\n",
    "        neg = len(ticker_data[ticker_data['label'] == 'negative'])\n",
    "        neu = len(ticker_data[ticker_data['label'] == 'neutral'])\n",
    "        \n",
    "        print(f\"{ticker:<8} {total:>8} {pos:>10} {neg:>10} {neu:>10}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Total':<8} {len(df):>8}\")\n",
    "    print()\n",
    "    \n",
    "    # Stock distribution balance\n",
    "    print(\"Stock distribution balance:\")\n",
    "    max_ticker_count = ticker_counts.max()\n",
    "    min_ticker_count = ticker_counts.min()\n",
    "    ticker_balance = min_ticker_count / max_ticker_count\n",
    "    \n",
    "    print(f\"  Most:  {ticker_counts.idxmax()} ({max_ticker_count} items)\")\n",
    "    print(f\"  Least: {ticker_counts.idxmin()} ({min_ticker_count} items)\")\n",
    "    print(f\"  Balance ratio: {ticker_balance:.2f}\")\n",
    "    \n",
    "    if ticker_balance >= 0.7:\n",
    "        print(f\"  Rating: â­â­â­â­â­ Stock distribution is balanced\")\n",
    "    elif ticker_balance >= 0.5:\n",
    "        print(f\"  Rating: â­â­â­â­ Stock distribution is fairly balanced\")\n",
    "    else:\n",
    "        print(f\"  Rating: â­â­â­ Stock distribution has variance\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def text_quality_analysis(df):\n",
    "    \"\"\"Text quality analysis\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"5. Text Quality Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Text length statistics\n",
    "    df['text_length'] = df['text'].str.len()\n",
    "    df['word_count'] = df['text'].str.split().str.len()\n",
    "    \n",
    "    print(\"Text length statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Avg chars: {df['text_length'].mean():.1f}\")\n",
    "    print(f\"  Median:     {df['text_length'].median():.1f}\")\n",
    "    print(f\"  Shortest:       {df['text_length'].min()} chars\")\n",
    "    print(f\"  Longest:      {df['text_length'].max()} chars\")\n",
    "    print()\n",
    "    \n",
    "    print(f\"  Avg words: {df['word_count'].mean():.1f}\")\n",
    "    print(f\"  Median:    {df['word_count'].median():.1f}\")\n",
    "    print(f\"  Fewest:      {df['word_count'].min()} words\")\n",
    "    print(f\"  Most:      {df['word_count'].max()} words\")\n",
    "    print()\n",
    "    \n",
    "    # Text length distribution\n",
    "    print(\"Text length distribution:\")\n",
    "    print(\"-\" * 80)\n",
    "    length_bins = [\n",
    "        (0, 50, \"Very short (<50 chars)\"),\n",
    "        (50, 100, \"Short (50-100)\"),\n",
    "        (100, 150, \"Medium (100-150)\"),\n",
    "        (150, 200, \"Long (150-200)\"),\n",
    "        (200, float('inf'), \"Very long (>200)\")\n",
    "    ]\n",
    "    \n",
    "    for low, high, desc in length_bins:\n",
    "        count = len(df[(df['text_length'] > low) & (df['text_length'] <= high)])\n",
    "        percentage = count / len(df) * 100\n",
    "        bar = \"â–ˆ\" * int(percentage / 2)\n",
    "        print(f\"  {desc:<25} {count:>5} items ({percentage:>5.1f}%) {bar}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Check nulls and duplicates\n",
    "    print(\"Data completeness:\")\n",
    "    print(\"-\" * 80)\n",
    "    null_count = df['text'].isnull().sum()\n",
    "    empty_count = (df['text'].str.strip() == '').sum()\n",
    "    duplicate_count = df.duplicated(subset=['ticker', 'text']).sum()\n",
    "    \n",
    "    print(f\"  Null count: {null_count}\")\n",
    "    print(f\"  Empty text:   {empty_count}\")\n",
    "    print(f\"  Duplicates: {duplicate_count}\")\n",
    "    \n",
    "    if null_count == 0 and empty_count == 0:\n",
    "        print(f\"  Rating: â­â­â­â­â­ Data is complete\")\n",
    "    else:\n",
    "        print(f\"  Rating: âš ï¸  Issues exist, cleaning needed\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def time_distribution(df):\n",
    "    \"\"\"Time distribution analysis\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"6. Time Distribution Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # Monthly stats\n",
    "    df['year_month'] = df['date'].dt.to_period('M')\n",
    "    monthly_counts = df.groupby('year_month').size().sort_index()\n",
    "    \n",
    "    print(\"Monthly distribution:\")\n",
    "    print(\"-\" * 80)\n",
    "    for period, count in monthly_counts.items():\n",
    "        percentage = count / len(df) * 100\n",
    "        bar = \"â–ˆ\" * int(percentage / 2)\n",
    "        print(f\"  {period}:  {count:>5} items ({percentage:>5.1f}%) {bar}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Weekday stats\n",
    "    df['weekday'] = df['date'].dt.day_name()\n",
    "    weekday_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "    weekday_counts = df['weekday'].value_counts()\n",
    "    \n",
    "    print(\"Weekday distribution:\")\n",
    "    print(\"-\" * 80)\n",
    "    for day in weekday_order:\n",
    "        if day in weekday_counts.index:\n",
    "            count = weekday_counts[day]\n",
    "            percentage = count / len(df) * 100\n",
    "            bar = \"â–ˆ\" * int(percentage / 3)\n",
    "            print(f\"  {day:<10}:  {count:>5} items ({percentage:>5.1f}%) {bar}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def sample_display(df):\n",
    "    \"\"\"Sample display\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"7. Sample Data Display\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    for label in ['positive', 'negative', 'neutral']:\n",
    "        if label in df['label'].values:\n",
    "            print(f\"{label.upper()} samples (random 3):\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            samples = df[df['label'] == label].sample(min(3, len(df[df['label'] == label])))\n",
    "            \n",
    "            for idx, (i, row) in enumerate(samples.iterrows(), 1):\n",
    "                print(f\"\\n{idx}. Ticker: {row['ticker']} | Date: {row['date'].strftime('%Y-%m-%d')} | \"\n",
    "                      f\"Price change: {row['price_change']:+.2f}%\")\n",
    "                print(f\"   Text:  {row['text'][:100]}\")\n",
    "                if len(row['text']) > 100:\n",
    "                    print(f\"        {row['text'][100:200]}\")\n",
    "                    if len(row['text']) > 200:\n",
    "                        print(f\"        ...\")\n",
    "            print()\n",
    "\n",
    "def overall_quality_score(df):\n",
    "    \"\"\"Overall quality score\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"8. Overall Quality Score\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    scores = {}\n",
    "    \n",
    "    # 1. Data size (20 points)\n",
    "    data_size = len(df)\n",
    "    if data_size >= 8000:\n",
    "        scores['Data Size'] = 20\n",
    "    elif data_size >= 5000:\n",
    "        scores['Data Size'] = 15\n",
    "    elif data_size >= 3000:\n",
    "        scores['Data Size'] = 10\n",
    "    else:\n",
    "        scores['Data Size'] = 5\n",
    "    \n",
    "    # 2. Label balance (20 points)\n",
    "    label_counts = df['label'].value_counts()\n",
    "    balance_ratio = label_counts.min() / label_counts.max()\n",
    "    if balance_ratio >= 0.8:\n",
    "        scores['Label Balance'] = 20\n",
    "    elif balance_ratio >= 0.6:\n",
    "        scores['Label Balance'] = 15\n",
    "    elif balance_ratio >= 0.4:\n",
    "        scores['Label Balance'] = 10\n",
    "    else:\n",
    "        scores['Label Balance'] = 5\n",
    "    \n",
    "    # 3. Stock coverage (15 points)\n",
    "    ticker_count = df['ticker'].nunique()\n",
    "    if ticker_count >= 10:\n",
    "        scores['Stock Coverage'] = 15\n",
    "    elif ticker_count >= 7:\n",
    "        scores['Stock Coverage'] = 10\n",
    "    else:\n",
    "        scores['Stock Coverage'] = 5\n",
    "    \n",
    "    # 4. Text quality (15 points)\n",
    "    avg_length = df['text'].str.len().mean()\n",
    "    null_count = df['text'].isnull().sum()\n",
    "    if avg_length >= 80 and null_count == 0:\n",
    "        scores['Text Quality'] = 15\n",
    "    elif avg_length >= 50 and null_count == 0:\n",
    "        scores['Text Quality'] = 10\n",
    "    else:\n",
    "        scores['Text Quality'] = 5\n",
    "    \n",
    "    # 5. Price change reasonability (15 points)\n",
    "    price_std = df['price_change'].std()\n",
    "    if 1.0 <= price_std <= 5.0:\n",
    "        scores['Price Change'] = 15\n",
    "    elif 0.5 <= price_std <= 8.0:\n",
    "        scores['Price Change'] = 10\n",
    "    else:\n",
    "        scores['Price Change'] = 5\n",
    "    \n",
    "    # 6. Data completeness (15 points)\n",
    "    complete_ratio = 1 - (df.isnull().sum().sum() / (len(df) * len(df.columns)))\n",
    "    if complete_ratio >= 0.99:\n",
    "        scores['Completeness'] = 15\n",
    "    elif complete_ratio >= 0.95:\n",
    "        scores['Completeness'] = 10\n",
    "    else:\n",
    "        scores['Completeness'] = 5\n",
    "    \n",
    "    # Display scores\n",
    "    print(\"Score breakdown:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Category':<18} {'Score':>8} {'Max':>8} {'Grade':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    total_score = 0\n",
    "    for item, score in scores.items():\n",
    "        if item == 'Data Size' or item == 'Label Balance': \n",
    "            max_score = 20\n",
    "        else:\n",
    "            max_score = 15\n",
    "        \n",
    "        total_score += score\n",
    "        \n",
    "        if score >= max_score * 0.9:\n",
    "            grade = \"â­â­â­â­â­\"\n",
    "        elif score >= max_score * 0.7:\n",
    "            grade = \"â­â­â­â­\"\n",
    "        elif score >= max_score * 0.5:\n",
    "            grade = \"â­â­â­\"\n",
    "        else:\n",
    "            grade = \"â­â­\"\n",
    "        \n",
    "        print(f\"{item:<18} {score:>8} {max_score:>8} {grade:>10}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Total':<18} {total_score:>8} {'100':>8}\")\n",
    "    print()\n",
    "    \n",
    "    # Overall assessment\n",
    "    if total_score >= 90:\n",
    "        overall = \"â­â­â­â­â­ Excellent - Very high data quality, ready for training\"\n",
    "    elif total_score >= 75:\n",
    "        overall = \"â­â­â­â­ Good - High data quality, suitable for training\"\n",
    "    elif total_score >= 60:\n",
    "        overall = \"â­â­â­ Fair - Acceptable quality, optimization recommended\"\n",
    "    else: \n",
    "        overall = \"â­â­ Poor - Data quality improvement needed\"\n",
    "    \n",
    "    print(f\"Overall assessment: {overall}\")\n",
    "    print()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    # Load data\n",
    "    df = load_data()\n",
    "    \n",
    "    # 1. Basic statistics\n",
    "    basic_statistics(df)\n",
    "    \n",
    "    # 2. Label distribution\n",
    "    label_distribution(df)\n",
    "    \n",
    "    # 3. Price change analysis\n",
    "    price_change_analysis(df)\n",
    "    \n",
    "    # 4. Stock distribution\n",
    "    ticker_distribution(df)\n",
    "    \n",
    "    # 5. Text quality\n",
    "    text_quality_analysis(df)\n",
    "    \n",
    "    # 6. Time distribution\n",
    "    time_distribution(df)\n",
    "    \n",
    "    # 7. Sample display\n",
    "    sample_display(df)\n",
    "    \n",
    "    # 8. Overall score\n",
    "    overall_quality_score(df)\n",
    "    \n",
    "    # Complete\n",
    "    print(\"=\" * 80)\n",
    "    print(\"âœ… Analysis complete!\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"Recommendations:\")\n",
    "    print(\"  1. If overall score >= 75, you can directly use this for FinBERT training\")\n",
    "    print(\"  2. If any category scores low, you can optimize specifically\")\n",
    "    print(\"  3. Consider saving the analysis results for your project report\")\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\": \n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df5f482",
   "metadata": {},
   "source": [
    "##### 1.8: Convert training set from 3-class to 7-class (Surge, Rise, Small_rise, Stable, Small_drop, drop, crash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936cfb08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¯ é‡æ–°æ ‡æ³¨è®­ç»ƒæ•°æ®ä¸º7åˆ†ç±»\n",
      "================================================================================\n",
      "\n",
      "å°†æŠŠ 3åˆ†ç±» (positive/negative/neutral)\n",
      "è½¬æ¢ä¸º 7åˆ†ç±» (crash/drop/small_drop/stable/small_rise/rise/surge)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‚ ç¬¬1æ­¥ï¼šåŠ è½½åŸå§‹æ•°æ®\n",
      "================================================================================\n",
      "\n",
      "âœ… æ•°æ®åŠ è½½æˆåŠŸ: 8377 æ¡\n",
      "\n",
      "åŸå§‹3åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ:\n",
      "------------------------------------------------------------\n",
      "  positive  :  3020 æ¡ ( 36.1%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  negative  :  1983 æ¡ ( 23.7%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "  neutral   :  3374 æ¡ ( 40.3%) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ·ï¸  ç¬¬2æ­¥ï¼šé‡æ–°æ ‡æ³¨ä¸º7åˆ†ç±»\n",
      "================================================================================\n",
      "\n",
      "æ ‡æ³¨è§„åˆ™:\n",
      "------------------------------------------------------------\n",
      "  crash        (å¤§è·Œ): < -5%\n",
      "  drop         (ä¸­è·Œ): -5% ~ -2%\n",
      "  small_drop   (å°è·Œ): -2% ~ -1%\n",
      "  stable       (éœ‡è¡): -1% ~ 1%\n",
      "  small_rise   (å°æ¶¨): 1% ~ 2%\n",
      "  rise         (ä¸­æ¶¨): 2% ~ 5%\n",
      "  surge        (å¤§æ¶¨): > 5%\n",
      "\n",
      "å¼€å§‹æ ‡æ³¨...\n",
      "âœ… æ ‡æ³¨å®Œæˆ: 8377 / 8377 æ¡\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ç¬¬3æ­¥ï¼šåˆ†ææ–°æ ‡ç­¾åˆ†å¸ƒ\n",
      "================================================================================\n",
      "\n",
      "7åˆ†ç±»æ ‡ç­¾ç»Ÿè®¡:\n",
      "--------------------------------------------------------------------------------\n",
      "æ ‡ç­¾           ä¸­æ–‡       åŒºé—´                    æ•°é‡        ç™¾åˆ†æ¯” å¯è§†åŒ–                           \n",
      "--------------------------------------------------------------------------------\n",
      "crash        å¤§è·Œ       < -5%                102       1.2% \n",
      "drop         ä¸­è·Œ       -5% ~ -2%            880      10.5% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "small_drop   å°è·Œ       -2% ~ -1%           1001      11.9% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "stable       éœ‡è¡       -1% ~ 1%            3374      40.3% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "small_rise   å°æ¶¨       1% ~ 2%             2018      24.1% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "rise         ä¸­æ¶¨       2% ~ 5%              837      10.0% â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "surge        å¤§æ¶¨       > 5%                 165       2.0% \n",
      "--------------------------------------------------------------------------------\n",
      "æ€»è®¡                                      8377    100. 0%\n",
      "\n",
      "æ•°æ®å¹³è¡¡æ€§åˆ†æ:\n",
      "------------------------------------------------------------\n",
      "  æœ€å¤šç±»åˆ«: stable (3374 æ¡)\n",
      "  æœ€å°‘ç±»åˆ«: crash (102 æ¡)\n",
      "  å¹³è¡¡æ¯”ä¾‹: 0.030 (æœ€å°‘/æœ€å¤š)\n",
      "\n",
      "  è¯„ä¼°: â­â­ åˆ†å¸ƒå¾ˆä¸å‡è¡¡\n",
      "  å»ºè®®: è€ƒè™‘åˆå¹¶ç›¸ä¼¼ç±»åˆ«æˆ–ä½¿ç”¨è¿‡é‡‡æ ·\n",
      "\n",
      "å„æ ‡ç­¾çš„ä»·æ ¼å˜åŒ–ç»Ÿè®¡:\n",
      "--------------------------------------------------------------------------------\n",
      "æ ‡ç­¾                  å¹³å‡å€¼        ä¸­ä½æ•°        æ ‡å‡†å·®        æœ€å°å€¼        æœ€å¤§å€¼\n",
      "--------------------------------------------------------------------------------\n",
      "crash           -11.07%    -13.15%      3.43%    -13.15%     -5.15%\n",
      "drop             -3.26%     -3.27%      0.80%     -4.66%     -2.05%\n",
      "small_drop       -1.40%     -1.35%      0.33%     -1.97%     -1.00%\n",
      "stable            0.11%      0.12%      0.50%     -0.87%      1.00%\n",
      "small_rise        1.42%      1.42%      0.27%      1.01%      2.00%\n",
      "rise              2.70%      2.45%      0.57%      2.05%      4.08%\n",
      "surge             6.49%      6.32%      0.23%      6.32%      6.82%\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ” ç¬¬4æ­¥ï¼šæŸ¥çœ‹æ ·æœ¬æ•°æ®\n",
      "================================================================================\n",
      "\n",
      "CRASH (å¤§è·Œ < -5%) - å…± 102 æ¡\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  1. TSLA | 2025-11-03 | å˜åŒ–: -5.15%\n",
      "     Elon Musk Says Bill Gates Is 'Not Strong' In Science, Recalling Visit To Tesla Factory Where He Dism\n",
      "     ...\n",
      "\n",
      "  2. TSLA | 2025-11-03 | å˜åŒ–: -5.15%\n",
      "     Ford reports October sales gain after CEO warns of future industry threat\n",
      "\n",
      "  3. TSLA | 2025-11-03 | å˜åŒ–: -5.15%\n",
      "     This Silicon Valley startup is heading to China to make its batteries\n",
      "\n",
      "\n",
      "DROP (ä¸­è·Œ -5% ~ -2%) - å…± 880 æ¡\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  1. TSLA | 2025-12-05 | å˜åŒ–: -3.39%\n",
      "     3 Reasons to Sell GM and 1 Stock to Buy Instead\n",
      "\n",
      "  2. JPM | 2025-12-08 | å˜åŒ–: -4.66%\n",
      "     Financial Institutions, Inc. $FISI Shares Sold by Franklin Resources Inc.\n",
      "\n",
      "  3. GS | 2025-12-12 | å˜åŒ–: -2.82%\n",
      "     REG - Goldman Sachs  Co. Qualcomm Inc - Form 8.5 (EPTNON-RI) - Amend - Qualcomm INC\n",
      "\n",
      "\n",
      "SMALL_DROP (å°è·Œ -2% ~ -1%) - å…± 1001 æ¡\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  1. MSFT | 2025-12-11 | å˜åŒ–: -1.02%\n",
      "     The latest from the evolving AI channel: 11 December: ThoughtSpot, F5, NetApp...\n",
      "\n",
      "  2. GOOGL | 2025-11-25 | å˜åŒ–: -1.08%\n",
      "     Alphabet (GOOG) Class C Stock Jumps Again on Meta Chip Talks, AI Momentum and Buffett Stake  Novembe\n",
      "     ...\n",
      "\n",
      "  3. WMT | 2025-12-09 | å˜åŒ–: -1.64%\n",
      "     Which stores are offering extended holiday hours? See Best Buy, Target hours and more\n",
      "\n",
      "\n",
      "STABLE (éœ‡è¡ -1% ~ 1%) - å…± 3374 æ¡\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  1. AAPL | 2025-11-26 | å˜åŒ–: +0.47%\n",
      "     General Motors Just Lost Its Chief AI Officer After Only 8 Months\n",
      "\n",
      "  2. GOOGL | 2025-11-27 | å˜åŒ–: +0.07%\n",
      "     Alphabet Inc. $GOOGL Shares Acquired by L  S Advisors Inc\n",
      "\n",
      "  3. GOOGL | 2025-11-17 | å˜åŒ–: -0.26%\n",
      "     SP 500 rises, Dow Jones breaches records but NASDAQ falls as Wall Street ends day on a high: Leifras\n",
      "     ...\n",
      "\n",
      "\n",
      "SMALL_RISE (å°æ¶¨ 1% ~ 2%) - å…± 2018 æ¡\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  1. AAPL | 2025-11-29 | å˜åŒ–: +1.52%\n",
      "     United Capital Management of KS Inc. Boosts Position in Apple Inc. $AAPL\n",
      "\n",
      "  2. AAPL | 2025-11-23 | å˜åŒ–: +1.63%\n",
      "     BlueChip Wealth Advisors LLC Raises Stake in Apple Inc. $AAPL\n",
      "\n",
      "  3. BAC | 2025-12-09 | å˜åŒ–: +1.01%\n",
      "     Ossiam Sells 898,878 Shares of Bank of America Corporation $BAC\n",
      "\n",
      "\n",
      "RISE (ä¸­æ¶¨ 2% ~ 5%) - å…± 837 æ¡\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  1. GOOGL | 2025-11-18 | å˜åŒ–: +3.00%\n",
      "     Direxion Daily GOOGL Bull 2X Shares (NASDAQ:GGLL) Sets New 1-Year High - What's Next?\n",
      "\n",
      "  2. TSLA | 2025-12-12 | å˜åŒ–: +2.43%\n",
      "     Celcuity stock hits all-time high at 109.15 USD\n",
      "\n",
      "  3. NVDA | 2025-12-03 | å˜åŒ–: +2.12%\n",
      "     XPeng (XPEV) Drops 7.9% on Lower Nov Deliveries\n",
      "\n",
      "\n",
      "SURGE (å¤§æ¶¨ > 5%) - å…± 165 æ¡\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "  1. GOOGL | 2025-11-21 | å˜åŒ–: +6.32%\n",
      "     I Liked Alphabet (GOOGL) CEO's Skepticism, Says Jim Cramer\n",
      "\n",
      "  2. TSLA | 2025-11-23 | å˜åŒ–: +6.82%\n",
      "     Cybertruck 'The Best Robot Out There,' Says Jack Dorsey As Elon Musk Takes Note\n",
      "\n",
      "  3. TSLA | 2025-11-23 | å˜åŒ–: +6.82%\n",
      "     Prudential Financial Inc. Acquires 114,681 Shares of Tesla, Inc. $TSLA\n",
      "\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¾ ç¬¬5æ­¥ï¼šä¿å­˜æ•°æ®\n",
      "================================================================================\n",
      "\n",
      "âœ… å®Œæ•´æ•°æ®å·²ä¿å­˜:  financial_data/training_data_7class_full.csv\n",
      "   åŒ…å«åˆ—: ticker, date, text, label, price_change, label_7class\n",
      "\n",
      "âœ… è®­ç»ƒæ•°æ®å·²ä¿å­˜: financial_data/training_data_7class.csv\n",
      "   åŒ…å«åˆ—: ticker, date, text, label, price_change\n",
      "\n",
      "   æ–‡ä»¶å¤§å°: 886.42 KB\n",
      "   æ•°æ®æ¡æ•°: 8377\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ ç¬¬6æ­¥ï¼šç”Ÿæˆå¯è§†åŒ–ï¼ˆå¯é€‰ï¼‰\n",
      "================================================================================\n",
      "\n",
      "âœ… å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: financial_data/training_data_7class_distribution.png\n",
      "\n",
      "================================================================================\n",
      "âœ… ç¬¬1æ­¥å®Œæˆï¼æ•°æ®é‡æ–°æ ‡æ³¨æˆåŠŸï¼\n",
      "================================================================================\n",
      "\n",
      "ç”Ÿæˆæ–‡ä»¶:\n",
      "  â€¢ financial_data/training_data_7class.csv\n",
      "  â€¢ financial_data/training_data_7class_full. csv\n",
      "\n",
      "ä¸‹ä¸€æ­¥:\n",
      "  è¿è¡Œ train_finbert_7class.py å¼€å§‹è®­ç»ƒæ¨¡å‹\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# step4_relabel_7class.py\n",
    "\"\"\"\n",
    "Step 4: Relabel training data from 3-class to 7-class\n",
    "Divide into 7 sub-intervals based on actual price changes\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "INPUT_FILE = 'training_data_2025.csv'\n",
    "OUTPUT_FILE = 'financial_data/training_data_7class.csv'\n",
    "\n",
    "# 7 class definitions\n",
    "LABEL_DEFINITIONS = {\n",
    "    'crash': {\n",
    "        'name': 'å¤§è·Œ',\n",
    "        'range': '< -5%',\n",
    "        'threshold': (-float('inf'), -5)\n",
    "    },\n",
    "    'drop': {\n",
    "        'name': 'ä¸­è·Œ',\n",
    "        'range': '-5% ~ -2%',\n",
    "        'threshold': (-5, -2)\n",
    "    },\n",
    "    'small_drop': {\n",
    "        'name': 'å°è·Œ',\n",
    "        'range': '-2% ~ -1%',\n",
    "        'threshold': (-2, -1)\n",
    "    },\n",
    "    'stable': {\n",
    "        'name': 'éœ‡è¡',\n",
    "        'range': '-1% ~ 1%',\n",
    "        'threshold': (-1, 1)\n",
    "    },\n",
    "    'small_rise': {\n",
    "        'name': 'å°æ¶¨',\n",
    "        'range': '1% ~ 2%',\n",
    "        'threshold': (1, 2)\n",
    "    },\n",
    "    'rise': {\n",
    "        'name': 'ä¸­æ¶¨',\n",
    "        'range': '2% ~ 5%',\n",
    "        'threshold': (2, 5)\n",
    "    },\n",
    "    'surge': {\n",
    "        'name': 'å¤§æ¶¨',\n",
    "        'range': '> 5%',\n",
    "        'threshold': (5, float('inf'))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Label order (from big loss to big gain)\n",
    "LABELS_ORDER = ['crash', 'drop', 'small_drop', 'stable', 'small_rise', 'rise', 'surge']\n",
    "\n",
    "def print_section(title):\n",
    "    \"\"\"Print section divider\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def assign_7class_label(price_change):\n",
    "    \"\"\"\n",
    "    Assign 7-class label based on price change\n",
    "    \n",
    "    Args: \n",
    "        price_change: Price change percentage\n",
    "        \n",
    "    Returns:\n",
    "        Label name\n",
    "    \"\"\"\n",
    "    if pd.isna(price_change):\n",
    "        return None\n",
    "    \n",
    "    for label in LABELS_ORDER:\n",
    "        low, high = LABEL_DEFINITIONS[label]['threshold']\n",
    "        if low < price_change <= high:\n",
    "            return label\n",
    "    \n",
    "    # Fallback: if no match (theoretically shouldn't happen)\n",
    "    if price_change <= -5:\n",
    "        return 'crash'\n",
    "    else:\n",
    "        return 'surge'\n",
    "\n",
    "def load_and_relabel():\n",
    "    \"\"\"Load data and relabel\"\"\"\n",
    "    print_section(\"ğŸ“‚ Step 1: Load original data\")\n",
    "    \n",
    "    # Read data\n",
    "    if not os.path.exists(INPUT_FILE):\n",
    "        print(f\"âŒ Error: Cannot find file {INPUT_FILE}\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(INPUT_FILE, encoding='utf-8-sig')\n",
    "    print(f\"âœ… Data loaded successfully: {len(df)} entries\")\n",
    "    print()\n",
    "    \n",
    "    # Check required columns\n",
    "    if 'price_change' not in df.columns:\n",
    "        print(\"âŒ Error: Data does not have 'price_change' column\")\n",
    "        return None\n",
    "    \n",
    "    # Show original 3-class distribution\n",
    "    if 'label' in df.columns:\n",
    "        print(\"Original 3-class label distribution:\")\n",
    "        print(\"-\" * 60)\n",
    "        for label in ['positive', 'negative', 'neutral']:\n",
    "            if label in df['label'].values:\n",
    "                count = len(df[df['label'] == label])\n",
    "                percentage = count / len(df) * 100\n",
    "                bar = \"â–ˆ\" * int(percentage / 2)\n",
    "                print(f\"  {label.ljust(10)}: {count:>5} entries ({percentage:5.1f}%) {bar}\")\n",
    "        print()\n",
    "    \n",
    "    # Relabel to 7-class\n",
    "    print_section(\"ğŸ·ï¸ Step 2: Relabel to 7-class\")\n",
    "    \n",
    "    print(\"Labeling rules:\")\n",
    "    print(\"-\" * 60)\n",
    "    for label in LABELS_ORDER: \n",
    "        info = LABEL_DEFINITIONS[label]\n",
    "        print(f\"  {label.ljust(12)} ({info['name']}): {info['range']}\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Starting labeling...\")\n",
    "    df['label_7class'] = df['price_change'].apply(assign_7class_label)\n",
    "    \n",
    "    # Count labeling results\n",
    "    labeled_count = df['label_7class'].notna().sum()\n",
    "    print(f\"âœ… Labeling complete: {labeled_count} / {len(df)} entries\")\n",
    "    \n",
    "    if labeled_count < len(df):\n",
    "        null_count = len(df) - labeled_count\n",
    "        print(f\"âš ï¸ {null_count} entries cannot be labeled (price_change is null)\")\n",
    "        df = df[df['label_7class'].notna()].copy()\n",
    "        print(f\"   Removed, remaining {len(df)} entries\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def analyze_distribution(df):\n",
    "    \"\"\"Analyze 7-class label distribution\"\"\"\n",
    "    print_section(\"ğŸ“Š Step 3: Analyze new label distribution\")\n",
    "    \n",
    "    print(\"7-class label statistics:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Label':<12} {'Name':<12} {'Range':<15} {'Count':>8} {'Percentage':>10} {'Visualization':<30}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    total = len(df)\n",
    "    \n",
    "    for label in LABELS_ORDER:\n",
    "        info = LABEL_DEFINITIONS[label]\n",
    "        count = len(df[df['label_7class'] == label])\n",
    "        percentage = count / total * 100\n",
    "        bar = \"â–ˆ\" * int(percentage / 2)\n",
    "        \n",
    "        print(f\"{label:<12} {info['name']:<12} {info['range']:<15} {count:>8} {percentage:>9.1f}% {bar}\")\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Total':<39} {total:>8} {'100.0':>9}%\")\n",
    "    print()\n",
    "    \n",
    "    # Data balance analysis\n",
    "    print(\"Data balance analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    counts = df['label_7class'].value_counts()\n",
    "    max_count = counts.max()\n",
    "    min_count = counts.min()\n",
    "    balance_ratio = min_count / max_count\n",
    "    \n",
    "    print(f\"  Most frequent class: {counts.idxmax()} ({max_count} entries)\")\n",
    "    print(f\"  Least frequent class: {counts.idxmin()} ({min_count} entries)\")\n",
    "    print(f\"  Balance ratio: {balance_ratio:.3f} (min/max)\")\n",
    "    print()\n",
    "    \n",
    "    if balance_ratio >= 0.3:\n",
    "        print(\"  Assessment: â­â­â­â­ Distribution is reasonably balanced, suitable for training\")\n",
    "    elif balance_ratio >= 0.1:\n",
    "        print(\"  Assessment: â­â­â­ Distribution is somewhat uneven, but usable\")\n",
    "        print(\"  Suggestion: Consider using class weights during training\")\n",
    "    else:\n",
    "        print(\"  Assessment: â­â­ Distribution is very unbalanced\")\n",
    "        print(\"  Suggestion: Consider merging similar classes or using oversampling\")\n",
    "    print()\n",
    "    \n",
    "    # Price change statistics for each label\n",
    "    print(\"Price change statistics for each label:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Label':<12} {'Mean':>10} {'Median':>10} {'Std Dev':>10} {'Min':>10} {'Max':>10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for label in LABELS_ORDER:\n",
    "        label_data = df[df['label_7class'] == label]['price_change']\n",
    "        if len(label_data) > 0:\n",
    "            print(f\"{label:<12} {label_data.mean():>9.2f}% {label_data.median():>9.2f}% \"\n",
    "                  f\"{label_data.std():>9.2f}% {label_data.min():>9.2f}% {label_data.max():>9.2f}%\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "def show_samples(df):\n",
    "    \"\"\"Show samples for each category\"\"\"\n",
    "    print_section(\"ğŸ” Step 4: View sample data\")\n",
    "    \n",
    "    for label in LABELS_ORDER: \n",
    "        label_data = df[df['label_7class'] == label]\n",
    "        \n",
    "        if len(label_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        info = LABEL_DEFINITIONS[label]\n",
    "        print(f\"{label.upper()} ({info['name']} {info['range']}) - Total {len(label_data)} entries\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Randomly sample 3 entries\n",
    "        samples = label_data.sample(min(3, len(label_data)))\n",
    "        \n",
    "        for idx, (i, row) in enumerate(samples.iterrows(), 1):\n",
    "            print(f\"\\n  {idx}. {row['ticker']} | {str(row['date'])[:10]} | Change: {row['price_change']:+.2f}%\")\n",
    "            print(f\"     {row['text'][:100]}\")\n",
    "            if len(row['text']) > 100:\n",
    "                print(f\"     ...\")\n",
    "        \n",
    "        print(\"\\n\")\n",
    "\n",
    "def save_data(df):\n",
    "    \"\"\"Save relabeled data\"\"\"\n",
    "    print_section(\"ğŸ’¾ Step 5: Save data\")\n",
    "    \n",
    "    # Keep original label, add new label_7class\n",
    "    # Also create a clean version for training\n",
    "    \n",
    "    # Save full version (including all columns)\n",
    "    full_output = OUTPUT_FILE.replace('.csv', '_full.csv')\n",
    "    df.to_csv(full_output, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… Full data saved: {full_output}\")\n",
    "    print(f\"   Columns: {', '.join(df.columns)}\")\n",
    "    print()\n",
    "    \n",
    "    # Save training version (only necessary columns)\n",
    "    training_columns = ['ticker', 'date', 'text', 'label_7class', 'price_change']\n",
    "    df_training = df[training_columns].copy()\n",
    "    df_training = df_training.rename(columns={'label_7class': 'label'})\n",
    "    \n",
    "    df_training.to_csv(OUTPUT_FILE, index=False, encoding='utf-8-sig')\n",
    "    print(f\"âœ… Training data saved: {OUTPUT_FILE}\")\n",
    "    print(f\"   Columns: {', '.join(df_training.columns)}\")\n",
    "    print()\n",
    "    \n",
    "    # Show file size\n",
    "    import os\n",
    "    file_size = os.path.getsize(OUTPUT_FILE) / 1024\n",
    "    print(f\"   File size: {file_size:.2f} KB\")\n",
    "    print(f\"   Number of entries: {len(df_training)}\")\n",
    "    print()\n",
    "\n",
    "def create_visualization(df):\n",
    "    \"\"\"Create visualization charts (optional)\"\"\"\n",
    "    print_section(\"ğŸ“ˆ Step 6: Generate visualization (optional)\")\n",
    "    \n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']\n",
    "        plt.rcParams['axes.unicode_minus'] = False\n",
    "        \n",
    "        # Create charts\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Chart 1: Label distribution\n",
    "        label_counts = df['label_7class'].value_counts().reindex(LABELS_ORDER)\n",
    "        colors = ['#d32f2f', '#f57c00', '#fbc02d', '#9e9e9e', '#8bc34a', '#4caf50', '#2e7d32']\n",
    "        \n",
    "        axes[0].bar(range(len(LABELS_ORDER)), label_counts.values, color=colors)\n",
    "        axes[0].set_xticks(range(len(LABELS_ORDER)))\n",
    "        axes[0].set_xticklabels([LABEL_DEFINITIONS[l]['name'] for l in LABELS_ORDER], rotation=45)\n",
    "        axes[0].set_ylabel('Count')\n",
    "        axes[0].set_title('7-Class Label Distribution')\n",
    "        axes[0].grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Add value labels\n",
    "        for i, v in enumerate(label_counts.values):\n",
    "            axes[0].text(i, v + 50, str(v), ha='center', va='bottom')\n",
    "        \n",
    "        # Chart 2: Price change distribution\n",
    "        axes[1].hist(df['price_change'], bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "        axes[1].axvline(x=-5, color='red', linestyle='--', alpha=0.5, label='Big/Medium Loss')\n",
    "        axes[1].axvline(x=-2, color='orange', linestyle='--', alpha=0.5, label='Medium/Small Loss')\n",
    "        axes[1].axvline(x=-1, color='yellow', linestyle='--', alpha=0.5, label='Small Loss/Flat')\n",
    "        axes[1].axvline(x=1, color='lightgreen', linestyle='--', alpha=0.5, label='Flat/Small Gain')\n",
    "        axes[1].axvline(x=2, color='green', linestyle='--', alpha=0.5, label='Small/Medium Gain')\n",
    "        axes[1].axvline(x=5, color='darkgreen', linestyle='--', alpha=0.5, label='Medium/Big Gain')\n",
    "        axes[1].set_xlabel('Price Change (%)')\n",
    "        axes[1].set_ylabel('Frequency')\n",
    "        axes[1].set_title('Price Change Distribution with Classification Thresholds')\n",
    "        axes[1].legend(fontsize=8)\n",
    "        axes[1].grid(alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save chart\n",
    "        output_image = OUTPUT_FILE.replace('.csv', '_distribution.png')\n",
    "        plt.savefig(output_image, dpi=150, bbox_inches='tight')\n",
    "        print(f\"âœ… Visualization chart saved: {output_image}\")\n",
    "        print()\n",
    "        \n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Visualization generation failed: {e}\")\n",
    "        print(\"   (This does not affect training, can be skipped)\")\n",
    "        print()\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ¯ Relabel Training Data to 7-Class\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(\"Converting from 3-class (positive/negative/neutral)\")\n",
    "    print(\"to 7-class (crash/drop/small_drop/stable/small_rise/rise/surge)\")\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    print()\n",
    "    \n",
    "    # 1. Load and relabel\n",
    "    df = load_and_relabel()\n",
    "    if df is None: \n",
    "        return\n",
    "    \n",
    "    # 2. Analyze distribution\n",
    "    analyze_distribution(df)\n",
    "    \n",
    "    # 3. Show samples\n",
    "    show_samples(df)\n",
    "    \n",
    "    # 4. Save data\n",
    "    save_data(df)\n",
    "    \n",
    "    # 5. Generate visualization\n",
    "    create_visualization(df)\n",
    "    \n",
    "    # Complete\n",
    "    print(\"=\" * 80)\n",
    "    print(\"âœ… Step 1 Complete! Data relabeling successful!\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    print(f\"Generated files:\")\n",
    "    print(f\"  â€¢ {OUTPUT_FILE}\")\n",
    "    print(f\"  â€¢ {OUTPUT_FILE.replace('.csv', '_full.csv')}\")\n",
    "    print()\n",
    "    print(\"Next step:\")\n",
    "    print(\"  Run train_finbert_7class.py to start training the model\")\n",
    "    print()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1beb592",
   "metadata": {},
   "source": [
    "## PART 2: Using FinBERT to Train a 7-Class Classification Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a21227a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ğŸ¯ FinBERT 7åˆ†ç±»æ¨¡å‹è®­ç»ƒ\n",
      "================================================================================\n",
      "\n",
      "å¼€å§‹æ—¶é—´: 2025-12-13 22:23:34\n",
      "æ•°æ®æ–‡ä»¶: financial_data/training_data_7class.csv\n",
      "è¾“å‡ºç›®å½•: finbert_7class_model\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“‚ ç¬¬1æ­¥ï¼šåŠ è½½æ•°æ®\n",
      "================================================================================\n",
      "\n",
      "âœ… æ•°æ®åŠ è½½æˆåŠŸ:  8377 æ¡\n",
      "âœ… æœ‰æ•ˆæ•°æ®: 8377 æ¡\n",
      "\n",
      "7åˆ†ç±»æ ‡ç­¾åˆ†å¸ƒ:\n",
      "--------------------------------------------------------------------------------\n",
      "æ ‡ç­¾           ä¸­æ–‡     åŒºé—´                    æ•°é‡        ç™¾åˆ†æ¯” å¯è§†åŒ–                           \n",
      "--------------------------------------------------------------------------------\n",
      "crash        å¤§è·Œ     < -5%                102       1.2% \n",
      "drop         ä¸­è·Œ     -5% ~ -2%            880      10.5% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "small_drop   å°è·Œ     -2% ~ -1%           1001      11.9% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "stable       éœ‡è¡     -1% ~ 1%            3374      40.3% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "small_rise   å°æ¶¨     1% ~ 2%             2018      24.1% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "rise         ä¸­æ¶¨     2% ~ 5%              837      10.0% â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "surge        å¤§æ¶¨     > 5%                 165       2.0% \n",
      "\n",
      "\n",
      "================================================================================\n",
      "âœ‚ï¸ ç¬¬2æ­¥ï¼šåˆ’åˆ†æ•°æ®é›†\n",
      "================================================================================\n",
      "\n",
      "è®­ç»ƒé›†:  5863 æ¡ (70%)\n",
      "éªŒè¯é›†: 1257 æ¡ (15%)\n",
      "æµ‹è¯•é›†: 1257 æ¡ (15%)\n",
      "\n",
      "å„æ•°æ®é›†æ ‡ç­¾åˆ†å¸ƒ:\n",
      "--------------------------------------------------------------------------------\n",
      "è®­ç»ƒé›†:\n",
      "  crash: 71\n",
      "  drop: 616\n",
      "  small_drop: 701\n",
      "  stable: 2361\n",
      "  small_rise: 1412\n",
      "  rise: 586\n",
      "  surge: 116\n",
      "\n",
      "éªŒè¯é›†:\n",
      "  crash: 15\n",
      "  drop: 132\n",
      "  small_drop: 150\n",
      "  stable: 507\n",
      "  small_rise: 303\n",
      "  rise: 126\n",
      "  surge: 24\n",
      "\n",
      "æµ‹è¯•é›†:\n",
      "  crash: 16\n",
      "  drop: 132\n",
      "  small_drop: 150\n",
      "  stable: 506\n",
      "  small_rise: 303\n",
      "  rise: 125\n",
      "  surge: 25\n",
      "\n",
      "\n",
      "================================================================================\n",
      "âš–ï¸ ç¬¬3æ­¥ï¼šè®¡ç®—ç±»åˆ«æƒé‡\n",
      "================================================================================\n",
      "\n",
      "ç±»åˆ«æƒé‡ï¼ˆç”¨äºå¹³è¡¡ä¸å‡è¡¡æ•°æ®ï¼‰:\n",
      "------------------------------------------------------------\n",
      "  crash        (å¤§è·Œ): 11.7968\n",
      "  drop         (ä¸­è·Œ): 1.3597\n",
      "  small_drop   (å°è·Œ): 1.1948\n",
      "  stable       (éœ‡è¡): 0.3548\n",
      "  small_rise   (å°æ¶¨): 0.5932\n",
      "  rise         (ä¸­æ¶¨): 1.4293\n",
      "  surge        (å¤§æ¶¨): 7.2204\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ”§ åŠ è½½ Tokenizer\n",
      "================================================================================\n",
      "\n",
      "âœ… Tokenizer åŠ è½½æˆåŠŸ\n",
      "\n",
      "================================================================================\n",
      "ğŸ”„ ç¬¬4æ­¥ï¼šæ•°æ®é¢„å¤„ç†\n",
      "================================================================================\n",
      "\n",
      "è½¬æ¢ä¸º Dataset æ ¼å¼...\n",
      "âœ… Dataset åˆ›å»ºæˆåŠŸ\n",
      "\n",
      "Tokenizing æ–‡æœ¬...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eaee92046b28489bbeb3b6eaf5e18f39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5863 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098754cd12a84bdaa57084e8e97637c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "312c350e6ff3460287ed90fbf129317c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1257 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Tokenizing å®Œæˆ\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸš€ ç¬¬5æ­¥ï¼šå¼€å§‹è®­ç»ƒ\n",
      "================================================================================\n",
      "\n",
      "ä½¿ç”¨è®¾å¤‡: cuda\n",
      "GPU:  NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "\n",
      "åŠ è½½ FinBERT é¢„è®­ç»ƒæ¨¡å‹...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ProsusAI/finbert and are newly initialized because the shapes did not match:\n",
      "- classifier.weight: found shape torch.Size([3, 768]) in the checkpoint and torch.Size([7, 768]) in the model instantiated\n",
      "- classifier.bias: found shape torch.Size([3]) in the checkpoint and torch.Size([7]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ\n",
      "   æ¨¡å‹å‚æ•°é‡: 109,487,623\n",
      "\n",
      "è®­ç»ƒé…ç½®:\n",
      "------------------------------------------------------------\n",
      "  Epochs: 10\n",
      "  Batch size:  16\n",
      "  Learning rate: 2e-05\n",
      "  æ€»è®­ç»ƒæ­¥æ•°: 3660\n",
      "  ä½¿ç”¨ç±»åˆ«æƒé‡: æ˜¯\n",
      "  Early stopping patience: 3 è½®\n",
      "\n",
      "å¼€å§‹è®­ç»ƒ...\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3670' max='3670' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3670/3670 07:53, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.918900</td>\n",
       "      <td>1.929901</td>\n",
       "      <td>0.185362</td>\n",
       "      <td>0.260759</td>\n",
       "      <td>0.185362</td>\n",
       "      <td>0.184609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.719900</td>\n",
       "      <td>1.802483</td>\n",
       "      <td>0.185362</td>\n",
       "      <td>0.234727</td>\n",
       "      <td>0.185362</td>\n",
       "      <td>0.143664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.650400</td>\n",
       "      <td>1.925867</td>\n",
       "      <td>0.199682</td>\n",
       "      <td>0.362848</td>\n",
       "      <td>0.199682</td>\n",
       "      <td>0.183674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.345400</td>\n",
       "      <td>1.946019</td>\n",
       "      <td>0.221161</td>\n",
       "      <td>0.335865</td>\n",
       "      <td>0.221161</td>\n",
       "      <td>0.223619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.125800</td>\n",
       "      <td>2.131968</td>\n",
       "      <td>0.245028</td>\n",
       "      <td>0.298651</td>\n",
       "      <td>0.245028</td>\n",
       "      <td>0.246421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.963100</td>\n",
       "      <td>2.350612</td>\n",
       "      <td>0.247414</td>\n",
       "      <td>0.329535</td>\n",
       "      <td>0.247414</td>\n",
       "      <td>0.259660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.845200</td>\n",
       "      <td>2.597356</td>\n",
       "      <td>0.272076</td>\n",
       "      <td>0.314367</td>\n",
       "      <td>0.272076</td>\n",
       "      <td>0.275644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.665200</td>\n",
       "      <td>2.643265</td>\n",
       "      <td>0.303898</td>\n",
       "      <td>0.352034</td>\n",
       "      <td>0.303898</td>\n",
       "      <td>0.318622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.586800</td>\n",
       "      <td>2.814600</td>\n",
       "      <td>0.303898</td>\n",
       "      <td>0.337569</td>\n",
       "      <td>0.303898</td>\n",
       "      <td>0.315045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.480700</td>\n",
       "      <td>2.871447</td>\n",
       "      <td>0.308671</td>\n",
       "      <td>0.337108</td>\n",
       "      <td>0.308671</td>\n",
       "      <td>0.318426</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "âœ… è®­ç»ƒå®Œæˆ!  è€—æ—¶: 0:07:54.040669\n",
      "   å®é™…è®­ç»ƒè½®æ•°: 10.0\n",
      "\n",
      "ä¿å­˜æ¨¡å‹...\n",
      "âœ… æ¨¡å‹å·²ä¿å­˜åˆ°:  finbert_7class_model\n",
      "   - æ¨¡å‹æƒé‡\n",
      "   - Tokenizer\n",
      "   - æ ‡ç­¾æ˜ å°„\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“Š ç¬¬6æ­¥ï¼šè¯„ä¼°æ¨¡å‹\n",
      "================================================================================\n",
      "\n",
      "åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°...\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æµ‹è¯•é›†ç»“æœ:\n",
      "------------------------------------------------------------\n",
      "  å‡†ç¡®ç‡ (Accuracy): 0.3023 (30.23%)\n",
      "  ç²¾ç¡®ç‡ (Precision): 0.3405\n",
      "  å¬å›ç‡ (Recall): 0.3023\n",
      "  F1-Score: 0.3156\n",
      "\n",
      "å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡:\n",
      "--------------------------------------------------------------------------------\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "     crash (å¤§è·Œ)     0.3684    0.4375    0.4000        16\n",
      "      drop (ä¸­è·Œ)     0.1908    0.2500    0.2164       132\n",
      "small_drop (å°è·Œ)     0.2000    0.2267    0.2125       150\n",
      "    stable (éœ‡è¡)     0.4881    0.3656    0.4181       506\n",
      "small_rise (å°æ¶¨)     0.3271    0.2871    0.3058       303\n",
      "      rise (ä¸­æ¶¨)     0.1458    0.2240    0.1767       125\n",
      "     surge (å¤§æ¶¨)     0.1034    0.2400    0.1446        25\n",
      "\n",
      "       accuracy                         0.3023      1257\n",
      "      macro avg     0.2605    0.2901    0.2677      1257\n",
      "   weighted avg     0.3405    0.3023    0.3156      1257\n",
      "\n",
      "\n",
      "æ··æ·†çŸ©é˜µ:\n",
      "--------------------------------------------------------------------------------\n",
      "          çœŸå®\\é¢„æµ‹     crash      dropsmall_drop    stablesmall_rise      rise     surge\n",
      "--------------------------------------------------------------------------------\n",
      "          crash         7         0         2         5         0         2         0\n",
      "           drop         1        33        16        29        26        26         1\n",
      "     small_drop         1        18        34        42        22        24         9\n",
      "         stable         6        62        60       185       100        66        27\n",
      "     small_rise         3        36        35        87        87        44        11\n",
      "           rise         1        22        18        26        26        28         4\n",
      "          surge         0         2         5         5         5         2         6\n",
      "\n",
      "é”™è¯¯æ ·æœ¬åˆ†æï¼ˆæ˜¾ç¤ºå‰5ä¸ªï¼‰:\n",
      "--------------------------------------------------------------------------------\n",
      "æ€»é”™è¯¯æ•°: 877 / 1257 (69.8%)\n",
      "\n",
      "\n",
      "é”™è¯¯æ ·æœ¬ 1:\n",
      "  æ–‡æœ¬: Working capital per share of Alphabet Inc (Google) Class A  BMV:GOOGL...\n",
      "  çœŸå®æ ‡ç­¾: small_rise (å°æ¶¨ 1% ~ 2%)\n",
      "  é¢„æµ‹æ ‡ç­¾: surge (å¤§æ¶¨ > 5%)\n",
      "  ä»·æ ¼å˜åŒ–: 1.21%\n",
      "\n",
      "é”™è¯¯æ ·æœ¬ 3:\n",
      "  æ–‡æœ¬: Sei Investments Co. Boosts Holdings in JPMorgan Chase  Co. $JPM...\n",
      "  çœŸå®æ ‡ç­¾: stable (éœ‡è¡ -1% ~ 1%)\n",
      "  é¢„æµ‹æ ‡ç­¾: drop (ä¸­è·Œ -5% ~ -2%)\n",
      "  ä»·æ ¼å˜åŒ–: 0.36%\n",
      "\n",
      "é”™è¯¯æ ·æœ¬ 4:\n",
      "  æ–‡æœ¬: Flowco Holdings EVP Roberts sells $362k in stock...\n",
      "  çœŸå®æ ‡ç­¾: rise (ä¸­æ¶¨ 2% ~ 5%)\n",
      "  é¢„æµ‹æ ‡ç­¾: small_rise (å°æ¶¨ 1% ~ 2%)\n",
      "  ä»·æ ¼å˜åŒ–: 3.19%\n",
      "\n",
      "é”™è¯¯æ ·æœ¬ 5:\n",
      "  æ–‡æœ¬: Lumbard  Kellner LLC Reduces Position in Five9, Inc. $FIVN...\n",
      "  çœŸå®æ ‡ç­¾: stable (éœ‡è¡ -1% ~ 1%)\n",
      "  é¢„æµ‹æ ‡ç­¾: small_rise (å°æ¶¨ 1% ~ 2%)\n",
      "  ä»·æ ¼å˜åŒ–: 0.65%\n",
      "\n",
      "é”™è¯¯æ ·æœ¬ 6:\n",
      "  æ–‡æœ¬: Semtech (SMTC): Reassessing Valuation After Solid Q3 Results and Record Data Cen...\n",
      "  çœŸå®æ ‡ç­¾: drop (ä¸­è·Œ -5% ~ -2%)\n",
      "  é¢„æµ‹æ ‡ç­¾: small_drop (å°è·Œ -2% ~ -1%)\n",
      "  ä»·æ ¼å˜åŒ–: -2.29%\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ§ª ç¬¬7æ­¥ï¼šæµ‹è¯•é¢„æµ‹\n",
      "================================================================================\n",
      "\n",
      "æ¨¡å‹åœ¨è®¾å¤‡: cuda:0\n",
      "\n",
      "æµ‹è¯•æ–°é—»é¢„æµ‹:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“° Apple announces record-breaking quarterly earnings, stock soars 8%...\n",
      "   é¢„æµ‹:  SMALL_DROP (å°è·Œ -2% ~ -1%)\n",
      "   ç½®ä¿¡åº¦: 68.49%\n",
      "   Top 3: small_drop(68.5%) stable(22.0%) small_rise(3.8%) \n",
      "\n",
      "ğŸ“° Tesla faces massive safety recall, shares plunge dramatically...\n",
      "   é¢„æµ‹:  DROP (ä¸­è·Œ -5% ~ -2%)\n",
      "   ç½®ä¿¡åº¦: 61.16%\n",
      "   Top 3: drop(61.2%) rise(18.7%) small_rise(10.0%) \n",
      "\n",
      "ğŸ“° Microsoft reports steady cloud revenue growth...\n",
      "   é¢„æµ‹:  DROP (ä¸­è·Œ -5% ~ -2%)\n",
      "   ç½®ä¿¡åº¦: 78.65%\n",
      "   Top 3: drop(78.7%) rise(10.0%) stable(6.4%) \n",
      "\n",
      "ğŸ“° NVIDIA unveils revolutionary AI chip, exceeds all expectations...\n",
      "   é¢„æµ‹:  SMALL_DROP (å°è·Œ -2% ~ -1%)\n",
      "   ç½®ä¿¡åº¦: 72.89%\n",
      "   Top 3: small_drop(72.9%) drop(11.9%) stable(11.3%) \n",
      "\n",
      "ğŸ“° Amazon maintains market position amid competition...\n",
      "   é¢„æµ‹:  STABLE (éœ‡è¡ -1% ~ 1%)\n",
      "   ç½®ä¿¡åº¦: 54.36%\n",
      "   Top 3: stable(54.4%) small_drop(20.4%) small_rise(13.7%) \n",
      "\n",
      "ğŸ“° Google announces minor organizational changes...\n",
      "   é¢„æµ‹:  DROP (ä¸­è·Œ -5% ~ -2%)\n",
      "   ç½®ä¿¡åº¦: 73.35%\n",
      "   Top 3: drop(73.4%) stable(14.2%) small_rise(8.7%) \n",
      "\n",
      "âœ… è®­ç»ƒæ€»ç»“å·²ä¿å­˜:  finbert_7class_model/training_summary.json\n",
      "\n",
      "================================================================================\n",
      "âœ… è®­ç»ƒå®Œæˆ!\n",
      "================================================================================\n",
      "\n",
      "\n",
      "âŒ é”™è¯¯: Format specifier missing precision\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Ast\\AppData\\Local\\Temp\\ipykernel_6356\\3697687283.py\", line 610, in main\n",
      "    print(f\"æœ€ç»ˆæµ‹è¯•å‡†ç¡®ç‡: {accuracy:. 4f} ({accuracy*100:.2f}%)\")\n",
      "                            ^^^^^^^^^^^^^^^\n",
      "ValueError: Format specifier missing precision\n"
     ]
    }
   ],
   "source": [
    "# train_finbert_7class.py\n",
    "\"\"\"\n",
    "Train FinBERT 7-Class Classification Model\n",
    "Predict stock price movements based on news: Big Loss/Medium Loss/Small Loss/Flat/Small Gain/Medium Gain/Big Gain\n",
    "\"\"\"\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback\n",
    ")\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    precision_recall_fscore_support, \n",
    "    confusion_matrix,\n",
    "    classification_report\n",
    ")\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import torch\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "# ==================== Configuration ====================\n",
    "MODEL_NAME = 'ProsusAI/finbert'\n",
    "DATA_FILE = 'financial_data/training_data_7class.csv'\n",
    "OUTPUT_DIR = 'finbert_7class_model'\n",
    "\n",
    "# Training parameters - Optimized version\n",
    "TRAIN_PARAMS = {\n",
    "    'num_epochs': 10,           # 10 epochs is sufficient, avoiding overfitting\n",
    "    'batch_size': 16,\n",
    "    'learning_rate': 2e-5,      # Recommended learning rate for FinBERT\n",
    "    'weight_decay': 0.01,\n",
    "    'warmup_ratio': 0.1,\n",
    "    'max_length': 128,\n",
    "    'early_stopping_patience': 3  # Increased to 3 epochs patience, giving the model more chances\n",
    "}\n",
    "\n",
    "# 7 labels (in order)\n",
    "LABELS_ORDER = ['crash', 'drop', 'small_drop', 'stable', 'small_rise', 'rise', 'surge']\n",
    "\n",
    "# Label mapping\n",
    "label2id = {label: i for i, label in enumerate(LABELS_ORDER)}\n",
    "id2label = {i: label for i, label in enumerate(LABELS_ORDER)}\n",
    "\n",
    "# Label names and ranges (for display)\n",
    "LABEL_INFO = {\n",
    "    'crash': {'name': 'Big Loss', 'range': '< -5%'},\n",
    "    'drop': {'name': 'Medium Loss', 'range': '-5% ~ -2%'},\n",
    "    'small_drop': {'name': 'Small Loss', 'range': '-2% ~ -1%'},\n",
    "    'stable': {'name': 'Flat', 'range': '-1% ~ 1%'},\n",
    "    'small_rise': {'name': 'Small Gain', 'range': '1% ~ 2%'},\n",
    "    'rise': {'name': 'Medium Gain', 'range': '2% ~ 5%'},\n",
    "    'surge': {'name': 'Big Gain', 'range': '> 5%'}\n",
    "}\n",
    "\n",
    "# ==================== Helper Functions ====================\n",
    "\n",
    "def print_section(title):\n",
    "    \"\"\"Print section divider\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(title)\n",
    "    print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load and prepare data\"\"\"\n",
    "    print_section(\"ğŸ“‚ Step 1: Load Data\")\n",
    "    \n",
    "    # Read data\n",
    "    if not os.path.exists(DATA_FILE):\n",
    "        print(f\"âŒ Error: Cannot find file {DATA_FILE}\")\n",
    "        return None\n",
    "    \n",
    "    df = pd.read_csv(DATA_FILE, encoding='utf-8-sig')\n",
    "    print(f\"âœ… Data loaded successfully: {len(df)} entries\")\n",
    "    \n",
    "    # Check required columns\n",
    "    required_cols = ['text', 'label']\n",
    "    for col in required_cols:\n",
    "        if col not in df.columns:\n",
    "            print(f\"âŒ Error: Missing '{col}' column\")\n",
    "            return None\n",
    "    \n",
    "    # Remove null values\n",
    "    before = len(df)\n",
    "    df = df[df['text'].notna() & df['label'].notna()]\n",
    "    df = df[df['text'].str.strip() != '']\n",
    "    after = len(df)\n",
    "    \n",
    "    if before > after:\n",
    "        print(f\"âš ï¸ Removed {before - after} empty entries\")\n",
    "    \n",
    "    # Convert labels to IDs\n",
    "    df['label_id'] = df['label'].map(label2id)\n",
    "    \n",
    "    # Check labels\n",
    "    if df['label_id'].isna().any():\n",
    "        print(\"âŒ Error: Unrecognized labels exist\")\n",
    "        unknown_labels = df[df['label_id'].isna()]['label'].unique()\n",
    "        print(f\"Unknown labels: {unknown_labels}\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"âœ… Valid data: {len(df)} entries\")\n",
    "    print()\n",
    "    \n",
    "    # Display label distribution\n",
    "    print(\"7-class label distribution:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Label':<12} {'Name':<12} {'Range':<15} {'Count':>8} {'Percentage':>10} {'Visualization':<30}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for label in LABELS_ORDER:\n",
    "        count = len(df[df['label'] == label])\n",
    "        percentage = count / len(df) * 100\n",
    "        bar = \"â–ˆ\" * int(percentage / 2)\n",
    "        info = LABEL_INFO[label]\n",
    "        print(f\"{label:<12} {info['name']:<12} {info['range']:<15} {count:>8} {percentage:>9.1f}% {bar}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return df\n",
    "\n",
    "def split_dataset(df):\n",
    "    \"\"\"Split dataset\"\"\"\n",
    "    print_section(\"âœ‚ï¸ Step 2: Split Dataset\")\n",
    "    \n",
    "    # 70% training, 15% validation, 15% test\n",
    "    train_df, temp_df = train_test_split(\n",
    "        df, \n",
    "        test_size=0.3, \n",
    "        random_state=42, \n",
    "        stratify=df['label_id']\n",
    "    )\n",
    "    \n",
    "    val_df, test_df = train_test_split(\n",
    "        temp_df, \n",
    "        test_size=0.5, \n",
    "        random_state=42, \n",
    "        stratify=temp_df['label_id']\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {len(train_df)} entries (70%)\")\n",
    "    print(f\"Validation set: {len(val_df)} entries (15%)\")\n",
    "    print(f\"Test set: {len(test_df)} entries (15%)\")\n",
    "    print()\n",
    "    \n",
    "    # Check label distribution in each set\n",
    "    print(\"Label distribution in each dataset:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for dataset_name, dataset_df in [('Training set', train_df), ('Validation set', val_df), ('Test set', test_df)]:\n",
    "        print(f\"{dataset_name}:\")\n",
    "        for label in LABELS_ORDER: \n",
    "            count = len(dataset_df[dataset_df['label'] == label])\n",
    "            print(f\"  {label}: {count}\")\n",
    "        print()\n",
    "    \n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def compute_class_weights(train_df):\n",
    "    \"\"\"Compute class weights (handle imbalanced data)\"\"\"\n",
    "    print_section(\"âš–ï¸ Step 3: Compute Class Weights\")\n",
    "    \n",
    "    # Compute class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.array(LABELS_ORDER),\n",
    "        y=train_df['label'].values\n",
    "    )\n",
    "    \n",
    "    print(\"Class weights (for balancing imbalanced data):\")\n",
    "    print(\"-\" * 60)\n",
    "    for label, weight in zip(LABELS_ORDER, class_weights):\n",
    "        print(f\"  {label:<12} ({LABEL_INFO[label]['name']}): {weight:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Convert to tensor\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float32)\n",
    "    \n",
    "    return class_weights_tensor\n",
    "\n",
    "def create_datasets(train_df, val_df, test_df, tokenizer):\n",
    "    \"\"\"Create Hugging Face Dataset\"\"\"\n",
    "    print_section(\"ğŸ”„ Step 4: Data Preprocessing\")\n",
    "    \n",
    "    print(\"Converting to Dataset format...\")\n",
    "    \n",
    "    # Keep only necessary columns\n",
    "    train_dataset = Dataset.from_pandas(train_df[['text', 'label_id']])\n",
    "    val_dataset = Dataset.from_pandas(val_df[['text', 'label_id']])\n",
    "    test_dataset = Dataset.from_pandas(test_df[['text', 'label_id']])\n",
    "    \n",
    "    print(\"âœ… Dataset created successfully\")\n",
    "    print()\n",
    "    \n",
    "    # Tokenize\n",
    "    print(\"Tokenizing text...\")\n",
    "    \n",
    "    def tokenize_function(examples):\n",
    "        return tokenizer(\n",
    "            examples['text'],\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=TRAIN_PARAMS['max_length']\n",
    "        )\n",
    "    \n",
    "    train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "    val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
    "    test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
    "    \n",
    "    print(\"âœ… Tokenization complete\")\n",
    "    print()\n",
    "    \n",
    "    # Set format\n",
    "    train_dataset = train_dataset.rename_column('label_id', 'labels')\n",
    "    val_dataset = val_dataset.rename_column('label_id', 'labels')\n",
    "    test_dataset = test_dataset.rename_column('label_id', 'labels')\n",
    "    \n",
    "    train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    val_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    \"\"\"Compute evaluation metrics\"\"\"\n",
    "    labels = pred.label_ids\n",
    "    preds = np.argmax(pred.predictions, axis=1)\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    \n",
    "    # Precision, Recall, F1\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "class WeightedLossTrainer(Trainer):\n",
    "    \"\"\"Custom Trainer with class weight support\"\"\"\n",
    "    \n",
    "    def __init__(self, class_weights=None, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Use weighted cross entropy loss\n",
    "        if self.class_weights is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        else:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss()\n",
    "        \n",
    "        loss = loss_fct(logits, labels)\n",
    "        \n",
    "        return (loss, outputs) if return_outputs else loss\n",
    "\n",
    "def train_model(train_dataset, val_dataset, tokenizer, class_weights):\n",
    "    \"\"\"Train model\"\"\"\n",
    "    print_section(\"ğŸš€ Step 5: Start Training\")\n",
    "    \n",
    "    # Check device\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    print(f\"Using device: {device}\")\n",
    "    if device == \"cuda\":\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        print(\"âš ï¸ Using CPU for training, speed will be slower (estimated 2-3 hours)\")\n",
    "        print(\"   If you have a GPU, please install the CUDA version of PyTorch\")\n",
    "    print()\n",
    "    \n",
    "    # Load model\n",
    "    print(\"Loading FinBERT pretrained model...\")\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=7,  # 7 classes\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "        ignore_mismatched_sizes=True\n",
    "    )\n",
    "    print(\"âœ… Model loaded successfully\")\n",
    "    print(f\"   Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "    print()\n",
    "    \n",
    "    # Training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=OUTPUT_DIR,\n",
    "        num_train_epochs=TRAIN_PARAMS['num_epochs'],\n",
    "        per_device_train_batch_size=TRAIN_PARAMS['batch_size'],\n",
    "        per_device_eval_batch_size=TRAIN_PARAMS['batch_size'] * 2,\n",
    "        learning_rate=TRAIN_PARAMS['learning_rate'],\n",
    "        weight_decay=TRAIN_PARAMS['weight_decay'],\n",
    "        warmup_ratio=TRAIN_PARAMS['warmup_ratio'],\n",
    "        \n",
    "        # Evaluation and save strategy\n",
    "        eval_strategy='epoch',\n",
    "        save_strategy='epoch',\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='f1',\n",
    "        greater_is_better=True,\n",
    "        \n",
    "        # Logging\n",
    "        logging_dir=f'{OUTPUT_DIR}/logs',\n",
    "        logging_steps=50,\n",
    "        logging_strategy='steps',\n",
    "        \n",
    "        # Other\n",
    "        push_to_hub=False,\n",
    "        report_to='none',\n",
    "        save_total_limit=2,\n",
    "        fp16=torch.cuda.is_available(),  # Use mixed precision training if GPU available\n",
    "    )\n",
    "    \n",
    "    # Create Trainer (with weighted loss)\n",
    "    trainer = WeightedLossTrainer(\n",
    "        class_weights=class_weights,\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        compute_metrics=compute_metrics,\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=TRAIN_PARAMS['early_stopping_patience'])]\n",
    "    )\n",
    "    \n",
    "    # Display training configuration\n",
    "    print(\"Training configuration:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  Epochs: {TRAIN_PARAMS['num_epochs']}\")\n",
    "    print(f\"  Batch size: {TRAIN_PARAMS['batch_size']}\")\n",
    "    print(f\"  Learning rate: {TRAIN_PARAMS['learning_rate']}\")\n",
    "    print(f\"  Total training steps: {len(train_dataset) // TRAIN_PARAMS['batch_size'] * TRAIN_PARAMS['num_epochs']}\")\n",
    "    print(f\"  Using class weights: Yes\")\n",
    "    print(f\"  Early stopping patience: {TRAIN_PARAMS['early_stopping_patience']} epochs\")\n",
    "    print()\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    # Train\n",
    "    trainer.train()\n",
    "    \n",
    "    end_time = datetime.now()\n",
    "    duration = end_time - start_time\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    print(f\"âœ… Training complete! Duration: {duration}\")\n",
    "    \n",
    "    # Get training history\n",
    "    if hasattr(trainer.state, 'log_history'):\n",
    "        print(f\"   Actual training epochs: {trainer.state.epoch}\")\n",
    "        if trainer.state.epoch < TRAIN_PARAMS['num_epochs']:\n",
    "            print(f\"   âš ï¸ Early stopping triggered: stopped after epoch {int(trainer.state.epoch)} (max epochs set: {TRAIN_PARAMS['num_epochs']})\")\n",
    "    print()\n",
    "    \n",
    "    # Save model\n",
    "    print(\"Saving model...\")\n",
    "    model.save_pretrained(OUTPUT_DIR)\n",
    "    tokenizer.save_pretrained(OUTPUT_DIR)\n",
    "    \n",
    "    # Save label mapping\n",
    "    label_mapping = {\n",
    "        'label2id': label2id,\n",
    "        'id2label': id2label,\n",
    "        'label_info': LABEL_INFO,\n",
    "        'labels_order': LABELS_ORDER\n",
    "    }\n",
    "    \n",
    "    with open(f'{OUTPUT_DIR}/label_mapping.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(label_mapping, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Model saved to: {OUTPUT_DIR}\")\n",
    "    print(f\"   - Model weights\")\n",
    "    print(f\"   - Tokenizer\")\n",
    "    print(f\"   - Label mapping\")\n",
    "    print()\n",
    "    \n",
    "    return trainer, model\n",
    "\n",
    "def evaluate_model(trainer, test_dataset, test_df):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    print_section(\"ğŸ“Š Step 6: Evaluate Model\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    print(\"Evaluating on test set...\")\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    \n",
    "    preds = np.argmax(predictions.predictions, axis=1)\n",
    "    labels = predictions.label_ids\n",
    "    \n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, preds, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    print(\"Test set results:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"  Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-Score: {f1:.4f}\")\n",
    "    print()\n",
    "    \n",
    "    # Detailed metrics for each class\n",
    "    print(\"Detailed metrics for each class:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    report = classification_report(\n",
    "        labels, preds, \n",
    "        target_names=[f\"{l} ({LABEL_INFO[l]['name']})\" for l in LABELS_ORDER],\n",
    "        digits=4,\n",
    "        zero_division=0\n",
    "    )\n",
    "    print(report)\n",
    "    print()\n",
    "    \n",
    "    # Confusion matrix\n",
    "    print(\"Confusion matrix:\")\n",
    "    print(\"-\" * 80)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    \n",
    "    # Pretty print\n",
    "    print(f\"{'Actual\\\\Predicted':>15}\", end='')\n",
    "    for label in LABELS_ORDER:\n",
    "        print(f\"{label:>10}\", end='')\n",
    "    print()\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, label in enumerate(LABELS_ORDER):\n",
    "        print(f\"{label:>15}\", end='')\n",
    "        for j in range(len(LABELS_ORDER)):\n",
    "            print(f\"{cm[i][j]:>10}\", end='')\n",
    "        print()\n",
    "    print()\n",
    "    \n",
    "    # Error analysis\n",
    "    print(\"Error sample analysis (showing first 5):\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    test_df_copy = test_df.copy().reset_index(drop=True)\n",
    "    test_df_copy['predicted'] = [id2label[p] for p in preds]\n",
    "    test_df_copy['correct'] = test_df_copy['label'] == test_df_copy['predicted']\n",
    "    \n",
    "    errors = test_df_copy[~test_df_copy['correct']]\n",
    "    \n",
    "    if len(errors) > 0:\n",
    "        print(f\"Total errors: {len(errors)} / {len(test_df_copy)} ({len(errors)/len(test_df_copy)*100:.1f}%)\")\n",
    "        print()\n",
    "        \n",
    "        for idx, row in errors.head(5).iterrows():\n",
    "            print(f\"\\nError sample {idx + 1}:\")\n",
    "            print(f\"  Text: {row['text'][:80]}...\")\n",
    "            print(f\"  Actual label: {row['label']} ({LABEL_INFO[row['label']]['name']} {LABEL_INFO[row['label']]['range']})\")\n",
    "            print(f\"  Predicted label: {row['predicted']} ({LABEL_INFO[row['predicted']]['name']} {LABEL_INFO[row['predicted']]['range']})\")\n",
    "            if 'price_change' in row:\n",
    "                print(f\"  Price change: {row['price_change']:.2f}%\")\n",
    "    else:\n",
    "        print(\"ğŸ‰ No errors! Perfect!\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    return accuracy, f1, cm\n",
    "\n",
    "def test_prediction(model, tokenizer):\n",
    "    \"\"\"Test prediction function\"\"\"\n",
    "    print_section(\"ğŸ§ª Step 7: Test Prediction\")\n",
    "    \n",
    "    # Get the device where the model is located\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"Model on device: {device}\")\n",
    "    print()\n",
    "    \n",
    "    test_texts = [\n",
    "        \"Apple announces record-breaking quarterly earnings, stock soars 8%\",\n",
    "        \"Tesla faces massive safety recall, shares plunge dramatically\",\n",
    "        \"Microsoft reports steady cloud revenue growth\",\n",
    "        \"NVIDIA unveils revolutionary AI chip, exceeds all expectations\",\n",
    "        \"Amazon maintains market position amid competition\",\n",
    "        \"Google announces minor organizational changes\"\n",
    "    ]\n",
    "    \n",
    "    print(\"Test news prediction:\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for text in test_texts:\n",
    "        # Tokenize\n",
    "        inputs = tokenizer(\n",
    "            text, \n",
    "            return_tensors='pt', \n",
    "            truncation=True, \n",
    "            max_length=128\n",
    "        )\n",
    "        \n",
    "        # Move inputs to the device where the model is located\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Predict\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            probs = torch.softmax(logits, dim=1)[0]\n",
    "        \n",
    "        pred_idx = torch.argmax(probs).item()\n",
    "        pred_label = id2label[pred_idx]\n",
    "        confidence = probs[pred_idx].item()\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nğŸ“° {text[:70]}...\")\n",
    "        print(f\"   Prediction: {pred_label.upper()} ({LABEL_INFO[pred_label]['name']} {LABEL_INFO[pred_label]['range']})\")\n",
    "        print(f\"   Confidence: {confidence:.2%}\")\n",
    "        \n",
    "        # Display top 3 most likely classes\n",
    "        top3_indices = torch.topk(probs, 3).indices\n",
    "        print(f\"   Top 3: \", end='')\n",
    "        for idx in top3_indices:\n",
    "            label = id2label[idx.item()]\n",
    "            prob = probs[idx].item()\n",
    "            print(f\"{label}({prob:.1%}) \", end='')\n",
    "        print()\n",
    "    \n",
    "    print()\n",
    "\n",
    "def save_training_summary(accuracy, f1, cm, duration):\n",
    "    \"\"\"Save training summary\"\"\"\n",
    "    summary = {\n",
    "        'model_name': MODEL_NAME,\n",
    "        'num_labels': 7,\n",
    "        'labels': LABELS_ORDER,\n",
    "        'training_params': TRAIN_PARAMS,\n",
    "        'data_file': DATA_FILE,\n",
    "        'test_accuracy': float(accuracy),\n",
    "        'test_f1': float(f1),\n",
    "        'training_duration': str(duration),\n",
    "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "    }\n",
    "    \n",
    "    with open(f'{OUTPUT_DIR}/training_summary.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"âœ… Training summary saved: {OUTPUT_DIR}/training_summary.json\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ¯ FinBERT 7-Class Model Training\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"\\nStart time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"Data file: {DATA_FILE}\")\n",
    "    print(f\"Output directory: {OUTPUT_DIR}\")\n",
    "    print()\n",
    "    \n",
    "    start_time = datetime.now()\n",
    "    \n",
    "    try:\n",
    "        # 1. Load data\n",
    "        df = load_and_prepare_data()\n",
    "        if df is None:\n",
    "            return\n",
    "        \n",
    "        # 2. Split dataset\n",
    "        train_df, val_df, test_df = split_dataset(df)\n",
    "        \n",
    "        # 3. Compute class weights\n",
    "        class_weights = compute_class_weights(train_df)\n",
    "        \n",
    "        # 4. Load tokenizer\n",
    "        print_section(\"ğŸ”§ Load Tokenizer\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "        print(\"âœ… Tokenizer loaded successfully\")\n",
    "        \n",
    "        # 5. Create datasets\n",
    "        train_dataset, val_dataset, test_dataset = create_datasets(\n",
    "            train_df, val_df, test_df, tokenizer\n",
    "        )\n",
    "        \n",
    "        # 6. Train model\n",
    "        trainer, model = train_model(train_dataset, val_dataset, tokenizer, class_weights)\n",
    "        \n",
    "        # 7. Evaluate model\n",
    "        accuracy, f1, cm = evaluate_model(trainer, test_dataset, test_df)\n",
    "        \n",
    "        # 8. Test prediction\n",
    "        test_prediction(model, tokenizer)\n",
    "        \n",
    "        # 9. Save training summary\n",
    "        end_time = datetime.now()\n",
    "        duration = end_time - start_time\n",
    "        save_training_summary(accuracy, f1, cm, duration)\n",
    "        \n",
    "        # Complete\n",
    "        print_section(\"âœ… Training Complete!\")\n",
    "        print(f\"Final test accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "        print(f\"Final F1-Score: {f1:.4f}\")\n",
    "        print(f\"Model saved at: {OUTPUT_DIR}\")\n",
    "        print(f\"Total duration: {duration}\")\n",
    "        print(f\"\\nEnd time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(\"=\" * 80 + \"\\n\")\n",
    "        \n",
    "        print(\"ğŸ‰ Congratulations! 7-class model training successful!\")\n",
    "        print()\n",
    "        print(\"Next steps:\")\n",
    "        print(\"  1. Use the model to predict new news\")\n",
    "        print(\"  2. Analyze model performance\")\n",
    "        print(\"  3. Write project report\")\n",
    "        print()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nâŒ Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fda1bc6",
   "metadata": {},
   "source": [
    "## PART 3: Finbert 7-Class News Prediction Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3697476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ¯ FinBERT 7åˆ†ç±»æ–°é—»é¢„æµ‹å·¥å…·ï¼ˆç¨³å®šç‰ˆï¼‰\n",
      "================================================================================\n",
      "\n",
      "åŠ è½½æ¨¡å‹...\n",
      "âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!\n",
      "   æ¨¡å‹ä½ç½®: finbert_7class_model\n",
      "   è®¾å¤‡:   cuda\n",
      "   æ¨¡å¼: è¯„ä¼°æ¨¡å¼ (eval)\n",
      "\n",
      "è¯·é€‰æ‹©:\n",
      "  1. ä¸€è‡´æ€§æµ‹è¯•\n",
      "  2. äº¤äº’å¼é¢„æµ‹\n",
      "  3. é€€å‡º\n",
      "\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° æ–°é—»:\n",
      "   The situation of coated magazine printing paper will continue to be weak\n",
      "\n",
      "ğŸ¯ é¢„æµ‹ç»“æœ:\n",
      "--------------------------------------------------------------------------------\n",
      "   é¢„æµ‹ç±»åˆ«: RISE\n",
      "   ä¸­æ–‡åç§°: ä¸­æ¶¨\n",
      "   é¢„æœŸæ¶¨è·Œ: 2% ~ 5%\n",
      "   ç½®ä¿¡åº¦:    43.29%\n",
      "\n",
      "ğŸ“Š å„ç±»åˆ«æ¦‚ç‡:\n",
      "--------------------------------------------------------------------------------\n",
      "   rise         (ä¸­æ¶¨ 2% ~ 5%): 43.29% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   small_rise   (å°æ¶¨ 1% ~ 2%): 31.48% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   stable       (éœ‡è¡ -1% ~ 1%): 15.80% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   drop         (ä¸­è·Œ -5% ~ -2%):  8.22% â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   small_drop   (å°è·Œ -2% ~ -1%):  0.82% \n",
      "   crash        (å¤§è·Œ < -5%):  0.27% \n",
      "   surge        (å¤§æ¶¨ > 5%):  0.11% \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° æ–°é—»:\n",
      "   This implementation is very important to the operator , since it is about to launch its Fixed to Mobile convergence service in Brazil\n",
      "\n",
      "ğŸ¯ é¢„æµ‹ç»“æœ:\n",
      "--------------------------------------------------------------------------------\n",
      "   é¢„æµ‹ç±»åˆ«: DROP\n",
      "   ä¸­æ–‡åç§°: ä¸­è·Œ\n",
      "   é¢„æœŸæ¶¨è·Œ: -5% ~ -2%\n",
      "   ç½®ä¿¡åº¦:    59.76%\n",
      "\n",
      "ğŸ“Š å„ç±»åˆ«æ¦‚ç‡:\n",
      "--------------------------------------------------------------------------------\n",
      "   drop         (ä¸­è·Œ -5% ~ -2%): 59.76% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   stable       (éœ‡è¡ -1% ~ 1%): 15.74% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   small_rise   (å°æ¶¨ 1% ~ 2%): 15.59% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   rise         (ä¸­æ¶¨ 2% ~ 5%):  7.85% â–ˆâ–ˆâ–ˆ\n",
      "   small_drop   (å°è·Œ -2% ~ -1%):  0.62% \n",
      "   surge        (å¤§æ¶¨ > 5%):  0.23% \n",
      "   crash        (å¤§è·Œ < -5%):  0.20% \n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "ğŸ“° æ–°é—»:\n",
      "   Pre-tax loss totaled euro 0.3 million , compared to a loss of euro 2.2 million in the first quarter of 2005 .\n",
      "\n",
      "ğŸ¯ é¢„æµ‹ç»“æœ:\n",
      "--------------------------------------------------------------------------------\n",
      "   é¢„æµ‹ç±»åˆ«: RISE\n",
      "   ä¸­æ–‡åç§°: ä¸­æ¶¨\n",
      "   é¢„æœŸæ¶¨è·Œ: 2% ~ 5%\n",
      "   ç½®ä¿¡åº¦:    64.09%\n",
      "\n",
      "ğŸ“Š å„ç±»åˆ«æ¦‚ç‡:\n",
      "--------------------------------------------------------------------------------\n",
      "   rise         (ä¸­æ¶¨ 2% ~ 5%): 64.09% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   crash        (å¤§è·Œ < -5%): 17.16% â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   small_drop   (å°è·Œ -2% ~ -1%):  8.12% â–ˆâ–ˆâ–ˆâ–ˆ\n",
      "   stable       (éœ‡è¡ -1% ~ 1%):  4.41% â–ˆâ–ˆ\n",
      "   drop         (ä¸­è·Œ -5% ~ -2%):  3.97% â–ˆ\n",
      "   small_rise   (å°æ¶¨ 1% ~ 2%):  1.40% \n",
      "   surge        (å¤§æ¶¨ > 5%):  0.85% \n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict_news_stable.py\n",
    "\n",
    "\"\"\"\n",
    "Stable prediction script - Ensure reproducible results\n",
    "\"\"\"\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "import torch\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# ==================== Set Random Seed ====================\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seed to ensure reproducible results\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends. cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "# ==================== Configuration ====================\n",
    "\n",
    "MODEL_PATH = 'finbert_7class_model'\n",
    "\n",
    "# Load label mapping\n",
    "\n",
    "with open(f'{MODEL_PATH}/label_mapping.json', 'r', encoding='utf-8') as f:\n",
    "    label_mapping = json.load(f)\n",
    "\n",
    "LABELS_ORDER = label_mapping['labels_order']\n",
    "id2label = {int(k): v for k, v in label_mapping['id2label'].items()}\n",
    "LABEL_INFO = label_mapping['label_info']\n",
    "\n",
    "# ==================== Load Model ====================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ¯ FinBERT 7-Class News Prediction Tool (Stable Version)\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "print(\"Loading model...\")\n",
    "\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "\n",
    "    # âœ… Set to evaluation mode (important!)\n",
    "    model.eval()\n",
    "\n",
    "    # Move to device\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "    print(\"âœ… Model loaded successfully!\")\n",
    "    print(f\"   Model path: {MODEL_PATH}\")\n",
    "    print(f\"   Device:     {device}\")\n",
    "    print(f\"   Mode:       Evaluation (eval)\")\n",
    "    print()\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Failed to load model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# ==================== Prediction Function ====================\n",
    "\n",
    "def predict_single_news(text):\n",
    "\n",
    "    \"\"\"Predict a single news item (stable version)\"\"\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(\n",
    "        text, \n",
    "        return_tensors='pt', \n",
    "        truncation=True, \n",
    "        max_length=128,\n",
    "        padding=True\n",
    "    )\n",
    "\n",
    "    # Move to correct device\n",
    "\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # âœ… Predict (disable gradient calculation)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        probabilities = torch. softmax(logits, dim=1)[0].cpu()\n",
    "\n",
    "    # Get prediction result\n",
    "\n",
    "    predicted_idx = torch.argmax(probabilities).item()\n",
    "    predicted_label = id2label[predicted_idx]\n",
    "    confidence = probabilities[predicted_idx]. item()\n",
    "    \n",
    "    # Build result\n",
    "\n",
    "    result = {\n",
    "        'text': text,\n",
    "        'predicted_label': predicted_label,\n",
    "        'predicted_name': LABEL_INFO[predicted_label]['name'],\n",
    "        'predicted_range': LABEL_INFO[predicted_label]['range'],\n",
    "        'confidence': confidence,\n",
    "        'all_probabilities': {\n",
    "            LABELS_ORDER[i]: probabilities[i].item() \n",
    "            for i in range(len(LABELS_ORDER))\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return result\n",
    "\n",
    "def display_prediction(result):\n",
    "\n",
    "    \"\"\"Nicely display prediction result\"\"\"\n",
    "\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ“° News:\")\n",
    "    print(f\"   {result['text']}\")\n",
    "    print()\n",
    "    print(\"ğŸ¯ Prediction Result:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"   Predicted Label: {result['predicted_label'].upper()}\")\n",
    "    print(f\"   Name:           {result['predicted_name']}\")\n",
    "    print(f\"   Expected Range: {result['predicted_range']}\")\n",
    "    print(f\"   Confidence:     {result['confidence']:.2%}\")\n",
    "    print()\n",
    "\n",
    "    # Show all probabilities\n",
    "\n",
    "    print(\"ğŸ“Š Probabilities for Each Class:\")\n",
    "\n",
    "    print(\"-\" * 80)\n",
    "    sorted_probs = sorted(\n",
    "        result['all_probabilities']. items(), \n",
    "        key=lambda x: x[1], \n",
    "        reverse=True\n",
    "    )\n",
    "    \n",
    "    for label, prob in sorted_probs: \n",
    "        info = LABEL_INFO[label]\n",
    "        bar = \"â–ˆ\" * int(prob * 50)\n",
    "        print(f\"   {label. ljust(12)} ({info['name']} {info['range']}): {prob: >6.2%} {bar}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "\n",
    "# ==================== Consistency Test ====================\n",
    "\n",
    "def test_consistency():\n",
    "    \"\"\"Test prediction consistency\"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ§ª Consistency Test\")\n",
    "    print(\"=\" * 80)\n",
    "    print()\n",
    "    \n",
    "    test_text = \"Apple announces record quarterly earnings beating all estimates\"\n",
    "\n",
    "    print(f\"Test news: {test_text}\")\n",
    "    print()\n",
    "    print(\"Predict 5 times in a row, check if results are consistent:\")\n",
    "    print(\"-\" * 80)\n",
    "    results = []\n",
    "    for i in range(5):\n",
    "        result = predict_single_news(test_text)\n",
    "        results.append(result)\n",
    "        print(f\"Run {i+1}:  {result['predicted_label']} - {result['confidence']:.4f}\")\n",
    "\n",
    "    # Check consistency\n",
    "\n",
    "    labels = [r['predicted_label'] for r in results]\n",
    "    confidences = [r['confidence'] for r in results]\n",
    "    \n",
    "    print()\n",
    "    if len(set(labels)) == 1:\n",
    "        print(\"âœ… Results are consistent! Prediction is stable.\")\n",
    "        print(f\"   Std Dev: {np.std(confidences):.6f}\")\n",
    "\n",
    "    else:\n",
    "        print(\"âŒ Results are inconsistent!\")\n",
    "        print(f\"   Predicted labels: {set(labels)}\")\n",
    "        print(\"   Possible reason: Model still has randomness or is overfitted\")\n",
    "\n",
    "    \n",
    "    print()\n",
    "\n",
    "# ==================== Main Program ====================\n",
    "\n",
    "def main():\n",
    "    print(\"Please select:\")\n",
    "    print(\"  1. Consistency Test\")\n",
    "    print(\"  2. Interactive Prediction\")\n",
    "    print(\"  3. Exit\")\n",
    "    print()\n",
    "\n",
    "    choice = input(\"Select (1-3):  \").strip()\n",
    "\n",
    "    print()\n",
    "    \n",
    "    if choice == '1':\n",
    "        test_consistency()\n",
    "    elif choice == '2':\n",
    "        while True:\n",
    "            text = input(\"ğŸ“° Please enter news (enter q to quit): \").strip()\n",
    "            if text.lower() == 'q':\n",
    "                break\n",
    "            if text:\n",
    "                result = predict_single_news(text)\n",
    "                display_prediction(result)\n",
    "    elif choice == '3':\n",
    "        print(\"ğŸ‘‹ Goodbye!\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae00d9fe",
   "metadata": {},
   "source": [
    "## PART 4: Finbert 7-Class News Prediction Visualization Interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14f9336c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”„ Loading model...\n",
      "âœ… Model loaded!\n",
      "================================================================================\n",
      "ğŸ“° FinBERT Financial News Sentiment Analysis - Interactive Prediction\n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebad6c4a967449b9d453066f52131b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h2>ğŸ“ Input News</h2>'), Textarea(value='', description='News: ', layout=Layout(heiâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Add this cell at the bottom of your Jupyter Notebook\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown, clear_output\n",
    "import torch\n",
    "\n",
    "# Ensure the model is loaded (if not, load it first)\n",
    "MODEL_PATH = 'finbert_7class_model'\n",
    "\n",
    "if 'model' not in globals():\n",
    "    print(\"ğŸ”„ Loading model...\")\n",
    "    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "    import json\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH)\n",
    "    model.eval()\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    with open(f'{MODEL_PATH}/label_mapping.json', 'r', encoding='utf-8') as f:\n",
    "        label_mapping = json.load(f)\n",
    "    \n",
    "    LABELS_ORDER = label_mapping['labels_order']\n",
    "    id2label = {int(k): v for k, v in label_mapping['id2label'].items()}\n",
    "    LABEL_INFO = label_mapping['label_info']\n",
    "    \n",
    "    print(\"âœ… Model loaded!\")\n",
    "\n",
    "# Create interactive interface\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“° FinBERT Financial News Sentiment Analysis - Interactive Prediction\")\n",
    "print(\"=\" * 80)\n",
    "print()\n",
    "\n",
    "# Input box\n",
    "text_input = widgets.Textarea(\n",
    "    value='',\n",
    "    placeholder='Enter news headline or content...',\n",
    "    description='News: ',\n",
    "    layout=widgets.Layout(width='100%', height='100px'),\n",
    "    style={'description_width': '60px'}\n",
    ")\n",
    "\n",
    "# Predict button\n",
    "predict_button = widgets.Button(\n",
    "    description='ğŸ” Analyze',\n",
    "    button_style='success',\n",
    "    layout=widgets.Layout(width='150px', height='40px')\n",
    ")\n",
    "\n",
    "# Example buttons\n",
    "example_buttons = []\n",
    "examples = [\n",
    "    \"Apple announces record quarterly earnings beating all estimates, stock soars 10%\",\n",
    "    \"Tesla faces massive safety recall affecting 2 million vehicles\",\n",
    "    \"Microsoft reports steady cloud revenue growth in line with expectations\",\n",
    "    \"NVIDIA unveils revolutionary AI chip exceeding analyst expectations\"\n",
    "]\n",
    "\n",
    "for i, ex in enumerate(examples):\n",
    "    btn = widgets.Button(\n",
    "        description=f'Example {i+1}',\n",
    "        button_style='info',\n",
    "        layout=widgets.Layout(width='120px')\n",
    "    )\n",
    "    btn.example_text = ex\n",
    "    example_buttons.append(btn)\n",
    "\n",
    "# Output area\n",
    "output_area = widgets.Output()\n",
    "\n",
    "# Prediction function\n",
    "def predict(text):\n",
    "    \"\"\"Predict and display result\"\"\"\n",
    "    \n",
    "    if not text or text.strip() == \"\":\n",
    "        with output_area:\n",
    "            clear_output()\n",
    "            display(Markdown(\"âš ï¸ **Please enter news content**\"))\n",
    "        return\n",
    "    \n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        display(Markdown(\"ğŸ”„ **Analyzing...**\"))\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, max_length=128, padding=True)\n",
    "    inputs = {k: v. to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Predict\n",
    "    with torch. no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        probabilities = torch.softmax(outputs.logits, dim=1)[0].cpu().numpy()\n",
    "    \n",
    "    # Get result\n",
    "    predicted_idx = probabilities.argmax()\n",
    "    predicted_label = id2label[predicted_idx]\n",
    "    confidence = probabilities[predicted_idx]\n",
    "    \n",
    "    info = LABEL_INFO[predicted_label]\n",
    "    \n",
    "    # Build display content\n",
    "    result_md = f\"\"\"\n",
    "---\n",
    "\n",
    "### ğŸ“° Input News\n",
    "> {text[: 200]}{'...' if len(text) > 200 else ''}\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Prediction Result\n",
    "\n",
    "| Item | Result |\n",
    "|------|------|\n",
    "| **Predicted Label** | `{predicted_label.upper()}` |\n",
    "| **Name** | {info['name']} |\n",
    "| **Expected Range** | {info['range']} |\n",
    "| **Confidence** | {confidence:.2%} |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Investment Suggestion\n",
    "\n",
    "\"\"\"\n",
    "    \n",
    "    # Add suggestion\n",
    "    if confidence < 0.4:\n",
    "        result_md += \"âš ï¸ **Low confidence, consider waiting**\\n\"\n",
    "    elif predicted_label in ['surge', 'rise']:\n",
    "        if confidence > 0.7:\n",
    "            result_md += \"ğŸ“ˆ **Strong bullish signal, consider buying**\\n\"\n",
    "        else:\n",
    "            result_md += \"ğŸ“ˆ **Bullish signal, buy with caution**\\n\"\n",
    "    elif predicted_label in ['crash', 'drop']:\n",
    "        if confidence > 0.7:\n",
    "            result_md += \"ğŸ“‰ **Strong bearish signal, consider selling or waiting**\\n\"\n",
    "        else:\n",
    "            result_md += \"ğŸ“‰ **Bearish signal, be cautious**\\n\"\n",
    "    elif predicted_label in ['small_rise']:\n",
    "        result_md += \"ğŸ“Š **Slightly bullish, consider small position buying**\\n\"\n",
    "    elif predicted_label in ['small_drop']:\n",
    "        result_md += \"ğŸ“Š **Slightly bearish, consider reducing position**\\n\"\n",
    "    else:\n",
    "        result_md += \"â¡ï¸ **Volatile signal, maintain position**\\n\"\n",
    "    \n",
    "    result_md += \"\\n> âš ï¸ For reference only. Investment involves risk. Make decisions carefully.\\n\\n---\\n\\n\"\n",
    "    \n",
    "    # Probability distribution\n",
    "    result_md += \"### ğŸ“Š Probability Distribution for Each Class\\n\\n\"\n",
    "    \n",
    "    # Sort\n",
    "    prob_pairs = [(LABELS_ORDER[i], probabilities[i]) for i in range(len(LABELS_ORDER))]\n",
    "    prob_pairs.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    for label, prob in prob_pairs:\n",
    "        info_label = LABEL_INFO[label]\n",
    "        bar = \"â–ˆ\" * int(prob * 30)\n",
    "        result_md += f\"- **{label}** ({info_label['name']} {info_label['range']}): {prob:.2%} {bar}\\n\"\n",
    "    \n",
    "    # Display result\n",
    "    with output_area:\n",
    "        clear_output()\n",
    "        display(Markdown(result_md))\n",
    "\n",
    "# Button click events\n",
    "def on_predict_clicked(b):\n",
    "    predict(text_input.value)\n",
    "\n",
    "def on_example_clicked(b):\n",
    "    text_input.value = b.example_text\n",
    "    predict(b.example_text)\n",
    "\n",
    "predict_button.on_click(on_predict_clicked)\n",
    "for btn in example_buttons:\n",
    "    btn.on_click(on_example_clicked)\n",
    "\n",
    "# Layout\n",
    "example_box = widgets.HBox(example_buttons)\n",
    "\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>ğŸ“ Input News</h2>\"),\n",
    "    text_input,\n",
    "    widgets.HBox([predict_button]),\n",
    "    widgets.HTML(\"<h3>ğŸ’¡ Quick Examples</h3>\"),\n",
    "    example_box,\n",
    "    widgets.HTML(\"<br>\"),\n",
    "    output_area\n",
    "])\n",
    "\n",
    "# Display interface\n",
    "display(ui)\n",
    "\n",
    "# Initial prompt\n",
    "with output_area:\n",
    "    display(Markdown(\"\"\"\n",
    "### ğŸ‘‹ Welcome to FinBERT Financial News Sentiment Analysis!\n",
    "\n",
    "**How to use:**\n",
    "1. Enter news in the input box above\n",
    "2. Click the ğŸ” Analyze button\n",
    "3. Or click the example buttons for quick test\n",
    "\n",
    "**7 Classes:**\n",
    "- ğŸ”´ Crash (<-5%)\n",
    "- ğŸŸ  Drop (-5~-2%)\n",
    "- ğŸŸ¡ Small Drop (-2~-1%)\n",
    "- âšª Volatile (-1~1%)\n",
    "- ğŸŸ¢ Small Rise (1~2%)\n",
    "- ğŸŸ¢ Rise (2~5%)\n",
    "- ğŸŸ¢ Surge (>5%)\n",
    "\"\"\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CISC7201",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
